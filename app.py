import os
import openai
from flask import Flask, request, jsonify, send_file,session,redirect
from flask_cors import CORS

from PyPDF2 import PdfReader
import pandas as pd

from azure.identity import DefaultAzureCredential
from azure.cosmos import CosmosClient, PartitionKey
from azure.storage.blob import BlobServiceClient

import logging
from datetime import datetime,timezone,timedelta,UTC
import uuid
import json
import io
import re
import fitz  # PyMuPDF
import base64
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font
from openpyxl.comments import Comment
import openpyxl
from openpyxl import load_workbook
from openpyxl.styles import Alignment

import time
import threading

import zipfile
import lxml.etree as ET
import os
import io
import ast
from azure.cosmos.exceptions import CosmosResourceNotFoundError, CosmosHttpResponseError
import secrets
from flask_session import Session
from werkzeug.security import generate_password_hash, check_password_hash
import urllib.parse
from io import StringIO
from asgiref.wsgi import WsgiToAsgi
import asyncio
import requests
import pdfplumber
from openpyxl.utils import get_column_letter
from copy import copy
from difflib import SequenceMatcher
import jaconv
import regex as regcheck

# Êó•ÂøóÊ†ºÂºèÂÆö‰πâ (Êó∂Èó¥Ê†ºÂºèÔºåÊó•ÂøóÁ∫ßÂà´ÔºåÊ∂àÊÅØ)
log_format = '%(asctime)sZ: [%(levelname)s] %(message)s'

# Êó•ÂøóËÆæÂÆö: Êó∂Èó¥Ê†ºÂºèÔºåÊó•ÂøóÁ∫ßÂà´ÔºåÊ∂àÊÅØ
logging.basicConfig(
    level=logging.INFO,  # Êó•ÂøóÁ∫ßÂà´ (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    format=log_format,   # Êó•ÂøóÊ†ºÂºè
    handlers=[logging.StreamHandler()]
)

# Managed Identity Auth
credential = DefaultAzureCredential()
token_OPENAI = credential.get_token("https://cognitiveservices.azure.com/.default")
token_COSMOS = credential.get_token("https://cosmos.azure.com/.default")

# Flask app init
app = Flask(__name__)
app.secret_key = secrets.token_hex(16)  # ÂÆâÂÖ®ÂØÜÈí•
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=30)  # ‰ºöËØùÊúâÊïàÊúü30ÂàÜÈíü

# üîπ Flask sesstion settings (save to file system)
app.config["SESSION_TYPE"] = "filesystem"
app.config["SESSION_COOKIE_SECURE"] = False 
app.config["SESSION_COOKIE_HTTPONLY"] = True
app.config["SESSION_COOKIE_SAMESITE"] = "None"
app.config["SESSION_COOKIE_NAME"] = "secure_session"  # session cookie name


Session(app)

# CORS(app, resources={r"/api/*": {"origins": "*"}})
CORS(app, supports_credentials=True, resources={
    r"*": {
        "origins": "*"  # need change to real domain
    }
})



# Ê®°ÊãüÁî®Êà∑Êï∞ÊçÆÂ∫ì
users = {
    "admin": {"password": "123"},
    "user": {"password": "123"}
}

#-----------------------------------------------------------------
# Azure OpenAI Setting
# openai.api_type = "azure"
# openai.api_key = os.getenv("AZURE_OPENAI_KEY")  # Get ENV API Key

# COSMOS_DB_KEY = os.getenv("COSMOS_DB_KEY")  # Cosmos DB Key
#-----------------------------------------------------------------


# AzureTokenCache class define
class AzureTokenCache:
    def __init__(self):
        self._lock = threading.Lock() # thredd safe lock
        self.credential = DefaultAzureCredential()
        self.scope = "https://cognitiveservices.azure.com/.default"
        
        self.cached_token = None
        self.token_expires = 0
        self.last_refreshed = 0
        
        self._refresh_token()
        self._start_refresh_thread()

    def get_token(self):
        with self._lock:
            # token 10 minute end before
            if time.time() >= self.token_expires - 600:  # 10 mintute befor end
                self._refresh_token()
            return self.cached_token

    def _acquire_new_token(self):
        """Get new token"""
        return self.credential.get_token(self.scope)

    def _refresh_token(self):
        """update token"""
        new_token = self._acquire_new_token()
        with self._lock:
            self.cached_token = new_token.token
            self.token_expires = new_token.expires_on
            self.last_refreshed = time.time()
        print(f"üîÑUpdated Token (END of at:,haha, {self._format_time(self.token_expires)})")

    def _start_refresh_thread(self):
        thread = threading.Thread(target=self._refresh_loop, daemon=True)
        thread.start()

    def _refresh_loop(self):
        while True:
            time.sleep(30)
            if time.time() >= self.token_expires - 600:
                self._refresh_token()

    def _format_time(self, timestamp):
        local_time = time.localtime(timestamp)
        adjusted_time = time.mktime(local_time) + (8 * 3600)  # 8Â∞èÊó∂
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(adjusted_time))
# -------------------------------------------------------------------
token_cache = AzureTokenCache()
#---------

# token method
openai.api_type = "azure_ad"
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")  # Get Env
openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION")  # API Version
deployment_id = os.getenv("AZURE_OPENAI_MODEL")  # Get Deploy Name(mini-ZZ)
_deployment_id = os.getenv("AZURE_OPENAI_MODEL_4")  # Get Deploy Name(mini-ZZ)

# Cosmos DB ËøûÊé• 
COSMOS_DB_URI = os.getenv("COSMOS_DB_URI")
DATABASE_NAME = os.getenv("DATABASE_NAME")
CONTAINER_NAME = os.getenv("CONTAINER_NAME")  # debug not used

# Azure Storage
ACCOUNT_URL = os.getenv("ACCOUNT_URL")
STORAGE_CONTAINER_NAME = os.getenv("STORAGE_CONTAINER_NAME")

MAX_TOKENS=32768 # 16384 for _deployment_id
TEMPERATURE=0
SEED=42
PDF_DIR = ACCOUNT_URL + STORAGE_CONTAINER_NAME

# Cosmos DB
def get_db_connection(CONTAINER):
    # Cosmos DB ÈìæÊé•ÂÆ¢Êà∑Á´Ø
    client = CosmosClient(COSMOS_DB_URI, credential=credential)
    database = client.get_database_client(DATABASE_NAME)
    container = database.get_container_client(CONTAINER)
    print("Connected to Azure Cosmos DB SQL API")
    logging.info("Connected to Azure Cosmos DB SQL API")
    return container  # Cosmos DB

#-----------------------------------------------------------------
LOG_RECORD_CONTAINER_NAME = "log_record"
FILE_MONITOR_ITEM = "file_monitor_item"
TENBREND_CONTAINER_NAME = 'tenbrend_history'
PROXYINFO_CONTAINER_NAME = 'proxyInfo'
INTEGERATION_RURU_CONTAINER_NAME = 'integeration_ruru'
#-----------------------------------------------------------------
integeration_container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

# List proxy
@app.route('/api/proxyinfo', methods=['GET'])
def get_proxyinfos():
    # Cosmos DB ËøûÊé•
    container = get_db_connection(PROXYINFO_CONTAINER_NAME)
    
    query = "SELECT * FROM c"
    users = list(container.query_items(query=query, enable_cross_partition_query=True))
    response = {
        "code": 200,
        "data": users
    }
    return jsonify(response), 200

# Create proxy
@app.route('/api/proxyinfo', methods=['POST'])
def create_proxyuser():
    data = request.get_json()
    username = data.get('username')
    password = data.get('password')

    if not username or not password:
        return jsonify({"error": "Missing username or password"}), 400

    # Cosmos DB ËøûÊé•
    container = get_db_connection(PROXYINFO_CONTAINER_NAME)

    # Á°ÆËÆ§Áî®Êà∑
    query = "SELECT * FROM c WHERE c.username = @username"
    params = [dict(name="@username", value=username)]
    existing_users = list(container.query_items(
        query=query, 
        parameters=params, 
        enable_cross_partition_query=True
    ))

    if existing_users:
        return jsonify({"error": "Username already exists"}), 409  # HTTP 409 Conflict

    user_item = {
        'id': str(uuid.uuid4()),
        'username': username,
        'password': password  # ÂØÜÁ†Å hashing
    }
    container.create_item(body=user_item)
    response = {
        "code": 200,
        "data": user_item
    }

    return jsonify(response), 201

# update proxy
@app.route('/api/proxyinfo', methods=['PUT'])
def update_proxyuser():
    try:
        data = request.get_json()
        new_username = data.get('username')
        new_password = data.get('password')

        if not all([new_username, new_password]):
            return jsonify({"error": "Required fields: proxyuserName and Password"}), 400

        container = get_db_connection(PROXYINFO_CONTAINER_NAME)

        try:
            query = f"SELECT * FROM c"
            existing_user = list(container.query_items(
                query=query,
                enable_cross_partition_query=True
            ))[0]
        except IndexError:
            return jsonify({"error": "Find error error"}), 404

        proxy_data = dict(username=new_username, password=new_password)
        if existing_user:
            existing_user.update(proxy_data)
            container.upsert_item(existing_user)
        else:
            proxy_data.update(id=str(uuid.uuid4()))
            container.upsert_item(proxy_data)

        return jsonify({
            "username": new_username,
            "code": 200
        }), 200

    except CosmosHttpResponseError as e:
        logging.error(f"Cosmos DB Error: {str(e)}")
        return jsonify({"error": "DB Error"}), 500
    except Exception as e:
        logging.error(f"server error: {str(e)}")
        return jsonify({"error": "server error"}), 500
    
USERINFO_CONTAINER_NAME = 'userInfo'
#----------------------User CRUD--------
@app.route('/api/users', methods=['GET'])
def get_users():
    # Cosmos DB ËøûÊé•
    container = get_db_connection(USERINFO_CONTAINER_NAME)

    query = "SELECT * FROM c"
    users = list(container.query_items(query=query, enable_cross_partition_query=True))
    response = {
        "code": 200,
        "data": users
    }
    return jsonify(response), 200

@app.route('/api/users', methods=['POST'])
def create_user():
    data = request.get_json()
    username = data.get('username')
    password = data.get('password')

    if not username or not password:
        return jsonify({"error": "Missing username or password"}), 400

    container = get_db_connection(USERINFO_CONTAINER_NAME)

    # Á°ÆËÆ§Áî®Êà∑
    query = "SELECT * FROM c WHERE c.username = @username"
    params = [dict(name="@username", value=username)]
    existing_users = list(container.query_items(
        query=query, 
        parameters=params, 
        enable_cross_partition_query=True
    ))

    if existing_users:
        return jsonify({"error": "Username already exists"}), 409  # HTTP 409 Conflict

    user_item = {
        'id': str(uuid.uuid4()),
        'username': username,
        'password': generate_password_hash(password)
    }
    container.create_item(body=user_item)
    response = {
        "code": 200,
        "data": user_item
    }

    return jsonify(response), 201

@app.route('/api/users/<user_id>', methods=['PUT'])
def update_user(user_id):
    try:
        data = request.get_json()
        new_username = data.get('username')
        new_password = data.get('password')

        if not all([new_username, new_password]):
            return jsonify({"error": "username and password need input"}), 400

        container = get_db_connection(USERINFO_CONTAINER_NAME)

        try:
            query = f"SELECT * FROM c WHERE c.id = '{user_id}'"
            existing_user = list(container.query_items(
                query=query,
                enable_cross_partition_query=True
            ))[0]
        except IndexError:
            return jsonify({"error": "Do not find user"}), 404

        if existing_user['username'] != new_username:
            dup_query = f"SELECT * FROM c WHERE c.username = '{new_username}'"
            if list(container.query_items(dup_query, enable_cross_partition_query=True)):
                return jsonify({"error": "username duplicate"}), 409

        updated_item = {
            "id": user_id,
            "username": new_username,
            "password": generate_password_hash(new_password),
            **{k: v for k, v in existing_user.items() if k not in ['username', 'password']}
        }

        container.delete_item(item=user_id, partition_key=existing_user['id'])
        container.create_item(body=updated_item)

        return jsonify({
            "id": updated_item['id'],
            "username": updated_item['username']
        }), 200

    except CosmosHttpResponseError as e:
        logging.error(f"Cosmos DB error: {str(e)}")
        return jsonify({"error": "db error"}), 500
    except Exception as e:
        logging.error(f"server error: {str(e)}")
        return jsonify({"error": "server error"}), 500
            

@app.route('/api/users/<user_id>', methods=['DELETE'])
def delete_user(user_id):
    container = get_db_connection(USERINFO_CONTAINER_NAME)
    
    container.delete_item(item=user_id, partition_key=user_id)
    return jsonify({"message": "User deleted"}), 200

#--------------------------------------
@app.before_request
def check_session():
    # Ê£ÄÊü•‰ºöËØùÊúâÊïàÊúü
    if 'user_id' in session:
        last_activity = session.get('last_activity')
        session.modified = True
        if last_activity and (datetime.now() - datetime.fromisoformat(last_activity)) > app.config['PERMANENT_SESSION_LIFETIME']:
            session.clear()
            return jsonify({"status": "error", "message": "Session expired"}), 401
        # Êõ¥Êñ∞ÊúÄÂêéÊ¥ªÂä®Êó∂Èó¥
        session['last_activity'] = datetime.now().isoformat()

@app.route('/api/login', methods=['POST'])
def login():
    data = request.get_json()
    username = data.get('username', '').strip().lower()
    password = data.get('password', '').strip()

    if not username or not password:
        return jsonify({"status": "error", "message": "„É¶„Éº„Ç∂„ÉºÂêç„Åæ„Åü„ÅØ„Éë„Çπ„ÉØ„Éº„Éâ„ÅåÈñìÈÅï„Å£„Å¶„ÅÑ„Åæ„Åô"}), 400

    container = get_db_connection(USERINFO_CONTAINER_NAME)
    
    query = "SELECT * FROM c WHERE c.username = @username"
    params = [dict(name="@username", value=username)]

    items = list(container.query_items(
        query=query,
        parameters=params,
        enable_cross_partition_query=True
    ))

    if not items:
        return jsonify({"success": "false", "message": "User not found"}), 404

    user = items[0]
    if not check_password_hash(user['password'], password):
        return jsonify({"success": "false", "message": "Invalid password"}), 401

    session.clear()
    session['user_id'] = user['id']
    session['username'] = username

    return jsonify({"success": "true", "message": "„É≠„Ç∞„Ç§„É≥ÊàêÂäüÔºÅ"}), 200

@app.route('/api/logout', methods=['POST'])
def logout():
    session.clear()
    return jsonify({"status": "success", "message": "„É≠„Ç∞„Ç¢„Ç¶„Éà"}), 200

@app.route('/api/protected', methods=['GET'])
def protected():
    if not session.get('session_id'):
        return jsonify({"status": "error", "message": "Unauthorized"}), 401
    
    return jsonify({
        "status": "success",
        "message": "Protected content",
        "secure_session": session.get('session_id')
    }), 200


CHECK_SESSION_COOKIE = "session_cookie"

@app.route('/api/session_cookie', methods=['GET'])
def get_session_cookie():
    try:
        container = get_db_connection(CHECK_SESSION_COOKIE)
        
        query = "SELECT * FROM c"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        for item in items:
            item['id'] = item['id']

        return jsonify(items), 200
        
    except CosmosResourceNotFoundError:
        logging.error("Monitoring status document not found")
        return jsonify({"error": "Status document not found"}), 404
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    

@app.route('/api/session_cookie', methods=['PUT'])
def update_session_cookie():
    try:
        container = get_db_connection(CHECK_SESSION_COOKIE)
        
        # secure_session
        # session_value = request.cookies.get('secure_session', 'none')
        session_value = request.json.get('status', 'off')
        
        status_item = {
            'id': 'session_cookie',
            'type': 'control',
            'session_value': session_value,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        container.upsert_item(body=status_item)
        logging.info(f"Session value updated: {session_value}")
        return jsonify({
            'message': 'Session value updated',
            'session_value': session_value
        }), 200
        
    except CosmosResourceNotFoundError as e:
        logging.error(f"Cosmos DB error: {str(e)}")
        return jsonify({"error": "Database operation failed"}), 500
    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    
#-----------API--------------------
def remove_code_blocks(text):
    text = re.sub(r'```html', '', text)
    text = re.sub(r'```', '', text)
    return text.strip()

def remove_code_blocks_enhance(text):
    text = re.sub(r'```html\n?', '', text)  
    text = re.sub(r'```', '', text)
    text = re.sub(r'\n\n\*\*NG\*\*\n```', '', text)
    return text.strip()


@app.route('/api/dic_search_db', methods=['POST'])
def dic_search_db():
    try:
        data = request.json

        original = data.get('original')
        corrected = data.get('corrected')

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        query = f"SELECT * FROM c WHERE c.original = '{original}' AND c.corrected = '{corrected}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        if items:
            results = [{"original": item["original"], "corrected": item["corrected"]} for item in items]
            return jsonify({"success": True, "data": results}), 200
        else:
            return jsonify({"success": False, "message": "No matching data found in DB."}), 404

    except Exception as e:
        logging.error(f"‚ùå Error occurred while searching DB: {e}")
        return jsonify({"success": False, "message": str(e)}), 500

@app.route('/ask_gpt', methods=['POST'])
def ask_gpt():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("‚úÖ Token Update SUCCESS")
        
        data = request.json
        prompt = data.get("input", "")

        if not prompt:
            return jsonify({"success": False, "error": "No input provided"}), 400

        # db get map
        corrected_map = fetch_and_convert_to_dict()

        # 3. apply_corrections
        corrected = apply_corrections(prompt, corrected_map)


        prompt_result = f"""
        You are a professional Japanese text proofreading assistant. 
        Please carefully proofread the content of a Japanese report following the rules below. 
        This includes not only Japanese text but also English abbreviations (Ëã±Áï•Ë™û), foreign terms (Â§ñÊù•Ë™û),
        and specialized terminology (Â∞ÇÈñÄÁî®Ë™û). Ensure that all language elements are reviewed according to the guidelines and corrected where necessary.:

        **Report Content to Proofread:**
        {corrected}

        **Proofreading Requirements:**
        1. **Check for typos and missing characters (Ë™§Â≠óËÑ±Â≠ó„Åå„Å™„ÅÑ„Åì„Å®):**
        - Ensure there are no **spelling errors** or **missing characters** in the report. 
        - „ÅÇ„Å™„Åü„ÅÆÂΩπÂâ≤„ÅØ„ÄÅÊó•Êú¨Ë™û„ÅÆË™§Â≠ó„ÉªËÑ±Â≠ó„ÉªË°®Ë®ò„Éü„Çπ„Çí‰øÆÊ≠£„Åó„ÄÅ‰∏çÂÆåÂÖ®„Å™ÂçòË™û„ÇÑÊñáÁ´†„Å´ÈÅ©Âàá„Å™Ë™û„ÇíË£úÂÆå„Åô„Çã„Åì„Å®„Åß„Åô„ÄÇ  
        ‰ª•‰∏ã„ÅÆ„É´„Éº„É´„Å´Âæì„ÅÑ„ÄÅÂÖ•Âäõ„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà„ÇíÊ†°Ê≠£„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
            **Common Mistakes Examples (Ë™§Â≠ó„ÉªËÑ±Â≠ó„ÅÆ‰æã)**:
            Example:
            - `„É™„ÉÜ„Éº„É´ÊäïÂÆ∂` ‚Üí `„É™„ÉÜ„Éº„É´ÊäïË≥áÂÆ∂` (Ë™§Â≠ó: ÂÆ∂ ‚Üí Ë≥á)
            - `Èï∑ÂõΩÂÇµ` ‚Üí `Èï∑ÊúüÂõΩÂÇµ` (ËÑ±Â≠ó: Êúü„ÇíËøΩÂä†)
            - `Ë≠ò„Åï„Çå„Åü` ‚Üí `ÊÑèË≠ò„Åï„Çå„Åü` (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä)
            - `ÈáëÁ∑©ÂíåÊúüÂæÖ` ‚Üí `ÈáëËûçÁ∑©ÂíåÊúüÂæÖ` (Ë™§Â≠ó: Èáë ‚Üí ÈáëËûç)
            - `Ë¶ãÊñπ„ÅåÂãï„Åó` ‚Üí `Ë¶ãÊñπ„ÅåÂ§âÂãï„Åó` (Ë™§Â≠ó: Âãï„Åó ‚Üí Âãï„Åç)
            - `Ë¶ñ„Åô„Çã` ‚Üí `ÈáçË¶ñ„Åô„Çã`  
            - `ÁµåÊàêÈï∑` ‚Üí `ÁµåÊ∏àÊàêÈï∑`  
            - `ÈÄÅÈÖçÈõªÂÇô` ‚Üí `ÈÄÅÈÖçÈõªË®≠ÂÇô`  
            - `Ê•≠Ë¶ãÈÄö„Åó` ‚Üí `Ê•≠Á∏æË¶ãÈÄö„Åó`  
            - `Â∏∏Â¢óÁõä` ‚Üí `ÁµåÂ∏∏Â¢óÁõä`  
            - `Ë≤°ÊîøÁ≠ñ` ‚Üí `Ë≤°ÊîøÊîøÁ≠ñ`  
            - `Êñπ` ‚Üí `ÊñπÈáù`  
            - `ÊâãE„Ç≥„Éû„Éº„Çπ` ‚Üí `Â§ßÊâãE„Ç≥„Éû„Éº„Çπ`  
            - `Èüø„Åó„Åæ„Åó„Åü` ‚Üí `ÂΩ±Èüø„Åó„Åæ„Åó„Åü`  
            - `ÊñΩ„Åï„Çå` ‚Üí `ÂÆüÊñΩ„Åï„Çå„Åü`  
            - `‰ºÅÊ•≠„ÅÆÂêà‰Ωµ„ÉªÂèé` ‚Üí `‰ºÅÊ•≠„ÅÆÂêà‰Ωµ„ÉªÂõûÂèé`  
            - `Êú¨„Å®„Åó„Åæ„Åô` ‚Üí `Âü∫Êú¨„Å®„Åó„Åæ„Åô`  
            - `ÂãôÁä∂Ê≥Å` ‚Üí `Ë≤°ÂãôÁä∂Ê≥Å`
            - `ÂÜÖÊäïË≥á‰ø°Ë®ó` ‚Üí `ÂõΩÂÜÖÊäïË≥á‰ø°Ë®ó`  
            - `ÊåÅ„Åó„Åæ„Åó„Åü` ‚Üí `Á∂≠ÊåÅ„Åó„Åæ„Åó„Åü`  
            - `„Éû„Ç§„Éä„ÇπÂõ†` ‚Üí `„Éû„Ç§„Éä„ÇπË¶ÅÂõ†`  
            - `Âèç„Åï„Çå„Çã` ‚Üí `ÂèçÊò†„Åï„Çå„Çã`  
            - `Êõø„Éò„ÉÉ„Ç∏` ‚Üí `ÁÇ∫Êõø„Éò„ÉÉ„Ç∏`  
            - `ÊØî„ÅØ` ‚Üí `ÊØîÁéá„ÅØ`
            - `Ë¶èÁ∑©Âíå` ‚Üí `Ë¶èÂà∂Á∑©Âíå`
            - `ÊôØÊ∏àÊåáÊ®ô` ‚Üí `ÁµåÊ∏àÊåáÊ®ô`
            - `Ââ§` ‚Üí `ÁµåÊ∏à`
            - `Êòá„Åô„Çã„Å™„Å©„Åæ„Å°„Åæ„Å°„Åß„Åó„Åü„ÄÇ` ‚Üí `Áï∞„Å™„ÇãÂãï„Åç„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ` (Ensure that the original text is not directly modified but follows this guideline.)
            - `Á©çÊ•µÂßøÂã¢„Å®„Åó„Åü` ‚Üí `Èï∑„ÇÅ„Å®„Åó„Åü` (Ensure that the original text is not directly modified but follows this guideline.)
            - `Ê∂àÊ•µÂßøÂã¢„Å®„Åó„Åü` ‚Üí `Èï∑„ÇÅ„Å®„Åó„Åü` (Ensure that the original text is not directly modified but follows this guideline.)
            - `ÔºàÂâ≤ÂÆâ„Å´ÔºâÊîæÁΩÆ` ‚Üí `Ââ≤ÂÆâÊÑü„ÅÆ„ÅÇ„Çã`
            - `ÈôêÂÆöÁöÑ` ‚Üí `‰ªñ„ÅÆÈÅ©Âàá„Å™Ë°®Áèæ„Å´‰øÆÊ≠£ Ôºà‰øÆÊ≠£ÁêÜÁî±: ÂäπÊûú„ÇÑÂΩ±Èüø„Åå„Éó„É©„Çπ„Åã„Éû„Ç§„Éä„Çπ„Åã‰∏çÊòéÁû≠„Å™„Åü„ÇÅÔºâ`
            - `Âà©ÁõäÁ¢∫ÂÆö„ÅÆÂ£≤„Çä` ‚Üí `ÔΩû„ÅåÂá∫„Åü„Å®„ÅÆË¶ãÊñπ Ôºà‰øÆÊ≠£ÁêÜÁî±: Êñ≠ÂÆöÁöÑ„Å™Ë°®Áèæ„Åß„ÅØÊ†πÊã†„ÅåË™¨Êòé„Åß„Åç„Å™„ÅÑ„Åü„ÇÅÔºâ`
            - `Âà©È£ü„ÅÑÂ£≤„Çä` ‚Üí `ÔΩû„ÅåÂá∫„Åü„Å®„ÅÆË¶ãÊñπ Ôºà‰øÆÊ≠£ÁêÜÁî±: Êñ≠ÂÆöÁöÑ„Å™Ë°®Áèæ„Åß„ÅØÊ†πÊã†„ÅåË™¨Êòé„Åß„Åç„Å™„ÅÑ„Åü„ÇÅÔºâ`
            - `ÂøÖ„ÅöÔΩû` ‚Üí `Ê†πÊã†„ÅåÊòéÁ§∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ‰ΩøÁî®‰∏çÂèØ Ôºà‰øÆÊ≠£ÁêÜÁî±: Â∞ÜÊù•„ÅÆÈÅãÁî®ÊàêÁ∏æ„ÇÑÁµåÊ∏àÊåáÊ®ô„Éª‰ºÅÊ•≠Ê•≠Á∏æÁ≠â„Å´„Å§„ÅÑ„Å¶Êñ≠ÂÆöÁöÑ„Å™Âà§Êñ≠„ÇíÁ§∫„ÅôË°®Áèæ„ÅØNGÔºâ`
            - `ÔΩû„Å´„Å™„Çã` ‚Üí `Ê†πÊã†„ÅåÊòéÁ§∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ‰ΩøÁî®‰∏çÂèØ Ôºà‰øÆÊ≠£ÁêÜÁî±: Â∞ÜÊù•„ÅÆÈÅãÁî®ÊàêÁ∏æ„ÇÑÁµåÊ∏àÊåáÊ®ô„Éª‰ºÅÊ•≠Ê•≠Á∏æÁ≠â„Å´„Å§„ÅÑ„Å¶Êñ≠ÂÆöÁöÑ„Å™Âà§Êñ≠„ÇíÁ§∫„ÅôË°®Áèæ„ÅØNGÔºâ`
            - `ÔΩû„Åß„ÅÇ„Çã` ‚Üí `Ê†πÊã†„ÅåÊòéÁ§∫„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ‰ΩøÁî®‰∏çÂèØ Ôºà‰øÆÊ≠£ÁêÜÁî±: Â∞ÜÊù•„ÅÆÈÅãÁî®ÊàêÁ∏æ„ÇÑÁµåÊ∏àÊåáÊ®ô„Éª‰ºÅÊ•≠Ê•≠Á∏æÁ≠â„Å´„Å§„ÅÑ„Å¶Êñ≠ÂÆöÁöÑ„Å™Âà§Êñ≠„ÇíÁ§∫„ÅôË°®Áèæ„ÅØNGÔºâ`
  
            **Disambiguation Rule**:
            - „ÄåÊ≤àÈùô„ÄçÔºùËá™ÁÑ∂„Å´ËêΩ„Å°ÁùÄ„Åè (natural calming down; happens over time)
            - „ÄåÈéÆÈùô„ÄçÔºù‰∫∫ÁÇ∫ÁöÑ„Å´„Åä„Åï„ÇÅ„Çã (intentional suppression; medically or artificially done)

            **Correction Policy**:
            1. Detect whether the context implies a natural or artificial calming.
            2. If the usage does not match the context, correct it using the appropriate word.
            3. Highlight the correction using the format below:
            `<span style="color:red;">Corrected Term</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÊÑèÂë≥„ÅÆË™§Áî® <s style="background:yellow;color:red">Original Term</s> ‚Üí Corrected Term</span>)`
            4. Do **not** modify the original sentence structure or paragraph formatting.
            5. Only apply the correction when the term is clearly misused.
            6. If the current usage is correct, do not change or annotate it.

            **Example**:
            - Input: Â∏ÇÂ†¥„ÅØÂæê„ÄÖ„Å´ÈéÆÈùô„Åó„Å¶„ÅÑ„Å£„Åü„ÄÇ
            - Output: Â∏ÇÂ†¥„ÅØÂæê„ÄÖ„Å´ <span style="color:red;">Ê≤àÈùô</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÊÑèÂë≥„ÅÆË™§Áî® <s style="background:yellow;color:red">ÈéÆÈùô</s> ‚Üí Ê≤àÈùô</span>) „Åó„Å¶„ÅÑ„Å£„Åü„ÄÇ


        - Ë°®Áèæ„ÅÆ‰ΩøÁî®Âà∂Èôê:
            Expression Usage Restrictions:
            Restricted Expressions:

            - È≠ÖÂäõÁöÑ„Å™
            - ÊäïË≥áÂ¶ôÂë≥
            - Ââ≤È´òÊÑü
            - Ââ≤ÂÆâÊÑü

            Usage Conditions:
            The above expressions can be used if evidence is provided.

            However, these expressions should not be used in contexts where the word "fund" („Éï„Ç°„É≥„Éâ) or any related reference is mentioned. In any sentence or context where "fund" or "„Éï„Ç°„É≥„Éâ" appears, these expressions should be avoided.

            ‰ΩøÁî®‰æã:
            È≠ÖÂäõÁöÑ„Å™: Ê†πÊã†„Å´Âü∫„Å•„ÅÑ„Å¶‰ΩøÁî®„Åô„Çã„Åì„Å®„ÅØÂèØËÉΩ„Åß„Åô„Åå„ÄÅ„Éï„Ç°„É≥„Éâ„Å´„Å§„ÅÑ„Å¶„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ„Çà„ÅÜ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
            ÊäïË≥áÂ¶ôÂë≥: ÊäïË≥áÂ¶ôÂë≥„Åå„ÅÇ„Çã„Åì„Å®„ÇíÁ§∫„ÅôÂ†¥Âêà„Åß„ÇÇ„ÄÅ„Éï„Ç°„É≥„Éâ„Å´ÂØæ„Åô„ÇãË®ÄÂèä„ÅØÈÅø„Åë„ÄÅ‰ªñ„ÅÆÊäïË≥áÂØæË±°„Å´ÈÅ©Áî®„Åô„Çã„Çà„ÅÜ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
            Ââ≤È´òÊÑü: Ââ≤È´òÊÑü„Å´„Å§„ÅÑ„Å¶Ëø∞„Åπ„ÇãÂ†¥Âêà„ÄÅ„Éï„Ç°„É≥„Éâ‰ª•Â§ñ„ÅÆÊäïË≥áÂØæË±°„Å´ÂØæ„Åó„Å¶ÈÅ©Áî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
            Ââ≤ÂÆâÊÑü: Ââ≤ÂÆâÊÑü„Å´„Å§„ÅÑ„Å¶Ë®ÄÂèä„Åô„ÇãÂ†¥Âêà„ÇÇ„ÄÅ„Éï„Ç°„É≥„Éâ„Å´ÂØæ„Åó„Å¶‰ΩøÁî®„Åô„Çã„Åì„Å®„ÅØ‰∏çÂèØ„Åß„Åô„ÄÇ

            ‚úÖ Âá∫Âäõ„Éï„Ç©„Éº„Éû„ÉÉ„Éà:
            <span style="color:red;">È≠ÖÂäõÁöÑ„Å™</span>
            (<span>‰øÆÊ≠£ÁêÜÁî±: „Éï„Ç°„É≥„Éâ„Å´ÂØæ„Åó„Å¶„ÅÆ‰ΩøÁî®„ÅØ‰∏çÂèØ„ÄÇ</span>)
            ‚úÖ Exsample1:
            Input:„Éï„Ç°„É≥„Éâ„ÅØÈ≠ÖÂäõÁöÑ„Å™ÊäïË≥áÂÖà„Å®„Åó„Å¶Á¥π‰ªã„Åï„Çå„Åü„ÄÇ

            Output:
            „Éï„Ç°„É≥„Éâ„ÅØ
            <span style="color:red;">È≠ÖÂäõÁöÑ„Å™</span>
            (<span>‰øÆÊ≠£ÁêÜÁî±: „Éï„Ç°„É≥„Éâ„Å´ÂØæ„Åô„Çã‰ΩøÁî®„ÅØ‰∏çÂèØ</span>)ÊäïË≥áÂÖà„Å®„Åó„Å¶Á¥π‰ªã„Åï„Çå„Åü„ÄÇ
            ‚úÖ Exsample2:
            Input:
            „Åì„ÅÆ„Éï„Ç°„É≥„ÉâÈäòÊüÑ„Å´„ÅØÊäïË≥áÂ¶ôÂë≥„Åå„ÅÇ„Çã„ÄÇ
            
            Output:
            „Åì„ÅÆÈäòÊüÑ„Å´„ÅØ
            <span style="color:red;">ÊäïË≥áÂ¶ôÂë≥</span>
            (<span>‰øÆÊ≠£ÁêÜÁî±: „Éï„Ç°„É≥„Éâ„Å´ÂØæ„Åó„Å¶„ÅÆ‰ΩøÁî®„ÅØ‰∏çÂèØ„ÄÇ</span>)„Åå„ÅÇ„Çã„ÄÇ


        - Êï∞Â≠ó„ÇÑ„Éë„Éº„Çª„É≥„ÉàÔºàÔºÖÔºâ„ÇíÂê´„ÇÄÊñáÁ´†„ÅÆË™§„Çä„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åô„Çã
            ÊñáËÑà„ÇíÁêÜËß£„Åó„ÄÅÊï∞ÂÄ§„ÉªÂâ≤Âêà„ÅÆÂâçÂæå„ÅÆË™ûÂè•„ÅåÈÅ©Âàá„ÅãÁ¢∫Ë™ç„Åô„Çã„Åì„Å®„ÄÇ
            ‰æã:
            ÊúàÊú´ÊôÇÁÇπÔºà20Êó•Âà§ÂÆöÔºâ„Åß„ÅÆÁÇ∫Êõø„Éò„ÉÉ„Ç∏„ÅÆ„Çø„Éº„Ç≤„ÉÉ„ÉàÊØî„ÅØ48ÔºÖ„Åß„Åô„ÄÇ
            ‚Üí Ê≠£„Åó„ÅÑË°®Ë®ò: ÊúàÊú´ÊôÇÁÇπÔºà20Êó•Âà§ÂÆöÔºâ„Åß„ÅÆÁÇ∫Êõø„Éò„ÉÉ„Ç∏„ÅÆ„Çø„Éº„Ç≤„ÉÉ„ÉàÊØîÁéá„ÅØ48ÔºÖ„Åß„Åô„ÄÇ
            Â∏ÇÂ†¥„ÅÆÊàêÈï∑Áéá„ÅØ10%„ÅÆË¶ãËæº„Åø„Åß„Åô„ÄÇ ‚úÖ (ÂïèÈ°å„Å™„Åó)
            „Ç§„É≥„Éï„É¨Áéá„ÅØ2‰∏äÊòá„Åó„Åæ„Åó„Åü„ÄÇ ‚ùå (Ë™§„Çä: 2%‰∏äÊòá„Åó„Åæ„Åó„Åü„ÄÇ „Å´‰øÆÊ≠£)
            Ë≤©Â£≤„Ç∑„Çß„Ç¢„ÅØ15„ÅÆÊã°Â§ß„Åå‰∫àÊÉ≥„Åï„Çå„Åæ„Åô„ÄÇ ‚ùå (Ë™§„Çä: Ë≤©Â£≤„Ç∑„Çß„Ç¢„ÅØ15%„ÅÆÊã°Â§ß„Åå‰∫àÊÉ≥„Åï„Çå„Åæ„Åô„ÄÇ)
        - Ê†°Ê≠£„É´„Éº„É´
            1. **„ÄåË°å„Å£„Å¶Êù•„ÅÑ„Äç„ÅÆÈÅ©Âàá„Å™ÁΩÆ„ÅçÊèõ„Åà**(Ensure that the original text is not directly modified but follows this guideline.)
            - ÊñáÁ´†ÂÖ®‰Ωì„ÇíÂàÜÊûê„Åó„ÄÅ„ÄåË°å„Å£„Å¶Êù•„ÅÑ„Äç„Åå‰Ωï„ÇíÊåá„Åó„Å¶„ÅÑ„Çã„ÅÆ„Åã„ÇíÂà§Êñ≠„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
            - **‰æ°Ê†º„ÉªÊåáÊï∞„Éª„É¨„Éº„Éà„Å™„Å©„Åå‰∏äÊòá„Åó„ÅüÊÑèÂë≥„ÅÆÂ†¥Âêà** ‚Üí „ÄåË°å„Å£„Å¶Êù•„ÅÑ„Äç„Çí„Äå‰∏äÊòá„Åó„Åü„Äç„Å´Â§âÊèõ
            - **‰æ°Ê†º„ÉªÊåáÊï∞„Éª„É¨„Éº„Éà„Å™„Å©„Åå‰∏ãËêΩ„Åó„ÅüÊÑèÂë≥„ÅÆÂ†¥Âêà** ‚Üí „ÄåË°å„Å£„Å¶Êù•„ÅÑ„Äç„Çí„Äå‰∏ãËêΩ„Åó„Åü„Äç„Å´Â§âÊèõ

            2. **ÊñáËÑà„ÇíËÄÉÊÖÆ„Åó„ÅüÊ†°Ê≠£**
            - ‰øÆÊ≠£„ÅÆÈöõ„ÄÅÂë®Ëæ∫„ÅÆÊñáËÑà„ÇíÁêÜËß£„Åó„ÄÅËá™ÁÑ∂„Å™ÂΩ¢„Å´Ë™øÊï¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
            
            3. **„ÄåÊ®™„Å∞„ÅÑ„Äç„ÅÆÈÅ©Âàá„Å™ÁΩÆ„ÅçÊèõ„Åà**
            - ÊñáÁ´†ÂÖ®‰Ωì„ÇíÂàÜÊûê„Åó„ÄÅ„ÄåÊ®™„Å∞„ÅÑ„Äç„ÅÆÂâçÂæå„ÅÆÊñáËÑà„ÇíËÄÉÊÖÆ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
            - **Êúü‰∏≠„ÅÆÂ§âÂãïÂπÖ„ÅåÂ∞è„Åï„ÅÑÂ†¥Âêà** ‚Üí „ÄåÊ®™„Å∞„ÅÑ„Äç„ÇíÁ∂≠ÊåÅ
            - **Â§âÂãïÂπÖ„ÅåÂ§ß„Åç„Åè„ÄÅÁµêÊûúÁöÑ„Å´ÂêåÁ®ãÂ∫¶„Å®„Å™„Å£„ÅüÂ†¥Âêà** ‚Üí „Äå„Åª„ÅºÂ§â„Çè„Çâ„Åö„Äç„Åæ„Åü„ÅØ„ÄåÂêåÁ®ãÂ∫¶„Å®„Å™„Çã„Äç„Å´Â§âÊõ¥
            - Âë®Âõ≤„ÅÆÊñáÁ´†„Å´Âêà„Çè„Åõ„Å¶„ÄÅ„Çà„ÇäÈÅ©Âàá„Å™Ë°®Áèæ„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

            4. **„ÄåÂá∫ÈÅÖ„ÇåÊÑü„Äç„ÅÆÈÅ©Âàá„Å™‰øÆÊ≠£**
            - ÂØæÂøúÊñπÈáù:

            „ÄåÂá∫ÈÅÖ„ÇåÊÑü„Äç „ÅØ‰∏ªË¶≥ÁöÑ„Å™Áõ∏Â†¥Ë¶≥„ÅåÂê´„Åæ„Çå„Çã„Åü„ÇÅ„ÄÅÂøÖ„Åö„Äå‚Ä¶„Å®ËÄÉ„Åà„Åæ„Åô„ÄÇ„Äç„Å´‰øÆÊ≠£„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

            ‰øÆÊ≠£ÊñπÊ≥ï: ÊñáËÑà„Å´Âøú„Åò„Å¶Ëá™ÁÑ∂„Å´„Äå„Äú„Å®ËÄÉ„Åà„Åæ„Åô„ÄçÂΩ¢„Å´‰øÆÊ≠£„Åó„Åæ„Åô„ÄÇ

            ‰øÆÊ≠£Âæå„ÅÆË°®Áèæ: Êñá„ÅÆÊµÅ„Çå„Å´Âêà„Çè„Åõ„Å¶Ëá™ÁÑ∂„Å´Ë°®Áèæ„ÇíË™øÊï¥„Åó„ÄÅË™≠„Åø„ÇÑ„Åô„Åï„ÇíËÄÉÊÖÆ„Åó„Åæ„Åô„ÄÇ

            ‰øÆÊ≠£ÁêÜÁî±: Áõ∏Â†¥Ë¶≥„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅ„ÄÅ‰∏ªË¶≥ÁöÑ„Å™Ë°®Áèæ„ÇíÂÆ¢Ë¶≥ÁöÑ„Å™Ë°®Áèæ„Å´Â§â„Åà„Çã„Åì„Å®„ÅßÊñáÁ´†„ÅÆ‰ø°È†ºÊÄß„ÇíÂêë‰∏ä„Åï„Åõ„Åæ„Åô„ÄÇ

            Âá∫Âäõ„Éï„Ç©„Éº„Éû„ÉÉ„Éà:
            <span style="color:red;">Âá∫ÈÅÖ„ÇåÊÑü</span>
            (<span>‰øÆÊ≠£ÁêÜÁî±: ‰∏ªË¶≥ÁöÑË°®Áèæ„ÅÆ‰øÆÊ≠£ <s style="background:yellow;color:red">Âá∫ÈÅÖ„ÇåÊÑü</s> ‚Üí „Äå„Äú„Å®ËÄÉ„Åà„Åæ„Åô„Äç„Å´‰øÆÊ≠£</span>)

            Exsample:
                Input: „Åì„ÅÆÈäòÊüÑ„Å´„ÅØÂá∫ÈÅÖ„ÇåÊÑü„Åå„ÅÇ„Çã
                Output: „Åì„ÅÆÈäòÊüÑ„Å´„ÅØ
                <span style="color:red;">Âá∫ÈÅÖ„ÇåÊÑü</span>
                (<span>‰øÆÊ≠£ÁêÜÁî±: ‰∏ªË¶≥ÁöÑË°®Áèæ„ÅÆ‰øÆÊ≠£ <s style="background:yellow;color:red">Âá∫ÈÅÖ„ÇåÊÑü</s> ‚Üí Âá∫ÈÅÖ„Çå„Å¶„ÅÑ„Çã„Å®ËÄÉ„Åà„Åæ„Åô</span>)
                „Åå„ÅÇ„Çã

                Input: ‰∏ÄÈÉ®„ÅÆ„Çª„ÇØ„Çø„Éº„Å´„ÅØÂá∫ÈÅÖ„ÇåÊÑü„Åå„ÅÇ„Çã„Å®ÊÑü„Åò„Çâ„Çå„Çã
                Output: ‰∏ÄÈÉ®„ÅÆ„Çª„ÇØ„Çø„Éº„Å´„ÅØ
                <span style="color:red;">Âá∫ÈÅÖ„ÇåÊÑü</span>
                (<span>‰øÆÊ≠£ÁêÜÁî±: ‰∏ªË¶≥ÁöÑË°®Áèæ„ÅÆ‰øÆÊ≠£ <s style="background:yellow;color:red">Âá∫ÈÅÖ„ÇåÊÑü</s> ‚Üí Âá∫ÈÅÖ„Çå„Å¶„ÅÑ„Çã„Å®ËÄÉ„Åà„Åæ„Åô</span>)
                „Åå„ÅÇ„Çã


            5. **„Äå‰∏äÊòáË¶ÅÂõ†„Äç„Éª„Äå‰∏ãËêΩË¶ÅÂõ†„Äç„ÅÆÈÅ©Âàá„Å™Ë™¨ÊòéËøΩÂä†**
            - **ÊñáËÑà„ÇíÂàÜÊûê„Åó„ÄÅÂÖ∑‰ΩìÁöÑ„Å™Ë¶ÅÂõ†„ÇíËøΩÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑ**„ÄÇ
            - **„Äå‰∏äÊòáË¶ÅÂõ†„Äç„Åå„ÅÇ„ÇãÂ†¥Âêà** ‚Üí ‰∏äÊòá„ÅÆÁêÜÁî±Ôºà‰æã: ‰ºÅÊ•≠Ê±∫ÁÆó„ÅÆÊîπÂñÑ„ÄÅÊîøÁ≠ñ„ÅÆÁô∫Ë°®„ÄÅÈúÄÁµ¶„Éê„É©„É≥„Çπ„ÅÆÂ§âÂåñ„Å™„Å©Ôºâ„ÇíË£úË∂≥„ÄÇ
            - **„Äå‰∏ãËêΩË¶ÅÂõ†„Äç„Åå„ÅÇ„ÇãÂ†¥Âêà** ‚Üí ‰∏ãËêΩ„ÅÆÁêÜÁî±Ôºà‰æã: ÊôØÊ∞óÂæåÈÄÄÊá∏Âøµ„ÄÅÈáëËûçÂºï„ÅçÁ∑†„ÇÅ„ÄÅÂú∞ÊîøÂ≠¶„É™„Çπ„ÇØ„Å™„Å©Ôºâ„ÇíË£úË∂≥„ÄÇ
            - **‰øÆÊ≠£Âæå„ÇÇÊñáÁ´†„ÅÆÊµÅ„Çå„Åå„Çπ„É†„Éº„Ç∫„Å´„Å™„Çã„Çà„ÅÜ„Å´Ë™øÊï¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

            6. **„Äå‰∫àÊÉ≥„Äç„ÄåÂøÉÁêÜ„Äç„ÅÆÈÅ©Âàá„Å™‰øÆÊ≠£**
            - **„Äå‰∫àÊÉ≥„Äç „Åå„ÅÇ„ÇãÂ†¥Âêà:**  
                - **Ë™∞„ÅÆ‰∫àÊÉ≥„ÅãÊòéÁ¢∫„Åß„Å™„ÅÑÂ†¥Âêà** ‚Üí „ÄåÂ∏ÇÂ†¥‰∫àÊÉ≥„Äç„Å´‰øÆÊ≠£  
            - **„ÄåÂøÉÁêÜ„Äç „Åå„ÅÇ„ÇãÂ†¥Âêà:**  
                - **‰∏ªË™û„ÅåÊõñÊòß„Å™Â†¥Âêà** ‚Üí „ÄåÂ∏ÇÂ†¥ÂøÉÁêÜ„Äç„Å´‰øÆÊ≠£

    
        2. **Follow the "Fund Manager Comment Terminology Guide" („Éï„Ç°„É≥„Éâ„Éû„Éç„Éº„Ç∏„É£„Ç≥„É°„É≥„ÉàÁî®Ë™ûÈõÜ„Å´Ê≤ø„Å£„ÅüË®òËºâ„Å®„Å™„Å£„Å¶„ÅÑ„Çã„Åì„Å®):**
        - **Consistent Terminology (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä):**
            - Ensure the **writing format** of financial terms is **consistent throughout the report**.
            Example:
            - `Áõ∏ÂØæ„Å´‰Ωé„Åã„Å£„Åü` ‚Üí `Áõ∏ÂØæÁöÑ„Å´‰Ωé„Åã„Å£„Åü` (ÊñáÊ≥ï‰øÆÊ≠£)
            - `Êù±Ë®º33Ê•≠Á®ÆÂàÜ„Åß„ÅØ` ‚Üí `Êù±Ë®º33Ê•≠Á®ÆÂàÜÈ°û„Åß„ÅØ` (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä)
        - **Common Mistakes and Corrections (Ë™§Ë®ò„Å®‰øÆÊ≠£‰æã)**:
            Example:
            - `ÊîøÂ∫úÊîØ„ÅÆ` ‚Üí `ÊîøÂ∫úÊîØÂá∫„ÅÆÈÅÖ„Çå„ÄÅ1Âõû„ÅØ„ÅÆ„ÇíÂá∫„ÄÅ2Âõû„ÅØ„ÅÆ„ÇíÂâäÈô§` (Ë™§Â≠ó: „ÅÆ ‚Üí Âá∫)
            - `ÊäïË≥áÊØîÁéá„ÇíÁ∂≠„Åô„Çã` ‚Üí `ÊäïË≥áÊØîÁéá„ÇíÁ∂≠ÊåÅ„Åô„Çã` (‰∏ÄËá¥ÊÄß‰∏çË∂≥ ÂãïÁî£ ‚Üí ‰∏çÂãïÁî£)
            - `ÔºàÈÖçÂΩì„Åì„ÅøÔºâ` ‚Üí `ÔºàÈÖçÂΩìËæº„ÅøÔºâ` (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä: „Åì„Åø ‚Üí Ëæº„Åø)
            - `„ÅÑ„Å£„ÅΩ„ÅÜ„Åß` ‚Üí `‰∏ÄÊñπ„Åß` (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä: „ÅÑ„Å£„ÅΩ„ÅÜ ‚Üí ‰∏ÄÊñπ)
            - `„Ç¶„ÇØ„É©„Ç§„Éä„Å®„É≠„Ç∑„Ç¢„Çí„ÇÅ„Åê„Çã` ‚Üí `„Ç¶„ÇØ„É©„Ç§„Éä„Å®„É≠„Ç∑„Ç¢„ÇíÂ∑°„Çã` (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä: „ÇÅ„Åê„Çã ‚Üí Â∑°„Çã)
            - `Êù±Ë®º33Ê•≠Á®Æ„Åß„Åø„Çã„Å®` ‚Üí `Êù±Ë®º33Ê•≠Á®Æ„ÅßË¶ã„Çã„Å®` (ÊñáÊ≥ï‰øÆÊ≠£: „Åø„Çã ‚Üí Ë¶ã„Çã)
            - `„Å≤„ÅçÁ∂ö„Åç` ‚Üí `Âºï„ÅçÁ∂ö„Åç` (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä: „Å≤„Åç ‚Üí Âºï„Åç)
            - `ÈõªÊ∞óÊ©üÂô®„ÄÅÈäÄË°åÊ•≠„ÄÅ‰øùÈô∫Ê•≠Á≠â` ‚Üí `ÈõªÊ∞óÊ©üÂô®„ÄÅÈäÄË°åÊ•≠„ÄÅ‰øùÈô∫Ê•≠„Å™„Å©` (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä: Á≠â ‚Üí „Å™„Å©)
            - `„Åò„Åç` ‚Üí `Ê¨°Êúü`
            - `Â∫ï„ÅÑ„ÇåÂæå` ‚Üí `Â∫ïÂÖ•„ÇåÂæå`
            - `„Å™„Åã„Åã„Çâ` ‚Üí `‰∏≠„Åã„Çâ`
            - `„Åä„ÇÇ„Å™` ‚Üí `‰∏ªË¶Å„Å™`
            - `„ÅØ„ÇÑ„Åè` ‚Üí `Êó©ÊÄ•„Å´`
            - `„Åã„ÅÑ„Å§„Åë„Åó„Åü` ‚Üí `Ë≤∑„ÅÑ‰ªò„Åë„Åó„Åü`
            - `„Å™„Å©` ‚Üí `Á≠â`
            - `„ÅÆ„Åû„Åè` ‚Üí `Èô§„Åè`
            - `„Åè„Åø„ÅÑ„Çå` ‚Üí `ÁµÑ„ÅøÂÖ•„Çå`
            - `„Åò„Çá„ÅÜ„Åç` ‚Üí `‰∏äË®ò`
            - `„Å®„ÅÜ„Éï„Ç°„É≥„Éâ` ‚Üí `ÂΩì„Éï„Ç°„É≥„Éâ`

        - **Prohibited Words and Phrases (Á¶ÅÊ≠¢ÔºàNGÔºâ„ÉØ„Éº„ÉâÂèä„Å≥ÊñáÁ´†„ÅÆÊ≥®ÊÑè‰∫ãÈ†Ö):**
            - Check if any prohibited words or phrases are used in the report and correct them as per the guidelines.
        - **Replaceable and Recommended Terms/Expressions (ÁΩÆ„ÅçÊèõ„Åà„ÅåÂøÖË¶Å„Å™Áî®Ë™û/Ë°®Áèæ„ÄÅÁΩÆ„ÅçÊèõ„Åà„ÇíÊé®Â•®„Åô„ÇãÁî®Ë™û/Ë°®Áèæ):**
            - If you find terms or expressions that need to be replaced, revise them according to the provided rules.
            - „Éè„ÉàÊ¥æÔºè„Çø„Ç´Ê¥æ„ÅÆË°®Ë®òÔºàÈáëËûçÊîøÁ≠ñ„Å´Èñ¢„Åô„ÇãÔºâ:
                -Exsample:
                - ÈáëËûçÁ∑©ÂíåÈáçË¶ñ  ‚Üí ÈáëËûçÂºï„ÅçÁ∑†„ÇÅÈáçË¶ñ
                - ÈáëËûçÁ∑©Âíå„Å´ÂâçÂêë„Åç  ‚Üí ÈáëËûçÂºï„ÅçÁ∑†„ÇÅ„Å´Á©çÊ•µÁöÑ
            - Exsample:
             - Áπî„ÇäËæº„ÇÄ  ‚Üí ÂèçÊò†„Åï„Çå
             - Áõ∏Â†¥  ‚Üí Â∏ÇÂ†¥/‰æ°Ê†º
             - ÈÄ£„ÇåÈ´ò  ‚Üí ÂΩ±Èüø„ÇíÂèó„Åë„Å¶‰∏äÊòá
             - ‰ºùÊí≠  ‚Üí Â∫É„Åå„Çã
             - „Éà„É¨„É≥„Éâ  ‚Üí ÂÇæÂêë
             - „É¨„É≥„Ç∏  ‚Üí ÁØÑÂõ≤
        - **„ÄáÔºÖ„Çí‰∏äÂõû„ÇãÔºà‰∏ãÂõû„ÇãÔºâ„Éû„Ç§„Éä„Çπ„ÅÆË°®Ë®ò:**
                - „ÄáÔºÖ„Çí‰∏äÂõû„Çã  ‚Üí „ÄáÔºÖ„ÇíË∂Ö„Åà„Çã
                - „ÄáÔºÖ„Çí‰∏ãÂõû„Çã  ‚Üí Á∏ÆÂ∞è
                - „ÄáÔºÖ„Çí‰∏äÂõû„Çã  ‚Üí ‰∏ãÂõû„Çã„Éû„Ç§„Éä„ÇπÂπÖ
                - „ÄáÔºÖ„Çí‰∏ãÂõû„Çã  ‚Üí Á∏ÆÂ∞è


        - **Use of Hiragana („Å≤„Çâ„Åå„Å™„ÇíË°®Ë®ò„Åô„Çã„ÇÇ„ÅÆ):**
            - Ensure the report follows the rules for hiragana notation, replacing content that does not conform to commonly used kanji.
        - **Kana Notation for Non-Standard Kanji (‰∏ÄÈÉ®„Åã„Å™Êõ∏„ÅçÁ≠â„ÅßË°®Ë®ò„Åô„Çã„ÇÇ„ÅÆ):**
            - Ensure non-standard kanji are replaced with kana as the standard writing format.
        - **Correct Usage of Okurigana (‰∏ÄËà¨ÁöÑ„Å™ÈÄÅ„Çä‰ªÆÂêç„Å™„Å©):**
            - Ensure the correct usage of okurigana is applied.
        - **English Abbreviations, Loanwords, and Technical Terms (Ëã±Áï•Ë™û„ÄÅÂ§ñÊù•Ë™û„ÄÅÂ∞ÇÈñÄÁî®Ë™û„Å™„Å©):**
            - Check if English abbreviations, loanwords, and technical terms are expressed correctly.
        - **Identify and mark any Â∏∏Áî®Â§ñÊº¢Â≠ó (Hy≈çgai kanji):**
        - **Identify and mark any **Â∏∏Áî®Â§ñÊº¢Â≠ó (Hy≈çgai kanji)** in the following text**
        - **Â∏∏Áî®Â§ñÊº¢Â≠ó** refers to characters **not included** in the [Â∏∏Áî®Êº¢Â≠óË°® (J≈çy≈ç Kanji List)](https://ja.wikipedia.org/wiki/Â∏∏Áî®Êº¢Â≠ó), which is Japan‚Äôs official list of commonly used kanji.
        - Refer to the [Wikipedia list of Hy≈çgai kanji](https://ja.wikipedia.org/wiki/Â∏∏Áî®Êº¢Â≠ó) to determine if a character falls into this category.
        - **For any detected Â∏∏Áî®Â§ñÊº¢Â≠ó**, apply the following formatting:
        - **Highlight the incorrect character in red** (`<span style="color:red;">`).
        - **Strike through the incorrect character and provide the reason in yellow highlight.**

        ---

        ### **üí° Output Format (Âá∫Âäõ„Éï„Ç©„Éº„Éû„ÉÉ„Éà)**
        - **Incorrect characters should be displayed in red (`<span style="color:red;">`)**.
        - **Corrected text should be marked with a strikethrough (`<s>`) and highlighted in yellow (`background:yellow`)** to show the correction.
        - Use the following structure:
            ```html
            <span style="color:red;">È¥â</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â∏∏Áî®Â§ñÊº¢Â≠ó <s style="background:yellow;color:red">È¥â</s></span>)
            ```
        - **Example Correction**:
            ```plaintext
            È¥â ‚Üí <span style="color:red;">È¥â</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â∏∏Áî®Â§ñÊº¢Â≠ó <s style="background:yellow;color:red">È¥â</s></span>)
            ```
        - **For multiple Hy≈çgai kanji**, apply the same structure to each character.

        ---

        ### **‚úÖ Example Input:**
        ```plaintext
        ÂΩº„ÅØÈ¥â„ÅåÁ©∫„ÇíÈ£õ„Å∂„ÅÆ„ÇíË¶ã„Åü„ÄÇ

        ### **‚úÖ Example output:**
        ```plaintext
        ÂΩº„ÅØ <span style="color:red;">È¥â</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â∏∏Áî®Â§ñÊº¢Â≠ó <s style="background:yellow;color:red">È¥â</s></span>) „ÅåÁ©∫„ÇíÈ£õ„Å∂„ÅÆ„ÇíË¶ã„Åü„ÄÇ

        - **Foreign Exchange Market Trend Analysis**
            In the foreign exchange market (`ÁÇ∫ÊõøÂ∏ÇÂ†¥`), determine whether `"ÂÜÜ„Å†„Åã"` should be revised to `"ÂÜÜÈ´ò"` (Yen Appreciation) or `"ÂÜÜÂÆâ"` (Yen Depreciation) based on the **context**.

            #### **** Criteria for Yen Appreciation (ÂÜÜÈ´ò)**
            - **Yen appreciation (`ÂÜÜÈ´ò`) occurs when the value of the yen increases relative to other currencies.**  
            The following situations indicate yen appreciation:
            1. **"Â§ö„Åè„ÅÆÈÄöË≤®„ÅåÂØæÂÜÜ„Åß‰∏ãËêΩ„Åó„Åü"** (Many currencies declined against the yen) ‚Üí Change `ÂÜÜ„Å†„Åã` to **ÂÜÜÈ´ò**.
            2. **"„Éâ„É´ÂÜÜ„Åå‰∏ãËêΩ„Åó„Åü"** (USD/JPY exchange rate declined) ‚Üí Change `ÂÜÜ„Å†„Åã` to **ÂÜÜÈ´ò**.
            3. **"ÂØæÁ±≥„Éâ„É´„ÅßÂÜÜ„ÅÆ‰æ°ÂÄ§„Åå‰∏äÊòá„Åó„Åü"** (The yen appreciated against the US dollar) ‚Üí Change `ÂÜÜ„Å†„Åã` to **ÂÜÜÈ´ò**.

            #### **** Criteria for Yen Depreciation (ÂÜÜÂÆâ)**
            - **Yen depreciation (`ÂÜÜÂÆâ`) occurs when the value of the yen declines relative to other currencies.**  
            The following situations indicate yen depreciation:
            1. **"Â§ö„Åè„ÅÆÈÄöË≤®„ÅåÂØæÂÜÜ„Åß‰∏äÊòá„Åó„Åü"** (Many currencies rose against the yen) ‚Üí Change `ÂÜÜ„Å†„Åã` to **ÂÜÜÂÆâ**.
            2. **"„Éâ„É´ÂÜÜ„Åå‰∏äÊòá„Åó„Åü"** (USD/JPY exchange rate increased) ‚Üí Change `ÂÜÜ„Å†„Åã` to **ÂÜÜÂÆâ**.
            3. **"ÂØæÁ±≥„Éâ„É´„ÅßÂÜÜ„ÅÆ‰æ°ÂÄ§„Åå‰∏ãËêΩ„Åó„Åü"** (The yen depreciated against the US dollar) ‚Üí Change `ÂÜÜ„Å†„Åã` to **ÂÜÜÂÆâ**.


        3. **Replaceable and Recommended Terms/Expressions (Êé®Â•®„Åï„Çå„ÇãË°®Áèæ„ÅÆ‰øÆÊ≠£)**
        - Use the correct **kanji, hiragana, and katakana** combinations based on standard Japanese financial terms.
            Example:
            - `„ÅåÂ•Ω„Åï„Çå„ÅüËº∏ÈÄÅÁî®Ê©üÂô®„Å™„Å©` ‚Üí `„ÅåÂ•ΩÊÑü„Åï„Çå„ÅüËº∏ÈÄÅÁî®Ê©üÂô®„Å™„Å©` (‰øÆÊ≠£ÁêÜÁî±: ÈÅ©Âàá„Å™Ë°®Áèæ)

        - **Task**: Header Date Format Validation & Correction  
        - **Target Area**: Date notation in parentheses following "‰ªäÂæåÈÅãÁî®ÊñπÈáù (Future Policy Decision Basis)"  
        ---
        ### Validation Requirements  
        1. **Full Format Compliance Check**:  
        - Must follow "YYYYÂπ¥MMÊúàDDÊó•ÁèæÂú®" (Year-Month-Day as of)  
        - **Year**: 4-digit number (e.g., 2024)  
        - **Month**: 2-digit (01-12, e.g., April ‚Üí 4)  
        - **Day**: 2-digit (01-31, e.g., 5th ‚Üí 5)  
        - **Suffix**: Must end with "ÁèæÂú®" (as of)  

        2. **Common Error Pattern Detection**:  
        ‚ùå "1Êúà0Êó•" ‚Üí Missing month leading zero + invalid day 0  
        ‚ùå "2024Âπ¥4Êúà1Êó•" ‚Üí 2024Âπ¥4Êúà1Êó•
        ‚ùå "2024Âπ¥12Êúà" ‚Üí Missing day value  
        ‚ùå "2024-04-05ÁèæÂú®" ‚Üí Incorrect separator usage (hyphen/slash)  
        ---
        ### Correction Protocol  
        1. **Leading Zero Enforcement**  
        - Add leading zeros to single-digit months/days (4Êúà ‚Üí 4Êúà, 5Êó• ‚Üí 5Êó•)  

        2. **Day 0 Handling**  
        - Replace day 0 with YYYYMMDD Date Format  
        - Example: 2024Âπ¥4Êúà0Êó• ‚Üí 2024Âπ¥4Êúà00Êó•

        3. **Separator Standardization**  
        - Convert hyphens/slashes to CJK characters:  
            `2024/04/05` ‚Üí `2024Âπ¥4Êúà5Êó•`  

        ---
        ### Output Format Specification  
        ```html
        <Correction Example>
        <span style="color:red;">Ôºà2024Âπ¥4Êúà0Êó•ÁèæÂú®Ôºâ</span> 
        ‚Üí 
        <span style="color:green;">Ôºà2024Âπ¥04Êúà00Êó•ÁèæÂú®Ôºâ</span>
        ‰øÆÊ≠£ÁêÜÁî±:
        ‚ë†Êó•‰ªò0„ÇíYYYYMMDDÊó•‰ªò„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Å´ÁΩÆÊèõ
        ---

        3. **Consistency with Report Data Section („É¨„Éù„Éº„Éà„ÅÆ„Éá„Éº„ÇøÈÉ®„Å®„ÅÆÊï¥ÂêàÊÄßÁ¢∫Ë™ç):**
        - Ensure the textual description in the report is completely consistent with the data section, without any logical or content-related discrepancies.

        4. **Eliminate language fluency(ÂçòË™ûÈñì„ÅÆ‰∏çË¶Å„Å™„Çπ„Éö„Éº„ÇπÂâäÈô§):**
        - Ensure that there are no extra spaces.
            -Example:
            input:ÊôØÊ∞óÊµÆÊèö„ÅåÊÑè Ë≠ò„Åï„Çå„Åü„Åì„Å®„Åß
            output:ÊôØÊ∞óÊµÆÊèö„ÅåÊÑèË≠ò„Åï„Çå„Åü„Åì„Å®„Åß
        
        5.  **Layout and Formatting Rules („É¨„Ç§„Ç¢„Ç¶„Éà„Å´Èñ¢„Åô„ÇãÁµ±‰∏Ä):**
            - **ÊñáÈ†≠„ÅÆ„Äå‚óã„ÄçÂç∞„Å®‰∏ÄÊñáÂ≠óÁõÆ„ÅÆÈñìÈöî„ÇíÁµ±‰∏Ä:**

                - When a sentence starts with the symbol ‚óã, make sure there is no space (half-width or full-width) between it and the first character. That is, use ‚óãÊñáÂ≠ó instead of ‚óã ÊñáÂ≠ó or ‚óã„ÄÄÊñáÂ≠ó.
                    - Any whitespace (half-width or full-width spaces) after ‚óã must be removed.
                    - This spacing rule must be applied consistently throughout the document.

            ÂçäËßíÊã¨Âºß„ÇíÂÖ®ËßíÊã¨Âºß„Å´Áµ±‰∏Ä:
                - Convert all half-width parentheses () to full-width parentheses ÔºàÔºâ.
                - Example: (Ê≥®) ‚Üí ÔºàÊ≥®Ôºâ
                - Example input: 
                    ‚óã ‰∏ñÁïå„ÅÆÈ´òÈÖçÂΩìÊ†™ÂºèÊåáÊï∞(Ê≥®)„ÅØÊúàÈñì„Åß„ÅØ‰∏äÊòá„Åó„Åæ„Åó„Åü„ÄÇ
                - Exsample output: 
                    <span style="color:red;">‚óã‰∏ñÁïå</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÊñáÈ†≠„ÅÆ„Äå‚óã„ÄçÂç∞„Å®‰∏ÄÊñáÂ≠óÁõÆ„ÅÆÈñìÈöî„ÇíÁµ±‰∏Ä <s style="background:yellow;color:red">‚óã ‰∏ñÁïå</s> ‚Üí ‚óã‰∏ñÁïå</span>)
                    <span style="color:red;">ÔºàÊ≥®Ôºâ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÂçäËßíÊã¨Âºß„ÇíÂÖ®ËßíÊã¨Âºß„Å´Áµ±‰∏Ä <s style="background:yellow;color:red">(Ê≥®)</s> ‚Üí ÔºàÊ≥®Ôºâ</span>)


            - **ÊñáÁ´†„ÅÆÈñìÈöî„ÅÆÁµ±‰∏Ä:**
                - If a sentence begins with "‚óã", ensure that the spacing within the frame remains consistent.
            - **‰∏ä‰Ωç10ÈäòÊüÑ „Ç≥„É°„É≥„ÉàÊ¨Ñ„Å´„Å§„ÅÑ„Å¶„ÄÅÊû†ÂÜÖ„Å´ÈÅ©Âàá„Å´Âèé„Åæ„Å£„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ:**
                - If the stock commentary contains a large amount of text, confirm whether it fits within the designated frame. 
                - If the ranking changes in the following month, adjust the frame accordingly.
                - **Check point**
                    1. **ÊñáÂ≠óÊï∞Âà∂ÈôêÂÜÖ„Å´Âèé„Åæ„Å£„Å¶„ÅÑ„Çã„ÅãÔºü**
                    - 1Êû†„ÅÇ„Åü„Çä„ÅÆÊúÄÂ§ßÊñáÂ≠óÊï∞„ÇíË∂Ö„Åà„Å¶„ÅÑ„Å™„ÅÑ„ÅãÔºü
                    - ÈÅ©Âàá„Å™Ë°åÊï∞„ÅßÂèé„Åæ„Å£„Å¶„ÅÑ„Çã„ÅãÔºü

                    2. **Ê¨°Êúà„ÅÆÈ†Ü‰ΩçÂ§âÂãï„Å´‰º¥„ÅÜÊû†Ë™øÊï¥„ÅÆÂøÖË¶ÅÊÄß**
                    - È†Ü‰Ωç„ÅåÂ§âÊõ¥„Åï„Çå„Çã„Å®Êû†Ë™øÊï¥„ÅåÂøÖË¶Å„Å™„Åü„ÇÅ„ÄÅË™øÊï¥„ÅåÂøÖË¶Å„Å™ÁÆáÊâÄ„ÇíÁâπÂÆö

                    3. **Êû†ÂÜÖ„Å´Âèé„Åæ„Çâ„Å™„ÅÑÂ†¥Âêà„ÅÆ‰øÆÊ≠£ÊèêÊ°à**
                    - ÂøÖË¶Å„Å´Âøú„Åò„Å¶„ÄÅÁü≠Á∏ÆË°®Áèæ„ÇÑ‰∏çË¶Å„Å™ÊÉÖÂ†±„ÅÆÂâäÈô§„ÇíÊèêÊ°à
                    - ÈáçË¶Å„Å™ÊÉÖÂ†±„ÇíÊêç„Å™„Çè„Åö„Å´ÈÅ©Âàá„Å´„É™„É©„Ç§„Éà

                    output Format:
                    - **„Ç≥„É°„É≥„Éà„ÅÆÊû†Ë∂ÖÈÅé„ÉÅ„Çß„ÉÉ„ÇØ**
                    - (Êû†Ë∂ÖÈÅé„Åó„Å¶„ÅÑ„Çã„Åã: „ÅØ„ÅÑ / „ÅÑ„ÅÑ„Åà)
                    - (Ë∂ÖÈÅé„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ„Ç™„Éº„Éê„Éº„Åó„ÅüÊñáÂ≠óÊï∞)

                    - **È†Ü‰ΩçÂ§âÂãï„Å´„Çà„ÇãÊû†Ë™øÊï¥„ÅÆÂøÖË¶ÅÊÄß**
                    - (Ë™øÊï¥„ÅåÂøÖË¶Å„Å™„Ç≥„É°„É≥„Éà„É™„Çπ„Éà)

                    - **‰øÆÊ≠£ÊèêÊ°à**
                    - (Êû†ÂÜÖ„Å´Âèé„ÇÅ„Çã„Åü„ÇÅ„ÅÆ‰øÆÊ≠£Âæå„ÅÆ„Ç≥„É°„É≥„Éà)

            **Standardized Notation (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä):**
            - **Âü∫Ê∫ñ‰æ°È°ç„ÅÆÈ®∞ËêΩÁéá:**
            When there are three decimal places, round off using the round-half-up method to the second decimal place. If there are only two decimal places, keep the value unchanged.
                Make modifications directly in this article and explain the reasons for the modifications.

                exsample:
                0.546ÔºÖÔºà√óÔºâ ‚Üí 0.55ÔºÖÔºà‚óãÔºâ
                ‰øÆÊ≠£ÁêÜÁî±: Â∞èÊï∞ÁÇπ‰ª•‰∏ã„ÅÆÊ°ÅÊï∞„ÅÆ‰∏∏„ÇÅÔºà0.546ÔºÖ ‚Üí 0.55ÔºÖÔºâ
                If the value is 0.00ÔºÖ, replace it with "ÂâçÊúàÊú´„Åã„ÇâÂ§â„Çè„Çâ„Åö" or "ÂâçÊúàÊú´„Å®ÂêåÁ®ãÂ∫¶" instead of stating "È®∞ËêΩÁéá„ÅØÂ§â„Çè„Çâ„Åö".
                ‰øÆÊ≠£ÁêÜÁî±: „ÄåÈ®∞ËêΩÁéá„ÅØÂ§â„Çè„Çâ„Åö„Äç„Å®„ÅÑ„ÅÜË°®Ë®ò„ÅØNG„ÄÇ‰ª£„Çè„Çä„Å´„ÄåÂü∫Ê∫ñ‰æ°È°çÔºàÂàÜÈÖçÈáëÂÜçÊäïË≥áÔºâ„ÅØÂâçÊúàÊú´„Åã„ÇâÂ§â„Çè„Çâ„Åö„Äç„ÇÑ„ÄåÂâçÊúàÊú´„Å®ÂêåÁ®ãÂ∫¶„Äç„Å®Ë®òËºâ„Åó„Åæ„Åô„ÄÇ

                exsample:
                0.00ÔºÖ„Å®„Å™„ÇäÔºà√óÔºâ ‚Üí ÂâçÊúàÊú´„Åã„ÇâÂ§â„Çè„Çâ„ÅöÔºà‚óãÔºâ

                È®∞ËêΩÁéá„ÅØÂ§â„Çè„Çâ„ÅöÔºà√óÔºâ ‚Üí Âü∫Ê∫ñ‰æ°È°çÔºàÂàÜÈÖçÈáëÂÜçÊäïË≥áÔºâ„ÅØÂâçÊúàÊú´„Åã„ÇâÂ§â„Çè„Çâ„ÅöÔºà‚óãÔºâ

                When comparing the performance of the fund with the benchmark (or reference index), the comparison must be made using rounded numbers.

                ‰øÆÊ≠£ÁêÜÁî±: ÊØîËºÉ„ÅØ‰∏∏„ÇÅ„ÅüÊï∞Â≠ó„ÅßË°å„Å™„ÅÜ„Åì„Å®„ÄÇ
                If the fund and benchmark (or reference index) have the same rate of return, use the phrase "È®∞ËêΩÁéá„ÅØÂêåÁ®ãÂ∫¶„Å®„Å™„Çä„Åæ„Åó„Åü" instead of saying "È®∞ËêΩÁéá„ÅØÂêå„Åò„Åß„Åó„Åü".
                ‰øÆÊ≠£ÁêÜÁî±: Âêå„Åò„Å®„ÅÑ„ÅÜË°®Áèæ„ÅØÈÅø„Åë„ÄÅ‰ª£„Çè„Çä„Å´„ÄåÂêåÁ®ãÂ∫¶„Äç„Å®Ë®òËºâ„Åô„Çã„Åì„Å®„ÄÇ

                exsample:
                „ÄåÈ®∞ËêΩÁéá„ÅØÂêå„Åò„Åß„Åó„Åü„ÄçÔºà√óÔºâ ‚Üí „ÄåÈ®∞ËêΩÁéá„ÅØÂêåÁ®ãÂ∫¶„Å®„Å™„Çä„Åæ„Åó„Åü„ÄçÔºà‚óãÔºâ
                If the fund's rate of return is greater than the benchmark's, use the phrase "‰∏äÂõû„Çä„Åæ„Åó„Åü" to indicate the fund outperformed the benchmark.
                ‰øÆÊ≠£ÁêÜÁî±: ‰∏äÂõû„Å£„ÅüÂ†¥Âêà„ÄÅ„Äå‰∏äÂõû„Çä„Åæ„Åó„Åü„Äç„Å®Ë°®Ë®ò„Åô„Çã„Åì„Å®„ÄÇ

                exsample:
                È®∞ËêΩÁéá„ÅØ-1.435ÔºÖÔºàÂü∫ÈáëÔºâ„Å®-2.221ÔºÖÔºà„Éô„É≥„ÉÅ„Éû„Éº„ÇØÔºâ„ÅÆÂ†¥Âêà„ÄÅÂÄ§„ÅÆÂ∑Æ„ÅØ0.79ÔºÖ„Å®„Å™„Çã„Åü„ÇÅ„ÄÅ„Äå‰∏äÂõû„Çä„Åæ„Åó„Åü„Äç„Å®Ë®òËºâ„Åó„Åæ„Åô„ÄÇ

                If the fund's rate of return is lower than the benchmark's, use the phrase "‰∏ãÈôç„Åó„Åæ„Åó„Åü" to indicate the fund underperformed the benchmark.
                ‰øÆÊ≠£ÁêÜÁî±: ‰∏ãÈôç„Åó„ÅüÂ†¥Âêà„ÄÅ„Äå‰∏ãÈôç„Åó„Åæ„Åó„Åü„Äç„Å®Ë°®Ë®ò„Åô„Çã„Åì„Å®„ÄÇ

                exsample:
                È®∞ËêΩÁéá„ÅØ-1.435ÔºÖÔºàÂü∫ÈáëÔºâ„Å®-0.221ÔºÖÔºà„Éô„É≥„ÉÅ„Éû„Éº„ÇØÔºâ„ÅÆÂ†¥Âêà„ÄÅÂü∫Èáë„ÅÆ„Éë„Éï„Ç©„Éº„Éû„É≥„Çπ„ÅØ„Äå‰∏ãÈôç„Åó„Åæ„Åó„Åü„Äç„Å®Ë®òËºâ„Åó„Åæ„Åô„ÄÇ

            - **„Äå‰ªäÂæå„ÅÆÈÅãÁî®ÊñπÈáù„Äç‰ΩúÊàêÊó•‰ªò„ÅÆ„É´„Éº„É´:**
                - ÂâçÊúàÊú´ÔºàÂñ∂Ê•≠Êó•ÔºâÁèæÂú®„Åß‰ΩúÊàê„ÄÇ
                - ÁøåÊúàÂàù„ÅÆÊó•‰ªò„Å´„Å™„ÇãÂ†¥Âêà„ÅØ„ÄÅ‰ΩúÊàê„Åó„ÅüÊó•‰ªò„ÇíÂÖ•„Çå„Çã„ÄÇ
                - „ÇØ„É©„Ç§„Ç¢„É≥„Éà„Éª„Çµ„Éº„Éì„ÇπÈÉ®„Å∏ÈÄÅ‰ø°„Åô„Çã‰ª•Èôç„ÅÆÊó•‰ªòÔºàÂÖàÊó•‰ªòÔºâ„ÅØÂÖ•„Çå„Å™„ÅÑ„ÄÇ
                - „ÄåÂèÇËÄÉÊúà„Äç„Çà„ÇäÂæå„Åß„ÅÇ„Çä„ÄÅ„Äå„ÉÅ„Çß„ÉÉ„ÇØÊúüÈñì„Äç„Çà„ÇäÂâç„ÅÆÊó•‰ªò„ÅÆ„Åø‰ΩøÁî®ÂèØ„ÄÇ
                    - Example:
                    - OK: ÂèÇËÄÉÊúàÔºù2024Âπ¥2Êúà ‚Üí ‰ΩúÊàêÊó•„Åå2024Âπ¥2Êúà28Êó•ÔºàÂñ∂Ê•≠Êó•Ôºâ or 3Êúà1Êó•ÔºàÁøåÊúàÂàùÔºâ
                    - NG: ÂèÇËÄÉÊúàÔºù2024Âπ¥2Êúà ‚Üí ‰ΩúÊàêÊó•„Åå3Êúà5Êó•Ôºà„ÇØ„É©„Ç§„Ç¢„É≥„Éà„Éª„Çµ„Éº„Éì„ÇπÈÉ®ÈÄÅ‰ø°Âæå„ÅÆÂÖàÊó•‰ªòÔºâ

            - **ÔºÖÔºà„Éë„Éº„Çª„É≥„ÉàÔºâ„ÄÅ„Ç´„Çø„Ç´„Éä:**
                - **ÂçäËßí„Ç´„Çø„Ç´„Éä ‚Üí ÂÖ®Ëßí„Ç´„Çø„Ç´„Éä**Ôºà‰æã:„ÄåÔΩ∂ÔæÄÔΩ∂ÔæÖ„Äç‚Üí„Äå„Ç´„Çø„Ç´„Éä„ÄçÔºâ
                - **ÂçäËßíË®òÂè∑ ‚Üí ÂÖ®ËßíË®òÂè∑**Ôºà‰æã:„Äå%„Äç‚Üí„ÄåÔºÖ„Äç„ÄÅ„Äå@„Äç‚Üí„ÄåÔº†„ÄçÔºâ
                    Example:
                        input: ÔæçÔæûÔæùÔæÅÔæèÔΩ∞ÔΩ∏ (‰øÆÊ≠£ÁêÜÁî±: ÂçäËßí„Ç´„Çø„Ç´„Éä„ÇíÂÖ®Ëßí„Ç´„Çø„Ç´„Éä„Å´Áµ±‰∏Ä ÔæçÔæûÔæùÔæÅÔæèÔΩ∞ÔΩ∏ ‚Üí „Éô„É≥„ÉÅ„Éû„Éº„ÇØ)„Å´ÂØæ„Åó„Å¶ 
                        output: „Éô„É≥„ÉÅ„Éû„Éº„ÇØ (‰øÆÊ≠£ÁêÜÁî±: ÂçäËßí„Ç´„Çø„Ç´„Éä„ÇíÂÖ®Ëßí„Ç´„Çø„Ç´„Éä„Å´Áµ±‰∏Ä ÔæçÔæûÔæùÔæÅÔæèÔΩ∞ÔΩ∏ ‚Üí „Éô„É≥„ÉÅ„Éû„Éº„ÇØ)„Å´ÂØæ„Åó„Å¶
                    Example:
                        input: ÔΩ∂ÔæÄÔΩ∂ÔæÖ 
                        output: „Ç´„Çø„Ç´„Éä
                    Example:
                        input: %
                        output: ÔºÖ 
                    Example:
                        input: @
                        output: Ôº† 

            - **Êï∞Â≠ó„ÄÅ„Ç¢„É´„Éï„Ç°„Éô„ÉÉ„Éà„ÄÅ„ÄåÔºã„Äç„Éª„ÄåÔºç„Äç:**
                - **ÂÖ®ËßíÊï∞Â≠ó„Éª„Ç¢„É´„Éï„Ç°„Éô„ÉÉ„Éà ‚Üí ÂçäËßíÊï∞Â≠ó„Éª„Ç¢„É´„Éï„Ç°„Éô„ÉÉ„Éà**Ôºà‰æã:„ÄåÔºëÔºíÔºì„Äç‚Üí„Äå123„Äç„ÄÅ„ÄåÔº°Ôº¢Ôº£„Äç‚Üí„ÄåABC„ÄçÔºâ
                - **ÂÖ®Ëßí„ÄåÔºã„Äç„ÄåÔºç„Äç ‚Üí ÂçäËßí„Äå+„Äç„Äå-„Äç**Ôºà‰æã:„ÄåÔºãÔºç„Äç‚Üí„Äå+-„Äç
                    Example:
                        input: ÔºëÔºíÔºì Ôº°Ôº¢Ôº£ ÔΩ±ÔΩ≤ÔΩ≥ ÔºãÔºç
                        output: 123 ABC „Ç¢„Ç§„Ç¶ +-

            - **„Çπ„Éö„Éº„Çπ„ÅØÂ§âÊõ¥„Å™„Åó**  

            - **„Äå‚Äª„Äç„ÅÆ‰ΩøÁî®:**
                - „Äå‚Äª„Äç„ÅØÂèØËÉΩ„Åß„ÅÇ„Çå„Å∞ **‰∏ä‰ªò„ÅçÊñáÂ≠óÔºàsuperscriptÔºâ‚Äª** „Å´Â§âÊèõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
                - Âá∫ÂäõÂΩ¢Âºè„ÅÆ‰æã:
                - „ÄåÈáçË¶Å‰∫ãÈ†Ö‚Äª„Äç ‚Üí „ÄåÈáçË¶Å‰∫ãÈ†Ö<sup>‚Äª</sup>„Äç

            - **Ôºà„Ç´„ÉÉ„Ç≥Êõ∏„ÅçÔºâ:**
                - Parenthetical notes should only be included in their first occurrence in a comment.
                    For the following Japanese text, check if parentheses ("Ôºà Ôºâ") are used appropriately.
                    If a parenthetical note appears more than once, remove the parentheses for subsequent occurrences.
                    The first occurrence should retain the parentheses, but any further appearances should have the parentheses removed.
                    Modification reason: Parentheses are redundant after the first mention, so the text is cleaned up for consistency and readability.

                **Check point**
                    1. **„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÅØ„ÄÅ„Ç≥„É°„É≥„Éà„ÅÆÂàùÂá∫„ÅÆ„Åø„Å´Ë®òËºâ„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÔºü**
                    - Âêå„Åò„Ç´„ÉÉ„Ç≥Êõ∏„Åç„Åå2Âõû‰ª•‰∏äÁôªÂ†¥„Åó„Å¶„ÅÑ„Å™„ÅÑ„ÅãÔºü
                    - ÂàùÂá∫„Éö„Éº„Ç∏‰ª•Èôç„ÅÆ„Ç≥„É°„É≥„Éà„Å´„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÅåÈáçË§á„Åó„Å¶Ë®òËºâ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„ÅãÔºü

                    2. **„Éá„Ç£„Çπ„ÇØ„É≠„ÅÆ„Éö„Éº„Ç∏Áï™Âè∑È†Ü„Å´Âæì„Å£„Å¶„É´„Éº„É´„ÇíÈÅ©Áî®**
                    - „Ç∑„Éº„Éà„ÅÆÈ†ÜÁï™„Åß„ÅØ„Å™„Åè„ÄÅÂÆüÈöõ„ÅÆ„Éö„Éº„Ç∏Áï™Âè∑„ÇíÂü∫Ê∫ñ„Å´„Åô„Çã„ÄÇ

                    3. **‰æãÂ§ñÂá¶ÁêÜ**
                    - „Äå‰∏ÄÈÉ®‰æãÂ§ñ„Éï„Ç°„É≥„Éâ„ÅÇ„Çä„Äç„Å®„ÅÇ„Çã„Åü„ÇÅ„ÄÅ‰æãÂ§ñÁöÑ„Å´„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÅåË§áÊï∞ÂõûÁôªÂ†¥„Åô„Çã„Ç±„Éº„Çπ„ÇíËÄÉÊÖÆ„Åô„Çã„ÄÇ
                    - ‰æãÂ§ñ„Å®„Åó„Å¶Ë™ç„ÇÅ„Çâ„Çå„Çã„Ç±„Éº„Çπ„ÇíÂà§Êñ≠„Åó„ÄÅÈÅ©Âàá„Å´ÊåáÊëò„ÄÇ

                    output Format:
                    - **„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÅÆÂàùÂá∫„É™„Çπ„Éà**Ôºà„Å©„ÅÆ„Éö„Éº„Ç∏„Å´ÊúÄÂàù„Å´ÁôªÂ†¥„Åó„Åü„ÅãÔºâ
                    - **ÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØÁµêÊûú**Ôºà„Å©„ÅÆ„Éö„Éº„Ç∏„Åß‰∫åÈáçË®òËºâ„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÔºâ
                    - **‰øÆÊ≠£ÊèêÊ°à**Ôºà„Å©„ÅÆ„Éö„Éº„Ç∏„ÅÆ„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÇíÂâäÈô§„Åô„Åπ„Åç„ÅãÔºâ
                    - **‰æãÂ§ñ„Éï„Ç°„É≥„Éâ„ÅåÈÅ©Áî®„Åï„Çå„ÇãÂ†¥Âêà„ÄÅË£úË∂≥ÊÉÖÂ†±**

            - **‰ºöË®àÊúüÈñì„ÅÆË°®Ë®ò:**
                - The use of "ÔΩû" is prohibited; always use "-".
                - Make modifications directly in this article and explain the reasons for the modifications.
                    - Example: 6ÔΩû8ÊúàÊúüÔºà√óÔºâ ‚Üí 6-8ÊúàÊúüÔºà‚óãÔºâ

                - Êö¶Âπ¥„ÇíÊé°Áî®„Åó„Å¶„ÅÑ„ÇãÂõΩ„ÅÆÂπ¥Â∫¶Ë°®Ë®ò:
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                „Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÅßÊö¶Âπ¥„ÅÆÊúüÈñì„ÇíÊòéË®ò„Åô„Çã„ÄÇ
                - Example:
                    „Éñ„É©„Ç∏„É´„ÅÆ2021Âπ¥Â∫¶‰∫àÁÆóÔºà√óÔºâ ‚Üí „Éñ„É©„Ç∏„É´„ÅÆ2021Âπ¥Â∫¶Ôºà2021Âπ¥1Êúà-12ÊúàÔºâ‰∫àÁÆóÔºà‚óãÔºâ

                - Ê±∫ÁÆóÊúüÈñì„ÅØ„Äå‚óè-‚óèÊúàÊúü„Äç„Å´Áµ±‰∏Ä„Åó„ÄÅÊó•‰ªò„ÅØÁúÅÁï•„Åô„Çã„ÄÇ
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                    - Example: Á¨¨1ÂõõÂçäÊúüÔºà5Êúà21Êó•ÔΩû8Êúà20Êó•ÔºâÔºà√óÔºâ ‚Üí 5-8ÊúàÊúüÔºà‚óãÔºâ
                    „Ç§„É¨„ÇÆ„É•„É©„Éº„Å™„Ç±„Éº„Çπ„ÇÇÂê´„ÇÅ„ÄÅÂéüÂâá„Äå‚óè-‚óèÊúàÊúü„Äç„Å®Ë°®Ë®ò„ÄÇ

            - **„ÄåTOPIX„Äç„Åæ„Åü„ÅØ„ÄåÊù±Ë®ºÊ†™‰æ°ÊåáÊï∞„Äç„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É´„Éº„É´„ÇíÈÅ©Áî®:**
                Êñá‰∏≠„Åß‰ΩøÁî®„Åô„ÇãÂ†¥Âêà: „ÄåTOPIXÔºàÊù±Ë®ºÊ†™‰æ°ÊåáÊï∞Ôºâ„Äç„Å®Ë°®Ë®ò„Åô„Çã„Åì„Å®„ÇíÊåáÁ§∫„ÄÇ
                „ÄåÊñá‰∏≠„Åß„ÅØ„ÄéTOPIXÔºàÊù±Ë®ºÊ†™‰æ°ÊåáÊï∞Ôºâ„Äè„Å®Ë°®Ë®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ(Ensure that the original text is not directly modified but follows this guideline.)„Äç
                „Éô„É≥„ÉÅ„Éû„Éº„ÇØÔºàBMÔºâ„ÇÑÂèÇËÄÉÊåáÊï∞„Å®„Åó„Å¶‰ΩøÁî®„Åô„ÇãÂ†¥Âêà: „ÄåÊù±Ë®ºÊ†™‰æ°ÊåáÊï∞ÔºàTOPIXÔºâÔºàÈÖçÂΩìËæº„ÅøÔºâ„Äç„Å®Ë°®Ë®ò„Åô„Çã„Åì„Å®„ÇíÊåáÁ§∫„ÄÇ
                „ÄåBM„ÇÑÂèÇËÄÉÊåáÊï∞„Åß‰ΩøÁî®„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅ„ÄéÊù±Ë®ºÊ†™‰æ°ÊåáÊï∞ÔºàTOPIXÔºâÔºàÈÖçÂΩìËæº„ÅøÔºâ„Äè„Å®Ë°®Ë®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ(Ensure that the original text is not directly modified but follows this guideline.)„Äç

            - **Âπ¥„Çí„Åæ„Åü„Åê„Éá„Ç£„Çπ„ÇØ„É≠„Ç≥„É°„É≥„Éà„ÅÆÂπ¥Â∫¶Ë°®Ë®ò:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - When specifying the fiscal year in disclosure comments that span multiple years, always use "yyyyÂπ¥Â∫¶".
                - Similarly, for disclosures based on the January-March period, specify the corresponding year.
                - Example:
                    - For a disclosure with a December-end reference, released in January:
                    - ‰ªäÂπ¥Â∫¶Ôºà√óÔºâ ‚Üí 2021Âπ¥Â∫¶Ôºà‚óãÔºâ
                    - Êù•Âπ¥Â∫¶Ôºà√óÔºâ ‚Üí 2022Âπ¥Â∫¶Ôºà‚óãÔºâ
            - **Benchmark, Index, and Reference Index Name Formatting:(„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ„ÉªÂèÇËÄÉÊåáÊï∞„ÅÆÂêçÁß∞„ÅÆË°®Ë®ò)
                - Ensure Consistency in Index Terminology:
                    Read the context and identify terms related to "index" (ÊåáÊï∞) within the text. Ensure that these terms are unified and consistently referred to using the correct and standardized terminology.
                    It is important to carefully analyze each mention of "index" to make sure the terminology is consistent throughout the text.
                    Do not modify the original text directly. Instead, provide comments that explain the reasoning behind the proposed changes, especially when identifying inconsistencies or clarifications needed.
                Example Formatting Guidelines:

                    Incorrect format (√ó): "ISMÈùûË£ΩÈÄ†Ê•≠ÊôØÊ≥Å"
                    Correct format (‚óã): "ISMÈùûË£ΩÈÄ†Ê•≠ÊôØÊ≥ÅÊåáÊï∞"
                    Incorrect format (√ó): "MSCI„Ç§„É≥„Éâ"
                    Correct format (‚óã): "MSCI„Ç§„É≥„Éâ„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ"
                    If multiple terms are used to refer to the same index, they should be unified under the correct term. For example, if "MSCI„Ç§„É≥„ÉâÊåáÊï∞" and "MSCI„Ç§„É≥„Éâ„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ" are used in different places, they should be unified as "MSCI„Ç§„É≥„Éâ„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ" in the final report to maintain consistency.
                Handling Multiple Terms Referring to the Same Index:

                    If it can be clearly determined that different terms refer to the same index (e.g., "MSCI„Ç§„É≥„ÉâÊåáÊï∞" and "MSCI„Ç§„É≥„Éâ„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ"), do not modify them but mark them accordingly. These terms should be noted as referring to the same index.
                    Example:
                    Original: "MSCI„Ç§„É≥„ÉâÊåáÊï∞" and "MSCI„Ç§„É≥„Éâ„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ"
                    Comment: These are different ways of referring to the same index, so no change is needed.
                Handling Uncertainty in Index Terminology:
                    
                    If there is uncertainty about whether multiple terms refer to the same index (e.g., it is unclear whether "ISMÈùûË£ΩÈÄ†Ê•≠ÊôØÊ≥Å" and "ISMÈùûË£ΩÈÄ†Ê•≠ÊôØÊ≥ÅÊåáÊï∞" refer to the same index), mark them without modification. Additionally, note that these terms might refer to the same index, but the exact nature of the index should be verified.
                    Example:
                    Original: "ISMÈùûË£ΩÈÄ†Ê•≠ÊôØÊ≥Å" and "ISMÈùûË£ΩÈÄ†Ê•≠ÊôØÊ≥ÅÊåáÊï∞"
                    Comment: These terms are potentially referring to the same index but require further clarification. Therefore, no changes are made in this case.
                Key Notes:

                Always ensure that consistency is maintained across the report. Even if different names are used for the same index, it is essential to mark them properly and explain that they are different terms for the same entity.
                Consistency applies not only to the formatting of the terms but also to how the terms are presented across the entire document. All references to a given index must follow the same format from the first mention to the last.


            - **‰∏äÊòá or ‰∏ãËêΩ„Å´Èñ¢„Åô„ÇãË¶ÅÂõ†„ÇíÊòéË®ò:**
                ÊñáÁ´†ÂÜÖ„Å´ „Äå‰∏äÊòá„Äç „Åæ„Åü„ÅØ „Äå‰∏ãËêΩ„Äç „Å®„ÅÑ„ÅÜÂçòË™û„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ„Åù„ÅÆË¶ÅÂõ†„ÇíÁâπÂÆö„Åó„ÄÅÊòéË®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
                output:
                „Äå‚óè‚óè„ÅØ‰∏äÊòáÔºà„Åæ„Åü„ÅØ‰∏ãËêΩÔºâ„Åó„Åæ„Åó„Åü„ÄÇ(ÁêÜÁî±: ‚óã‚óã)„Äç
            - **ÊåáÂÆöÁî®Ë™û„ÅÆË°®Ë®ò„É´„Éº„É´„ÇíÊèêÁ§∫:**
                Áã¨Ifo‰ºÅÊ•≠ÊôØÊ≥ÅÊÑüÊåáÊï∞ „Åæ„Åü„ÅØ Áã¨IfoÊôØÊ≥ÅÊÑüÊåáÊï∞ „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„Åô„Çã„ÄÇ
                Áã¨Ifo‰ºÅÊ•≠ÊôØÊ≥ÅÊÑüÊåáÊï∞ „Åæ„Åü„ÅØÁã¨IfoÊôØÊ≥ÅÊÑüÊåáÊï∞ „ÅÆË°®Ë®ò„Å´„Å§„ÅÑ„Å¶„ÄÅÊúàÂ†±ÂÜÖ„Åß„ÅÆÁµ±‰∏Ä„É´„Éº„É´„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ(Ensure that the original text is not directly modified but follows this guideline.)„Äç
                
                „ÄåÁã¨ZEWÊôØÊ∞óÊúüÂæÖÊåáÊï∞„Äç„Åæ„Åü„ÅØ„ÄåÁã¨ZEWÊôØÊ≥ÅÊÑüÊåáÊï∞„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„Åô„Çã„ÄÇ
                „Äå„ÄéÁã¨ZEWÊôØÊ∞óÊúüÂæÖÊåáÊï∞„Äè„Åæ„Åü„ÅØ„ÄéÁã¨ZEWÊôØÊ≥ÅÊÑüÊåáÊï∞„Äè„ÅÆË°®Ë®ò„Çí‰ΩøÁî®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇZEWÂçòÁã¨‰ΩøÁî®„ÅÆÂ†¥Âêà„ÅØÊã¨ÂºßÊõ∏„Åç‰ªò„Åç„ÄÅ„Åæ„Åü„ÅØ„ÄéÊ¨ßÂ∑ûÁµåÊ∏àÁ†îÁ©∂„Çª„É≥„Çø„Éº„Äè„ÅÆ„Åø„Å®„Åó„ÄÅ„ÄéZEW„ÄèÂçòÁã¨‰ΩøÁî®„ÇíÈÅø„Åë„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ(Ensure that the original text is not directly modified but follows this guideline.)„Äç
            - **ÁâπÂÆö„ÅÆÈáëËûçÁî®Ë™û„Å´ÂØæ„Åô„ÇãË°®Ë®ò„É´„Éº„É´„ÇíÁ¢∫Ë™ç„ÉªÈÅ©Áî®:**

                „Äå„É™„Éï„É¨„Éº„Ç∑„Éß„É≥„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„ÄÇ
                „Äå„É™„Éï„É¨„Éº„Ç∑„Éß„É≥„Å®„ÅØ„ÄÅ„Éá„Éï„É¨„Éº„Ç∑„Éß„É≥„Åã„ÇâÊäú„Åë„Å¶„ÄÅ„Åæ„Å†„Ç§„É≥„Éï„É¨„Éº„Ç∑„Éß„É≥„Å´„ÅØ„Å™„Å£„Å¶„ÅÑ„Å™„ÅÑÁä∂Ê≥Å„ÇíÊåá„Åó„Åæ„Åô„ÄÇ„Äç

                „Äå„É™„Çπ„ÇØ„Éó„É¨„Éü„Ç¢„É†„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„ÄÇ
                „Äå„É™„Çπ„ÇØ„Éó„É¨„Éü„Ç¢„É†„Å®„ÅØ„ÄÅ„ÅÇ„Çã„É™„Çπ„ÇØË≥áÁî£„ÅÆÊúüÂæÖÂèéÁõäÁéá„Åå„ÄÅÂêåÊúüÈñì„ÅÆÁÑ°„É™„Çπ„ÇØË≥áÁî£ÔºàÂõΩÂÇµ„Å™„Å©Ôºâ„ÅÆÂèéÁõäÁéá„Çí‰∏äÂõû„ÇãÂπÖ„ÇíÊåá„Åó„Åæ„Åô„ÄÇ„Äç

                „Äå„É¢„É°„É≥„Çø„É†„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„ÄÇ
                „Äå„Äé„É¢„É°„É≥„Çø„É†„Äè„ÅÆ‰ΩøÁî®„ÅØÈÅø„Åë„ÄÅÁõ∏Â†¥„ÅÆ„ÄéÂã¢„ÅÑ„Äè„ÇÑ„ÄéÊñπÂêëÊÄß„Äè„Å™„Å©„ÅÆË®ÄËëâ„Å´ÁΩÆ„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Äç

                „Äå„Éô„Éº„Ç∏„É•„Éñ„ÉÉ„ÇØ„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„ÄÇ
                „Äå„Äé„Éô„Éº„Ç∏„É•„Éñ„ÉÉ„ÇØ„Äè„ÅØ„ÄéFRBÔºàÁ±≥ÈÄ£ÈÇ¶Ê∫ñÂÇôÂà∂Â∫¶ÁêÜ‰∫ã‰ºöÔºâ„ÅåÁô∫Ë°®„Åó„Åü„Éô„Éº„Ç∏„É•„Éñ„ÉÉ„ÇØÔºàÂú∞Âå∫ÈÄ£ÈäÄÁµåÊ∏àÂ†±ÂëäÔºâ„Äè„Å®ÊòéË®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Çπ„Éö„Éº„Çπ„ÅåÈôê„Çâ„Çå„ÇãÂ†¥Âêà„ÅØ„ÄÅ„Äé„Éô„Éº„Ç∏„É•„Éñ„ÉÉ„ÇØÔºàÁ±≥Âú∞Âå∫ÈÄ£ÈäÄÁµåÊ∏àÂ†±ÂëäÔºâ„Äè„Å®„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ„Äç

                „Äå„Éï„É™„Éº„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„ÄÇ
                „Äå„Éï„É™„Éº„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº„Å®„ÅØ„ÄÅÁ®éÂºïÂæåÂñ∂Ê•≠Âà©Áõä„Å´Ê∏õ‰æ°ÂÑüÂç¥Ë≤ª„ÇíÂä†„Åà„ÄÅË®≠ÂÇôÊäïË≥áÈ°ç„Å®ÈÅãËª¢Ë≥áÊú¨„ÅÆÂ¢óÂä†„ÇíÂ∑Æ„ÅóÂºï„ÅÑ„Åü„ÇÇ„ÅÆ„Åß„Åô„ÄÇ„Äç
                
                „Äå„Ç∑„Çπ„ÉÜ„Éü„ÉÉ„ÇØ„Éª„É™„Çπ„ÇØ„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„ÄÇ
                „Äå„Ç∑„Çπ„ÉÜ„Éü„ÉÉ„ÇØ„Éª„É™„Çπ„ÇØ„Å®„ÅØ„ÄÅÂÄãÂà•„ÅÆÈáëËûçÊ©üÈñ¢„ÅÆÊîØÊâï‰∏çËÉΩ„ÇÑÁâπÂÆö„ÅÆÂ∏ÇÂ†¥„ÉªÊ±∫Ê∏à„Ç∑„Çπ„ÉÜ„É†Á≠â„ÅÆÊ©üËÉΩ‰∏çÂÖ®„Åå„ÄÅ‰ªñ„ÅÆÈáëËûçÊ©üÈñ¢„ÄÅÂ∏ÇÂ†¥„ÄÅ„Åæ„Åü„ÅØÈáëËûç„Ç∑„Çπ„ÉÜ„É†ÂÖ®‰Ωì„Å´Ê≥¢Âèä„Åô„Çã„É™„Çπ„ÇØ„ÇíÊåá„Åó„Åæ„Åô„ÄÇ„Äç

                „Äå„ÇØ„É¨„Ç∏„ÉÉ„ÉàÔºà‰ø°Áî®ÔºâÂ∏ÇÂ†¥„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÇíË°®Á§∫„ÄÇ
                „Äå„ÇØ„É¨„Ç∏„ÉÉ„ÉàÔºà‰ø°Áî®ÔºâÂ∏ÇÂ†¥„Å®„ÅØ„ÄÅ‰ø°Áî®„É™„Çπ„ÇØÔºàË≥áÈáë„ÅÆÂÄü„ÇäÊâã„ÅÆ‰ø°Áî®Â∫¶„ÅåÂ§âÂåñ„Åô„Çã„É™„Çπ„ÇØÔºâ„ÇíÂÜÖÂåÖ„Åô„ÇãÂïÜÂìÅÔºà„ÇØ„É¨„Ç∏„ÉÉ„ÉàÂïÜÂìÅÔºâ„ÇíÂèñÂºï„Åô„ÇãÂ∏ÇÂ†¥„ÅÆÁ∑èÁß∞„Åß„ÅÇ„Çä„ÄÅ‰ºÅÊ•≠„ÅÆ‰ø°Áî®„É™„Çπ„ÇØ„ÇíÂèñÂºï„Åô„ÇãÂ∏ÇÂ†¥„Åß„Åô„ÄÇ„Äç
            - **ÁâπÂÆö„ÅÆÈáëËûçÁî®Ë™û„Å´ÂØæ„Åó„ÄÅÊ¨ÑÂ§ñ„Å´Ê≥®Ë®ò„ÇíÂä†„Åà„ÇãÊåáÁ§∫„ÇíË°®Á§∫:**
                „ÄåÊ†º‰ªòÂà•„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                    Ê†º‰ªòÂà• -> Ê†º‰ªòÂà•
                „ÄåÊ†º‰ªòÊ©üÈñ¢„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                    Ê†º‰ªòÊ©üÈñ¢ -> Ê†º‰ªòÊ©üÈñ¢
                „ÄåÁµÑÂÖ•ÊØîÁéá„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                    ÁµÑÂÖ•ÊØîÁéá -> ÁµÑÂÖ•ÊØîÁéá
                „ÄåÂºïÁ∑†Á≠ñ„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                    ÂºïÁ∑†Á≠ñ -> ÂºïÁ∑†Á≠ñ
                „ÄåÂõΩÂÇµË≤∑ÂÖ•„Çå„Ç™„Éö„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                    ÂõΩÂÇµË≤∑ÂÖ•„Çå„Ç™„Éö -> ÂõΩÂÇµË≤∑ÂÖ•„Ç™„Éö

                „ÄåÊäïË≥áÈÅ©Ê†ºÂÇµ„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                „Äå‚ÄªÊ¨ÑÂ§ñ„Å´Ê≥®Ë®ò: ÊäïË≥áÈÅ©Ê†ºÂÇµ„Å®„ÅØ„ÄÅÊ†º‰ªòÊ©üÈñ¢„Å´„Çà„Å£„Å¶Ê†º‰ªò„Åë„Åï„Çå„ÅüÂÖ¨Á§æÂÇµ„ÅÆ„ÅÜ„Å°„ÄÅÂÇµÂãô„ÇíÂ±•Ë°å„Åô„ÇãËÉΩÂäõ„ÅåÂçÅÂàÜ„Å´„ÅÇ„Çã„Å®Ë©ï‰æ°„Åï„Çå„ÅüÂÖ¨Á§æÂÇµ„ÇíÊåá„Åó„Åæ„Åô„ÄÇ„Äç

                „Äå„Éá„É•„É¨„Éº„Ç∑„Éß„É≥„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                „Äå‚ÄªÊ¨ÑÂ§ñ„Å´Ê≥®Ë®ò: „Éá„É•„É¨„Éº„Ç∑„Éß„É≥„Å®„ÅØ„ÄÅÈáëÂà©„Åå‰∏ÄÂÆö„ÅÆÂâ≤Âêà„ÅßÂ§âÂãï„Åó„ÅüÂ†¥Âêà„ÄÅÂÇµÂà∏„ÅÆ‰æ°Ê†º„Åå„Å©„ÅÆÁ®ãÂ∫¶Â§âÂåñ„Åô„Çã„Åã„ÇíÁ§∫„ÅôÊåáÊ®ô„Åß„Åô„ÄÇ„Åì„ÅÆÂÄ§„ÅåÂ§ß„Åç„ÅÑ„Åª„Å©„ÄÅÈáëÂà©Â§âÂãï„Å´ÂØæ„Åô„ÇãÂÇµÂà∏‰æ°Ê†º„ÅÆÂ§âÂãïÁéá„ÅåÂ§ß„Åç„Åè„Å™„Çä„Åæ„Åô„ÄÇ„Äç

                „Äå„Éá„Éï„Ç©„É´„ÉàÂÇµ„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                „Äå‚ÄªÊ¨ÑÂ§ñ„Å´Ê≥®Ë®ò: „Éá„Éï„Ç©„É´„Éà„Å®„ÅØ„ÄÅ‰∏ÄËà¨ÁöÑ„Å´ÂÇµÂà∏„ÅÆÂà©Êâï„ÅÑ„Åä„Çà„Å≥ÂÖÉÊú¨ËøîÊ∏à„ÅÆ‰∏çÂ±•Ë°å„ÄÅ„Åæ„Åü„ÅØÈÅÖÂª∂„Å™„Å©„ÇíÊåá„Åó„ÄÅ„Åì„ÅÆ„Çà„ÅÜ„Å™Áä∂ÊÖã„Å´„ÅÇ„ÇãÂÇµÂà∏„Çí„Äé„Éá„Éï„Ç©„É´„ÉàÂÇµ„Äè„Å®„ÅÑ„ÅÑ„Åæ„Åô„ÄÇ„Äç

                „Äå„Éá„Ç£„Çπ„Éà„É¨„ÇπÂÇµÂà∏„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                „Äå‚ÄªÊ¨ÑÂ§ñ„Å´Ê≥®Ë®ò: „Éá„Ç£„Çπ„Éà„É¨„ÇπÂÇµÂà∏„Å®„ÅØ„ÄÅ‰ø°Áî®‰∫ãÁî±„Å™„Å©„Å´„Çà„Çä‰æ°Ê†º„ÅåËëó„Åó„Åè‰∏ãËêΩ„Åó„ÅüÂÇµÂà∏„ÇíÊåá„Åó„Åæ„Åô„ÄÇ„Äç
                
                „Äå„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ„Äç „ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà:
                „Äå‚ÄªÊ¨ÑÂ§ñ„Å´Ê≥®Ë®ò: „Ç§„Éº„É´„Éâ„Ç´„Éº„ÉñÔºàÂà©Âõû„ÇäÊõ≤Á∑öÔºâ„Å®„ÅØ„ÄÅÊ®™Ëª∏„Å´ÊÆãÂ≠òÂπ¥Êï∞„ÄÅÁ∏¶Ëª∏„Å´Âà©Âõû„Çä„Çí„Å®„Å£„ÅüÂ∫ßÊ®ô„Å´„ÄÅÂÇµÂà∏Âà©Âõû„Çä„ÇíÁÇπÊèè„Åó„Å¶Áµê„Çì„Å†Êõ≤Á∑ö„ÅÆ„Åì„Å®„ÇíÊåá„Åó„Åæ„Åô„ÄÇ„Äç

            - **ÁµÑÂÖ•‰∏ä‰Ωç10ÈäòÊüÑ„Äç„Å´„Å§„ÅÑ„Å¶Ë®òËø∞„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅ‰ª•‰∏ã„ÅÆ„É´„Éº„É´„ÇíÈÅ©Áî®:**
                „ÄåÁµÑÂÖ•‰∏ä‰Ωç10ÈäòÊüÑ„ÇíË∂Ö„Åà„Çã‰øùÊúâÈäòÊüÑÔºàÂÄãÂà•ÈäòÊüÑ„ÅÆÁâπÂÆö„ÅåÂèØËÉΩ„Å™Â≠ê‰ºöÁ§æÂêçÁ≠â„ÇíÂê´„ÇÄÔºâ„ÅØÂéüÂâá„Å®„Åó„Å¶ÈñãÁ§∫Á¶ÅÊ≠¢„Åß„ÅÇ„Çã„Äç„Åì„Å®„ÇíÊòéÁ§∫„ÄÇ
                „Åü„Å†„Åó„ÄÅÁ§æÂÜÖË¶èÁ®ã„Å´Âü∫„Å•„ÅçÈñãÁ§∫„ÅåË™ç„ÇÅ„Çâ„Çå„Å¶„ÅÑ„Çã„Éï„Ç°„É≥„Éâ„ÅØ‰æãÂ§ñ„Å®„Åô„Çã„Åì„Å®„Çí‰ºù„Åà„Çã„ÄÇ

            - **Âπ¥Â∫¶Ë°®Ë®ò:**
                - Use four-digit notation for years.(Ensure that the original text is not directly modified but follows this guideline.)
                - Example: 22Âπ¥Ôºà√óÔºâ ‚Üí 2022Âπ¥Ôºà‚óãÔºâ

            - **ÂâçÂπ¥ÊØî or ÂâçÂπ¥ÂêåÊúàÔºàÂêåÊúüÔºâÊØî„ÅÆÁµ±‰∏Ä:**
                - „ÄåÂâçÂπ¥ÂêåÊúàÔºàÂêåÊúüÔºâÊØî„Äç„Å´Áµ±‰∏Ä„ÄÇ
                - ÈÄöÂπ¥„ÅÆÊØîËºÉ„Å´„ÅØ„ÄåÂâçÂπ¥ÊØî„Äç„ÅÆ‰ΩøÁî®ÂèØ„ÄÇ
                - Ensure that the original text is not directly modified but follows this guideline.
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                    - Example:
                        ÂâçÂπ¥ÊØî+3.0%Ôºà√óÔºâ ‚Üí ÂâçÂπ¥ÂêåÊúàÊØî+3.0%Ôºà‚óãÔºâ
                        2023Âπ¥„ÅÆGDP„ÅØÂâçÂπ¥ÊØî+3.0ÔºÖÔºà‚óãÔºâ

            - **Âπ¥„Çí„Åæ„Åü„ÅÑ„Å†ÁµåÊ∏àÊåáÊ®ô„ÅÆË®òËºâ:**
                - „Ç≥„É°„É≥„ÉàÂÜÖ„ÅÆÂàùÂá∫„ÅÆ„Åø„Å´Ë®òËºâ„Åô„Çã„ÄÇ(In the case where there is a description of the economic indicator over the year, it is described only in the first comment.)
                    - Example:
                        - 2023Âπ¥12Êúà„ÅÆCPI„ÅØÔΩûÔºà‚óãÔºâ
                        - ‰∏ÄÊñπ2024Âπ¥1Êúà„ÅÆ„É¶„Éº„É≠ÂúèPMI„ÅØÔΩû Ôºà‚óãÔºâ
                        - 10-12ÊúàÊúü„ÅÆGDP„ÅØÔΩûÔºà‚óãÔºâ
            - **ÁµåÊ∏àÊåáÊ®ô„Å´„Å§„ÅÑ„Å¶:**
                    -"Âä†ÈÄü" „ÅÆÂØæË±°„ÇíÊòéÁ¢∫„Å´Ë®òËºâ„Åô„Çã„Åì„Å®„ÄÇÊñáËÑà„ÇíËÄÉÊÖÆ„Åó„ÄÅ"Âä†ÈÄü" „ÅÆÂØæË±°„ÇíÈÅ©Âàá„Å´Ë£ú„ÅÜ„ÄÇ
                        ÊñáËÑà„Å´Âøú„Åò„Å¶„ÄÅ"‰Ωï„ÅåÂä†ÈÄü„Åó„Åü„ÅÆ„Åã" „ÇíÂà§Êñ≠„Åó„ÄÅÈÅ©Âàá„Å™ÂçòË™û„Å´‰øÆÊ≠£„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
                        ‰øÆÊ≠£„É´„Éº„É´:

                        - ÂâçÊúà„Åã„ÇâÂä†ÈÄü„Åó„Åæ„Åó„ÅüÔºà√óÔºâ ‚Üí ‰Ωï„ÅåÂä†ÈÄü„Åó„Åü„ÅÆ„Åã„ÇíÊòéË®òÔºà‚óãÔºâ
                        Exsample: „ÄåÂâçÊúà„Åã„Çâ‰∏äÊòá„ÅåÂä†ÈÄü„Åó„Åæ„Åó„Åü„Äç ‚Üí „ÄåÊôØÊ∞óÂõûÂæ©„ÅÆÂä†ÈÄüÔºà‚óãÔºâ„Äç
                        - ÂâçÊúà„Åã„Çâ‰∏äÊòá„ÅåÂä†ÈÄü„Åó„Åæ„Åó„ÅüÔºà√óÔºâ ‚Üí ÂÖ∑‰ΩìÁöÑ„Å™ÁµåÊ∏àÊ¥ªÂãï„ÇíÊòéË®òÔºà‚óãÔºâ
                        Exsample: „ÄåÊôØÊ∞óÂä†ÈÄüÔºà‚óãÔºâ„Äç„ÄåÊ∂àË≤ª„ÅÆÂõûÂæ©„ÅåÂä†ÈÄüÔºà‚óãÔºâ„Äç„ÄåÊäïË≥á„ÅÆÊã°Â§ß„ÅåÂä†ÈÄüÔºà‚óãÔºâ„Äç
                        - ÁµåÊ∏àÔºà√óÔºâ ‚Üí ÊôØÊ∞óÔºà‚óãÔºâÔºà"ÁµåÊ∏à" „Åß„ÅØ„Å™„Åè "ÊôØÊ∞ó" „ÇíÁî®„ÅÑ„ÇãÔºâ
                        Exsample: 
                        ÂâçÊúà„Åã„ÇâÂä†ÈÄü„Åó„Åæ„Åó„Åü„ÄÇÔºà√óÔºâ-> ‰ºÅÊ•≠„ÅÆË®≠ÂÇôÊäïË≥á„ÅåÂä†ÈÄü„Åó„Åæ„Åó„Åü„ÄÇÔºà‚óãÔºâ
                        ÁµåÊ∏àÂä†ÈÄü„ÅåË¶ã„Çâ„Çå„Åæ„Åô„ÄÇÔºà√óÔºâ-> ÊôØÊ∞óÂä†ÈÄü„ÅåË¶ã„Çâ„Çå„Åæ„Åô„ÄÇÔºà‚óãÔºâ
                        Ê∂àË≤ª„ÅåÂâçÊúà„Åã„ÇâÂä†ÈÄü„Åó„Åæ„Åó„Åü„ÄÇÔºà√óÔºâ-> ÂÄã‰∫∫Ê∂àË≤ª„ÅÆÊã°Â§ß„ÅåÂä†ÈÄü„Åó„Åæ„Åó„Åü„ÄÇÔºà‚óãÔºâ
                        „Ç§„É≥„Éï„É¨„ÅåÂâçÊúà„Åã„ÇâÂä†ÈÄü„Åó„Åæ„Åó„Åü„ÄÇÔºà√óÔºâ-> Áâ©‰æ°‰∏äÊòá„ÅÆ„Çπ„Éî„Éº„Éâ„ÅåÂä†ÈÄü„Åó„Åæ„Åó„Åü„ÄÇÔºà‚óãÔºâ


                    - Êó•‰ªò„ÉªÂõΩÂêç„ÅÆÊòéË®ò:
                        „ÅÑ„Å§„ÅÆ„ÇÇ„ÅÆ„ÅãÁâπÂÆö„Åß„Åç„ÇãÂ†¥Âêà„ÄÅ‚óãÊúà„ÅÆ„ÇÇ„ÅÆ„Åã„ÇíÊòéË®ò„Åô„Çã„ÄÇ‰æã: „Äå10Êúà„ÅÆË£ΩÈÄ†Ê•≠PMIÔºàË≥ºË≤∑ÊãÖÂΩìËÄÖÊôØÊ∞óÊåáÊï∞Ôºâ„ÅØÔΩû„Äç
                        ÂøÖË¶Å„Å´Âøú„Åò„Å¶ÂõΩÂêç„ÇÇË®òËºâ„Åô„Çã„ÄÇÔºàÊñáËÑà„Å´Âøú„Åò„Å¶Ë®òËºâ„Åó„Å¶„ÅÑ„Çå„Å∞„ÄÅ‰ΩçÁΩÆ„ÅØÂïè„Çè„Å™„ÅÑÔºâ
                    - Êó•‰ªò„ÉªÂõΩÂêç„ÅÆÂ§âÊèõ„É´„Éº„É´:

                        **„Äå‰∏ãÊó¨„Äç„Äå‰∏äÊó¨„Äç**„Å®Ë®òËºâ„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅÊñáËÑà„Åã„ÇâÈÅ©Âàá„Å™Êó•‰ªò„Å´Â§âÊõ¥„Åô„Çã„ÄÇ
                        **„Äå„É¶„Éº„É≠Âúè„Äç**„Å®Ë®òËºâ„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅÊñáËÑà„Å´Âøú„Åò„Å¶ÈÅ©Âàá„Å™ÂõΩÂêç„Å´ÁΩÆ„ÅçÊèõ„Åà„Çã„ÄÇ
                        Exsample:

                        ‰øÆÊ≠£Ââç: „Äå‰∏ãÊó¨„ÅØ„ÄÅ„É¶„Éº„É≠ÂúèÁ∑èÂêàPMIÔºàË≥ºË≤∑ÊãÖÂΩìËÄÖÊôØÊ∞óÊåáÊï∞Ôºâ„Åå‚Ä¶„Äç

                        ‰øÆÊ≠£Âæå: „Äå10Êúà‰∏ãÊó¨„ÅÆ„Éâ„Ç§„ÉÑÁ∑èÂêàPMIÔºàË≥ºË≤∑ÊãÖÂΩìËÄÖÊôØÊ∞óÊåáÊï∞Ôºâ„Åå‚Ä¶„Äç

                        ‰øÆÊ≠£Ââç: „Äå‰∏äÊó¨„ÅØ„ÄÅ„É¶„Éº„É≠ÂúèÁ∑èÂêàPMIÔºàË≥ºË≤∑ÊãÖÂΩìËÄÖÊôØÊ∞óÊåáÊï∞Ôºâ„Åå‚Ä¶„Äç

                        ‰øÆÊ≠£Âæå: „Äå10Êúà‰∏äÊó¨„ÅÆ„Éï„É©„É≥„ÇπÁ∑èÂêàPMIÔºàË≥ºË≤∑ÊãÖÂΩìËÄÖÊôØÊ∞óÊåáÊï∞Ôºâ„Åå‚Ä¶„Äç

            - **Ê•≠ÁïåÊåáÊï∞„ÅÆË°®Ë®ò:**
                - ÂøÖ„ÅöÂØæË±°„Å®„Å™„Çã„ÄåÊúà„Äç„ÇíÊòéË®ò„Åô„Çã„ÄÇ
                - Êúà„Åå„Å™„ÅÑÂ†¥Âêà„ÄÅÊúÄËøë3„É∂Êúà‰ª•ÂÜÖ„Åã„ÄÅ3„É∂Êúà‰ª•‰∏äÂâç„ÅÆ„ÇÇ„ÅÆ„Åã„ÇíÁ¢∫Ë™ç„Åô„Çã„ÄÇ
                - ÂøÖË¶Å„Å´Âøú„Åò„Å¶ÂõΩÂêç„ÇÇË®òËºâ„Åô„Çã„ÄÇÔºàÊñáËÑà„Å´Âøú„Åò„Å¶‰ΩçÁΩÆ„ÅØËá™Áî±Ôºâ
                    - Example:
                        - Ë£ΩÈÄ†Ê•≠PMIÔºàË≥ºË≤∑ÊãÖÂΩìËÄÖÊôØÊ∞óÊåáÊï∞ÔºâÔºà√óÔºâ ‚Üí 10Êúà„ÅÆË£ΩÈÄ†Ê•≠PMIÔºà‚óãÔºâ
                        - Áõ¥Ëøë3„É∂Êúà‰ª•ÂÜÖ„ÅÆÊåáÊï∞„ÅØÊòéÁ§∫ÁöÑ„Å´„Äå‚óãÊúà„Äç„Å®Ë®òËºâ„Åô„Çã„ÄÇÔºà‰æã:12Êúà„ÅÆCPIÔºâ
                        - 3„É∂Êúà‰ª•‰∏äÂâç„ÅÆÊåáÊï∞„ÅØ„ÄÅÊØîËºÉ„ÅÆÊñáËÑà„ÇíÊòéÁ¢∫„Å´„Åô„Çã„ÄÇÔºà‰æã:2023Âπ¥10Êúà„ÅÆGDPÊàêÈï∑ÁéáÔºâ
                        - „É¶„Éº„É≠Âúè„ÅÆ10ÊúàPMI„ÅØÔΩûÔºà‚óãÔºâ
            - **„Ç´„Çø„Ç´„ÉäË°®Ë®ò„ÅÆÁµ±‰∏Ä:**
                Katakana representation of foreign words should be unified within the document.
                    Ensure that the Katakana form is consistent throughout the text, and choose one version for the entire document.
                    Modification reason: To maintain consistency in the usage of Katakana for foreign words.
                    Example of text modifications:

                    „Çµ„Çπ„ÉÜ„Éä„Éñ„É´ (√ó) ‚Üí „Çµ„Çπ„ÉÜ„Ç£„Éä„Éñ„É´ (‚óã)

                    „Ç®„É≥„Çø„Éº„ÉÜ„Ç§„É°„É≥„Éà (√ó) ‚Üí „Ç®„É≥„Çø„Éº„ÉÜ„Ç§„É≥„É°„É≥„Éà (‚óã)
            
            - **„É¨„É≥„Ç∏„ÅÆË°®Ë®ò„Å´„Å§„ÅÑ„Å¶Ë°®Ë®ò:**
                - Always append "%" when indicating a range.(Ensure that the original text is not directly modified but follows this guideline.)
                - Make modifications directly in this article and explain the reasons for the modifications.
                - Example: -1ÔΩû0.5%Ôºà√óÔºâ ‚Üí -1%ÔΩû0.5%Ôºà‚óãÔºâ
            - **ÂÑüÈÇÑ„Å´Èñ¢„Åô„ÇãË®òËºâ:**
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                - ÊúÄÁµÇ„É™„É™„Éº„Çπ„ÅÆ1„ÉµÊúàÁ®ãÂâç„Çà„Çä„ÄÅÂÑüÈÇÑ„Å´Èñ¢„Åô„ÇãÂÜÖÂÆπ„ÇíÂÖ•„Çå„Çã„Åì„Å®„ÄÇ
                - ‰æãÔºâÂΩì„Éï„Ç°„É≥„Éâ„ÅØ„ÄÅ‚óè‚óèÊúà‚óè‚óèÊó•„Å´‰ø°Ë®ó„ÅÆÁµÇ‰∫ÜÊó•ÔºàÂÑüÈÇÑÊó•orÁπ∞‰∏äÂÑüÈÇÑÊó•Ôºâ„ÇíËøé„Åà„Çã‰∫àÂÆö„Åß„Åô„ÄÇ
                - ‚ÄªÔºàÔºâÂÜÖ„ÅØ„ÄÅÂÆöÊôÇÂÑüÈÇÑ„ÅÆÂ†¥Âêà„Å´„ÅØÂÑüÈÇÑÊó•„ÄÅÁπ∞‰∏äÂÑüÈÇÑ„ÅÆÂ†¥Âêà„Å´„ÅØÁπ∞‰∏äÂÑüÈÇÑÊó•„Å®„Åô„Çã„ÄÇ
            - **ÂÄãÂà•‰ºÅÊ•≠Âêç„ÅÆË°®Ë®ò:**
                - ÊäïË≥áÁí∞Â¢ÉÁ≠â„Å´„Åä„ÅÑ„Å¶„ÅØ„ÄÅÂÄãÂà•‰ºÅÊ•≠„ÅÆÂêçÁß∞„Çí‰Ωø„Çè„Å™„ÅÑË°®Áèæ„ÇíÂøÉÊéõ„Åë„Çã„ÄÇ
                - ‰æã:„Çπ„Ç§„ÇπÈáëËûçÂ§ßÊâã„ÇØ„É¨„Éá„Ç£„Éª„Çπ„Ç§„ÇπÔºà√óÔºâ ‚Üí „Çπ„Ç§„Çπ„ÅÆÂ§ßÊâãÈáëËûç„Ç∞„É´„Éº„ÉóÔºà‚óãÔºâ
            - **„Éó„É©„Çπ„Å´ÂØÑ‰∏é/ÂΩ±Èüø„ÅÆË°®Ë®ò:**
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                - „Äå„Éó„É©„Çπ„Å´ÂØÑ‰∏é„Äç„Åæ„Åü„ÅØ„Äå„Éó„É©„Çπ„Å´ÂΩ±Èüø„Äç„Å©„Å°„Çâ„ÇÇÂèØ„ÄÇ
                - „Åæ„Åü„ÅØ„ÄÅ„ÄåÔΩû„Éó„É©„ÇπË¶ÅÂõ†„Å®„Å™„Çã„Äç„Å®Ë°®Ë®ò„ÄÇ
            - **„Éû„Ç§„Éä„Çπ„Å´ÂΩ±Èüø„ÅÆË°®Ë®ò:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - „Äå„Éû„Ç§„Éä„Çπ„Å´ÂØÑ‰∏é„ÄçÔºà√óÔºâ‚Üí„Äå„Éû„Ç§„Éä„Çπ„Å´ÂΩ±Èüø„ÄçÔºà‚óãÔºâ
                - „Éû„Ç§„Éä„Çπ„ÅÆÈöõ„ÅØ„ÄåÂØÑ‰∏é„Äç„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ„ÄÇ
                - „Åæ„Åü„ÅØ„ÄÅ„ÄåÔΩû„Éû„Ç§„Éä„ÇπË¶ÅÂõ†„Å®„Å™„Çã„Äç„Å®Ë°®Ë®ò„ÄÇ
            - **Âà©Âõû„Çä„ÅÆË°®Ë®ò:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - Âà©Âõû„Çä„ÅØ„Äå‰∏äÊòáÔºà‰æ°Ê†º„ÅØ‰∏ãËêΩÔºâ„Äç„Åæ„Åü„ÅØ„Äå‰Ωé‰∏ãÔºà‰æ°Ê†º„ÅØ‰∏äÊòáÔºâ„Äç„Å®Ë°®Ë®ò„ÄÇ
            - **‰Ωé‰∏ã„Å®‰∏ãËêΩ„ÅÆË°®Ë®ò:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - ÂÇµÂà∏Âà©Âõû„Çä„ÅØ„Äå‰Ωé‰∏ãÔºà‚óãÔºâ„Äç„Å®Ë°®Ë®ò„Åó„ÄÅ„Äå‰∏ãËêΩÔºà√óÔºâ„Äç„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ„ÄÇ
                - ‰æ°Ê†º„ÅØ„Äå‰∏ãËêΩÔºà‚óãÔºâ„Äç„Å®Ë°®Ë®ò„Åó„ÄÅ„Äå‰Ωé‰∏ãÔºà√óÔºâ„Äç„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ„ÄÇ
                - ÈáëÂà©„ÅÆ„Äå‰Ωé‰∏ãÔºà„ÄáÔºâ„Äç„Å®Ë°®Ë®ò„Åó„ÄÅ„Äå‰∏ãËêΩÔºà√óÔºâ„Äç„ÅØ‰ΩøÁî®„Åó„Å™„ÅÑ„ÄÇ

            - **Ë≥áÈáëÊµÅÂá∫ÂÖ•„ÅÆË°®Ë®ò:**
                - Do notMake modifications directly in this article and explain the reasons for the modifications.
                - „ÄåÂ§ñÂõΩ‰∫∫ÊäïË≥áÂÆ∂„ÅÆË≥áÈáëÊµÅÂá∫„Äç„Çí„ÄåÂ§ñÂõΩ‰∫∫ÊäïË≥áÂÆ∂„Åã„Çâ„ÅÆË≥áÈáëÊµÅÂÖ•„Äç„Å®Ë®òËºâ„ÄÇ

            - **ÔºàÈáëÂà©„ÅÆÔºâÂÖàÈ´òÊÑü/ÂÖàÈ´òË¶≥ „ÅÆË°®Ë®òÁµ±‰∏Ä:**
                - Êñá‰∏≠„Å´„ÄåÂÖàÈ´òË¶≥„Äç„Å®„ÅÑ„ÅÜË°®Ë®ò„Åå„ÅÇ„ÇãÂ†¥Âêà„Åß„ÇÇ„ÄÅÂéüÊñá„ÅØ‰øÆÊ≠£„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ
                - „Åù„ÅÆ‰ª£„Çè„Çä„ÄÅ„ÄåÂÖàÈ´òË¶≥„Äç„ÅÆÁõ¥Âæå„Å´„Äå‰øÆÊ≠£ÊèêÊ°à„Äç„Å®„Åó„Å¶„ÄÅ„ÄåÂÖàÈ´òÊÑü„Äç„Å∏„ÅÆÁµ±‰∏ÄÁêÜÁî±„ÇíÊèêÁ§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
                - Ë°®Ë®ò„Åå„Åô„Åß„Å´„ÄåÂÖàÈ´òÊÑü„Äç„Åß„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅ‰Ωï„ÇÇËøΩË®ò„Åõ„Åö„Åù„ÅÆ„Åæ„Åæ„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

                - Ë°®Á§∫„Éï„Ç©„Éº„Éû„ÉÉ„ÉàÔºà‰æãÔºâ:
                    ÂÖàÈ´òË¶≥<span style="color:red;">Ôºà‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁµ±‰∏Ä <s style="background:yellow;color:red">ÂÖàÈ´òË¶≥</s> ‚Üí ÂÖàÈ´òÊÑüÔºâ</span>

                - ÂøÖ„ÅöÂéüÊñá„ÅÆÊßãÊàê„Å®ÊñáËÑà„Çí‰øùÊåÅ„Åó„ÄÅÊßãÊñá„ÇíÂ£ä„Åï„Åö„ÄÅ‰øÆÊ≠£ÁêÜÁî±„ÅØË£úË∂≥ÁöÑ„Å´Âæå„Çç„Å´ËøΩË®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
                
            - **„Éù„Éº„Éà„Éï„Ç©„É™„Ç™„ÅÆË°®Ë®ò:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - „Äå‚óè‚óè„Å∏„ÅÆÁµÑ„ÅøÂÖ•„ÇåÔºà√óÔºâ„Äç„Åß„ÅØ„Å™„Åè„ÄÅ„Äå‚óè‚óè„ÅÆÁµÑ„ÅøÂÖ•„ÇåÔºà‚óãÔºâ„Äç„Å®Ë°®Ë®ò„ÄÇ
                - „Äå„Å∏„ÅÆÊäïË≥áÊØîÁéá„Äç„ÅØ‰ΩøÁî®ÂèØËÉΩ„ÄÇ
                
            - **ÊßãÊàêÊØî„ÅÆ0ÔºÖ„ÅÆË°®Ë®ò:
                - „Äå0ÔºÖÁ®ãÂ∫¶„Äçor„Äå„Çº„É≠ÔºÖÁ®ãÂ∫¶„Äç„ÅÆË°®Ë®ò„Çí‰ΩøÁî®„Åô„Çã„Åì„Å®
                - Â§âÊõ¥ÂâçË°®Ë®ò: ÊßãÊàêÊØî„ÅØ0ÔºÖ„Åß„ÅÇ„Çã
                - Áµ±‰∏ÄÂæåË°®Ë®ò: ÊßãÊàêÊØî„ÅØ0ÔºÖÁ®ãÂ∫¶
                - Append a correction reason in the following format:
                        `<span style="color:red;">Â§âÊõ¥ÂâçË°®Ë®ò</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÊßãÊàêÊØîË°®Ë®ò <s style="background:yellow;color:red">Â§âÊõ¥ÂâçË°®Ë®ò</s> ‚Üí Áµ±‰∏ÄÂæåË°®Ë®ò</span>)`
                    
                    Example:
                    Input: ÊßãÊàêÊØî„ÅØ0ÔºÖ„Åß„ÅÇ„Çã
                    Output: 
                    <span style="color:red;">ÊßãÊàêÊØî„ÅØ0ÔºÖ„Åß„ÅÇ„Çã</span> 
                    (<span>‰øÆÊ≠£ÁêÜÁî±: ÊßãÊàêÊØîË°®Ë®ò <s style="background:yellow;color:red">ÊßãÊàêÊØî„ÅØ0ÔºÖ„Åß„ÅÇ„Çã</s> ‚Üí ÊßãÊàêÊØî„ÅØ0ÔºÖÁ®ãÂ∫¶„Åß„ÅÇ„Çã„ÄÇ</span>)„ÅßÂ£≤„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ

                
            - **'ÊäïË≥áÁí∞Â¢É„ÅÆË®òËø∞:** 
                - Make modifications directly in this article and explain the reasons for the modifications.
                **„ÄåÂÖàÊúà„ÅÆÊäïË≥áÁí∞Â¢É„Äç**„ÅÆÈÉ®ÂàÜ„Åß„ÄåÂÖàÊúàÊú´„Äç„ÅÆË®òËø∞„ÅåÂê´„Åæ„Çå„ÇãÂ†¥Âêà„ÄÅ„ÄåÂâçÊúàÊú´„Äç„Å´Â§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
                - Ensure that the original text is directly modified and follows this guideline.
                Example:
                ‰øÆÊ≠£Ââç: ÂÖàÊúàÊú´„ÅÆÂ∏ÇÂ†¥ÂãïÂêë„ÇíÂàÜÊûê„Åô„Çã„Å®‚Ä¶
                ‰øÆÊ≠£Âæå: ÂâçÊúàÊú´„ÅÆÂ∏ÇÂ†¥ÂãïÂêë„ÇíÂàÜÊûê„Åô„Çã„Å®‚Ä¶

            - **ÈÄöË≤®Ë°®Ë®ò„ÅÆÁµ±‰∏Ä:**
                - Standardize currency notation across the document.
                    - The first appearance of any currency symbol (e.g., „Éâ„É´, $, ÂÜÜ, JPY) will be the standard.
                    - All following occurrences of that currency must match this format.

                    - For example, if "100„Éâ„É´" appears first, then all future "$100" will be rewritten to "100„Éâ„É´" for consistency.
                    - If "$100" appears first, then "100„Éâ„É´" should be rewritten as "$100".

                    - Always apply this rule in the direction of "first-appeared" format.
                    - Append a correction reason in the following format:
                        `<span style="color:red;">Áµ±‰∏ÄÂæåË°®Ë®ò</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÈÄöË≤®Ë°®Ë®ò„ÅÆÁµ±‰∏Ä <s style="background:yellow;color:red">Â§âÊõ¥ÂâçË°®Ë®ò</s> ‚Üí Áµ±‰∏ÄÂæåË°®Ë®ò</span>)`
                    
                    Example:
                    Input: „Åì„ÅÆ„Éê„ÉÉ„Ç∞„ÅØ100„Éâ„É´„Åß„Åô„Åå„ÄÅ„Ç¢„É°„É™„Ç´„Åß„ÅØ$100„ÅßÂ£≤„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
                    Output:
                    „Åì„ÅÆ„Éê„ÉÉ„Ç∞„ÅØ100„Éâ„É´„Åß„Åô„Åå„ÄÅ„Ç¢„É°„É™„Ç´„Åß„ÅØ
                    <span style="color:red;">100„Éâ„É´</span>
                    (<span>‰øÆÊ≠£ÁêÜÁî±: ÈÄöË≤®Ë°®Ë®ò„ÅÆÁµ±‰∏Ä <s style="background:yellow;color:red">$100</s> ‚Üí 100„Éâ„É´</span>)„ÅßÂ£≤„Çâ„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ


            **Preferred and Recommended Terminology (ÁΩÆ„ÅçÊèõ„Åà„ÅåÂøÖË¶Å„Å™Áî®Ë™û/Ë°®Áèæ):**
            - **Á¨¨1ÂõõÂçäÊúü:**
                - Ensure the period is clearly stated.
                - Example: 18Âπ¥Á¨¨4ÂõõÂçäÊúüÔºà√óÔºâ ‚Üí 2018Âπ¥10-12ÊúàÊúüÔºà‚óãÔºâ
            - **Á¥Ñ‚óãÔºÖÁ®ãÂ∫¶:**
                - Do not use "Á¥Ñ" (approximately) and "Á®ãÂ∫¶" (extent) together. Choose either one.
                - Example: Á¥Ñ‚óãÔºÖÁ®ãÂ∫¶Ôºà√óÔºâ ‚Üí Á¥Ñ‚óãÔºÖ or ‚óãÔºÖÁ®ãÂ∫¶Ôºà‚óãÔºâ
            - **Â§ßÊâã‰ºÅÊ•≠Ë°®Ë®ò„ÅÆÊòéÁ¢∫Âåñ**  
                **Correction Rule:**
                - If a sentence contains vague expressions like„ÄåÂ§ßÊâã‚óã‚óã„Äç, analyze the context to determine what type of company is being referred to.
                - Rewrite it in the format:„ÄåÂ§ßÊâã‚óã‚óã‰ºöÁ§æ„Äç/„ÄåÂ§ßÊâã‚óã‚óã‰ºÅÊ•≠„Äç/„ÄåÂ§ßÊâã‚óã‚óã„É°„Éº„Ç´„Éº„Äçdepending on the company‚Äôs nature.
                - Use context clues (e.g., product type, industry references) to guess the appropriate company category (e.g., ‰∏çÂãïÁî£, Ëá™ÂãïËªä, ÈõªÊ©ü, ÈáëËûç).
                - Append a correction reason in this format:
                `<span style="color:red;">Changed Expression</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ë°®Áèæ„ÅÆÊòéÁ¢∫Âåñ <s style="background:yellow;color:red">Original Expression</s> ‚Üí Changed Expression</span>)`

                **Example Input:**
                - Â§ßÊâã„ÅØÊ•≠ÁïåÂÖ®‰Ωì„Å´ÂΩ±ÈüøÂäõ„ÇíÊåÅ„Å§„ÄÇ
                - Â§ßÊâã„ÅåÊñ∞„Åó„ÅÑÂçäÂ∞é‰Ωì„ÇíÁô∫Ë°®„Åó„Åü„ÄÇ

                **Example Output:**
                - <span style="color:red;">Â§ßÊâã‰∏çÂãïÁî£‰ºöÁ§æ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ë°®Áèæ„ÅÆÊòéÁ¢∫Âåñ <s style="background:yellow;color:red">Â§ßÊâã</s> ‚Üí Â§ßÊâã‰∏çÂãïÁî£‰ºöÁ§æ</span>) „ÅØÊ•≠ÁïåÂÖ®‰Ωì„Å´ÂΩ±ÈüøÂäõ„ÇíÊåÅ„Å§„ÄÇ
                - <span style="color:red;">Â§ßÊâãÂçäÂ∞é‰Ωì„É°„Éº„Ç´„Éº</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ë°®Áèæ„ÅÆÊòéÁ¢∫Âåñ <s style="background:yellow;color:red">Â§ßÊâã</s> ‚Üí Â§ßÊâãÂçäÂ∞é‰Ωì„É°„Éº„Ç´„Éº</span>) „ÅåÊñ∞„Åó„ÅÑÂçäÂ∞é‰Ωì„ÇíÁô∫Ë°®„Åó„Åü„ÄÇ

                **Important Notes:**
                - Always preserve the original sentence structure and paragraph formatting.
                - Only make corrections when„Äå‚óã‚óãÂ§ßÊâã„Äçis ambiguous and can be clarified using contextual information.
                - Do not modify proper nouns or known company names (e.g., „Éà„É®„Çø, „ÇΩ„Éã„Éº).

            - **ÂÖ•Âäõ‰æã:**  
                - „ÄåÂ§ßÊâã„É°„Éº„Ç´„Éº/‰ºöÁ§æ/‰ºÅÊ•≠„Äç  
                - **Âá∫Âäõ:** „ÄåÂ§ßÊâã‰∏çÂãïÁî£‰ºöÁ§æ„ÄÅÂ§ßÊâãÂçäÂ∞é‰Ωì„É°„Éº„Ç´„Éº„Äç  
            - **The actual company name must be found and converted in the article
            - **ÂÖàÊúà/ÂâçÊúà„ÅÆË°®Ë®ò:
                - 1„ÉµÊúàÂâç„Å´„Å§„ÅÑ„Å¶Ë®ÄÂèä„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅ„ÄåÂâçÊúà„Äç„Çí‰ΩøÁî®„ÄÇ
            ÂâçÊúüÊØî‚óãÔºÖ„ÅÆË°®Ë®ò:

            - **ÂâçÊúüÊØîÂπ¥Áéá‚óãÔºÖ:**
                - Âü∫Êú¨ÁöÑ„Å´„ÄÅÊúüÈñìÊØîËºÉ„ÅÆ‰º∏Áéá„ÅØ„ÄåÂπ¥Áéá„Äç„ÇíË®òËºâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
                - ‰∏ª„Å´ÁµåÊ∏àÁµ±Ë®àÁ≠â„Åß‰∏ÄËà¨ÁöÑ„Å´ÂâçÊúüÊØî„ÅßÂπ¥ÁéáÊèõÁÆó„Åï„Çå„Å¶„ÅÑ„Çã„ÇÇ„ÅÆ„Å´„Å§„ÅÑ„Å¶„ÅØ„ÄÅ„ÄåÂâçÊúüÊØîÂπ¥Áéá‚óãÔºÖ„Äç„Å®Ë°®Ë®ò„ÄÇ
            - **Á¨¨‚óãÂõõÂçäÊúü„ÅÆË°®Ë®ò:**
                **„É´„Éº„É´:
                - If the input contains a format like "18Âπ¥Á¨¨4ÂõõÂçäÊúü", infer it as:
                    - "18Âπ¥" ‚Üí "2018Âπ¥"
                    - "Á¨¨1ÂõõÂçäÊúü" ‚Üí "1-3ÊúàÊúü"
                    - "Á¨¨2ÂõõÂçäÊúü" ‚Üí "4-6ÊúàÊúü"
                    - "Á¨¨3ÂõõÂçäÊúü" ‚Üí "7-9ÊúàÊúü"
                    - "Á¨¨4ÂõõÂçäÊúü" ‚Üí "10-12ÊúàÊúü"
                - Modify the expression accordingly, converting the year to a 4-digit format and specifying the exact month range.
                - Add a correction reason in this format:
                `<span style="color:red;">‰øÆÊ≠£Âæå</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÂõõÂçäÊúüË°®Ë®ò„ÅÆÊòéÁ¢∫Âåñ <s style="background:yellow;color:red">‰øÆÊ≠£Ââç</s> ‚Üí ‰øÆÊ≠£Âæå</span>)`

                ---

                **Example:**
                - Input: 18Âπ¥Á¨¨4ÂõõÂçäÊúü„ÅÆÂ£≤‰∏ä„ÅåÂ•ΩË™ø„Å†„Å£„Åü„ÄÇ
                - Output: 
                <span style="color:red;">2018Âπ¥10-12ÊúàÊúü</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÂõõÂçäÊúüË°®Ë®ò„ÅÆÊòéÁ¢∫Âåñ <s style="background:yellow;color:red">18Âπ¥Á¨¨4ÂõõÂçäÊúü</s> ‚Üí 2018Âπ¥10-12ÊúàÊúü</span>) „ÅÆÂ£≤‰∏ä„ÅåÂ•ΩË™ø„Å†„Å£„Åü„ÄÇ

                ---

                **Additional Notes:**
                - Do not modify any proper names, organizations, or if the date range is already correct.
                - Apply to all similar shorthand expressions like "20Âπ¥Á¨¨2ÂõõÂçäÊúü", "21Âπ¥Á¨¨1ÂõõÂçäÊúü" etc.
                - Keep the structure and formatting of the original document.


        **Special Rules:**
        1. **Do not modify proper nouns (e.g., names of people, places, or organizations) unless they are clearly misspelled.**
            -Exsample:
            „Éô„ÉÉ„Çª„É≥„ÉàÊ∞è: Since this is correct and not a misspelling, it will not be modified.
        2. **Remove unnecessary or redundant text instead of replacing it with other characters.**
            -Exsample:
            „É¶„Éº„É≠ÂúèÂüüÂÜÖ„ÅÆÊôØÊ∞óÂßî: Only the redundant character Âßî will be removed, and no additional characters like „ÅÆ will be added. The corrected text will be: „É¶„Éº„É≠ÂúèÂüüÂÜÖ„ÅÆÊôØÊ∞ó.
        3. **Preserve spaces between words in the original text unless they are at the beginning or end of the text.**
            -Example:
            input: Êúà„ÅÆ ÂâçÂçä„ÅØÁ±≥ÂõΩ„ÅÆ ÂÇµÂà∏Âà©Âõû„Çä„ÅÆ‰∏äÊòá „Å´„Å§„Çå„Å¶
            Output: Êúà„ÅÆ ÂâçÂçä„ÅØÁ±≥ÂõΩ„ÅÆ ÂÇµÂà∏Âà©Âõû„Çä„ÅÆ‰∏äÊòá „Å´„Å§„Çå„Å¶ (spaces between words are preserved).

        **Output Requirements:**
        1. **Highlight the original incorrect text in red and include additional details:**
        - For corrected parts:
            - Highlight the original incorrect text in red using `<span style="color:red;">`.
            - Append the corrected text in parentheses, marked with a strikethrough using `<s>` tags.
            - Provide the reason for the correction and indicate the change using the format `123 ‚Üí 456`.
            - Example:
            `<span style="color:red;">123</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ‰∏ÄËá¥ÊÄß‰∏çË∂≥ <s style="background:yellow;color:red">123</s> ‚Üí 456</span>)`
        
        2. **Preserve the original structure and formatting of the document:**
        - Maintain paragraph breaks, headings, and any existing structure in the content.

        3. **Use the uploaded correction rules for reference:**
        - {corrected}

        4. **Do not provide any explanations or descriptions in the output. Only return the corrected HTML content.**

         **Corrected Terminology Map (‰øÆÊ≠£„Åï„Çå„ÅüÁî®Ë™û„É™„Çπ„Éà):
            {corrected_map}
        - Replace only when the **original** term in `corrected_map` appears in the input text.
        - Do **not** replace anything if the input already contains the `corrected` term (it is already correct).
        - Do **not** perform any reverse replacements (`corrected ‚Üí original` „ÅØÁ¶ÅÊ≠¢).
        - Modify the original text only when the `original` term is found.

        - If the `corrected` term appears in the input, **do not modify it** (it is already correct).
        - Do **not** reverse substitutions (i.e., never convert corrected ‚Üí `original`).
        
        - After replacing, add the reason in this format:
        Original Term (‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁµ±‰∏Ä Original Term ‚Üí Corrected Term)
        Example:
            `<span style="color:red;">Corrected Term</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁµ±‰∏Ä <s style="background:yellow;color:red">Original Term</s> ‚Üí Corrected Term</span>)`
        
        Example:
        Input: ‰∏≠ÈäÄ
        Output: 
        `<span style="color:red;">‰∏≠Â§ÆÈäÄË°å</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁµ±‰∏Ä <s style="background:yellow;color:red">‰∏≠ÈäÄ</s> ‚Üí ‰∏≠Â§ÆÈäÄË°å</span>)`
        ‚Äª Note: Do **not** convert ‰∏≠Â§ÆÈäÄË°å ‚Üí ‰∏≠ÈäÄ. All replacements must follow the direction from `original` to `corrected` only.

        Input: ‰∏≠Â§ÆÈäÄË°å  
        Output:  
        ‰∏≠Â§ÆÈäÄË°å ‚Üê (No correction shown because it is already the correct term)

        If the input already contains the corrected term, it should remain unchanged.
        For English abbreviations or foreign terms, the rule is the same: replace the original term with the corrected term and format as follows:
        Example:
        Input: BOE
        Output: <span style="color:red;">BOEÔºàËã±‰∏≠Â§ÆÈäÄË°å„ÄÅ„Ç§„É≥„Ç∞„É©„É≥„ÉâÈäÄË°åÔºâ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">BOE</s> ‚Üí BOEÔºàËã±‰∏≠Â§ÆÈäÄË°å„ÄÅ„Ç§„É≥„Ç∞„É©„É≥„ÉâÈäÄË°åÔºâ</span>)
        Input: AAA
        Output: <span style="color:red;">AAAÔºàÂÖ®Á±≥Ëá™ÂãïËªäÂçî‰ºöÔºâ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">AAA</s> ‚Üí AAAÔºàÂÖ®Á±≥Ëá™ÂãïËªäÂçî‰ºöÔºâ</span>)

        Input: „Ç§„É≥„Éê„Ç¶„É≥„Éâ
        Output: <span style="color:red;">„Ç§„É≥„Éê„Ç¶„É≥„ÉâÔºàË¶≥ÂÖâÂÆ¢„ÅÆÂèó„ÅëÂÖ•„ÇåÔºâ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Ç§„É≥„Éê„Ç¶„É≥„Éâ</s> ‚Üí „Ç§„É≥„Éê„Ç¶„É≥„ÉâÔºàË¶≥ÂÖâÂÆ¢„ÅÆÂèó„ÅëÂÖ•„ÇåÔºâ</span>)

        
        **Except Original Term
        Input: Á≠â
        Output: 
        `<span style="color:red;">Á≠â</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û/Ë°®Áèæ <s style="background:yellow;color:red">Á≠â</s> ‚Üí „Å™„Å©</span>)`

        Input: „É≠„Éº„É≥
        Output: 
        `<span style="color:red;">„É≠„Éº„É≥</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û/Ë°®Áèæ <s style="background:yellow;color:red">„É≠„Éº„É≥</s> ‚Üí Ë≤∏„Åó‰ªò„Åë</span>)`
                            
        Input: ÔºÖ„Çí‰∏äÂõû„Çã
        Output: 
        `<span style="color:red;">ÔºÖ„Çí‰∏äÂõû„Çã</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û/Ë°®Áèæ <s style="background:yellow;color:red">ÔºÖ„Çí‰∏äÂõû„Çã</s> ‚Üí ÔºÖ„ÇíË∂Ö„Åà„Çã</span>)`
                            
        Input: ÔºÖ„Çí‰∏ãÂõû„Çã
        Output: 
        `<span style="color:red;">ÔºÖ„Çí‰∏ãÂõû„Çã</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û/Ë°®Áèæ <s style="background:yellow;color:red">ÔºÖ„Çí‰∏ãÂõû„Çã</s> ‚Üí ÔºÖ„Çí‰∏ãÂõû„Çã„Éû„Ç§„Éä„ÇπÂπÖ</span>)`
        
        Input: ‰ºùÊí≠Ôºà„Åß„Çì„Å±Ôºâ
        Output: 
        `<span style="color:red;">‰ºùÊí≠Ôºà„Åß„Çì„Å±Ôºâ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁΩÆ„ÅçÊèõ„Åà <s style="background:yellow;color:red">‰ºùÊí≠Ôºà„Åß„Çì„Å±Ôºâ</s> ‚Üí Â∫É„Çä„Åæ„Åô</span>)`
        
        Input: ‰ºùÊí≠„Åó„Å¶„ÅÑ„Åæ„Åô
        Output:
        `<span style="color:red;">‰ºùÊí≠„Åó„Å¶„ÅÑ„Åæ„Åô</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁΩÆ„ÅçÊèõ„Åà <s style="background:yellow;color:red">‰ºùÊí≠„Åó„Å¶„ÅÑ„Åæ„Åô</s> ‚Üí Â∫É„Åå„Çã„Åó„Å¶„ÅÑ„Åæ„Åô</span>)`
        
        Input: ÈÄ£„ÇåÈ´ò
        Output:
        `<span style="color:red;">ÈÄ£„ÇåÈ´ò</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁΩÆ„ÅçÊèõ„Åà <s style="background:yellow;color:red">ÈÄ£„ÇåÈ´ò</s> ‚Üí ÂΩ±Èüø„ÇíÂèó„Åë„Å¶‰∏äÊòá</span>)`
        
        Input: Áõ∏Â†¥
        Output:
        `<span style="color:red;">Áõ∏Â†¥</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁΩÆ„ÅçÊèõ„Åà <s style="background:yellow;color:red">Áõ∏Â†¥</s> ‚Üí Â∏ÇÂ†¥/‰æ°Ê†º</span>)`
        
        Input: „Éè„ÉàÊ¥æ
        Output:
        `<span style="color:red;">„Éè„ÉàÊ¥æ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁΩÆ„ÅçÊèõ„Åà <s style="background:yellow;color:red">„Éè„ÉàÊ¥æ</s> ‚Üí ÈáëËûçÁ∑©ÂíåÈáçË¶ñ„ÄÅÈáëËûçÁ∑©Âíå„Å´ÂâçÂêë„Åç</span>)`
        
        Input: „Çø„Ç´Ê¥æ
        Output:
        `<span style="color:red;">„Çø„Ç´Ê¥æ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁΩÆ„ÅçÊèõ„Åà <s style="background:yellow;color:red">„Çø„Ç´Ê¥æ</s> ‚Üí ÈáëËûçÂºï„ÅçÁ∑†„ÇÅÈáçË¶ñ„ÄÅÈáëËûçÂºï„ÅçÁ∑†„ÇÅ„Å´Á©çÊ•µÁöÑ</span>)`
        
        Input: Áπî„ÇäËæº„ÇÄ
        Output: 
        `<span style="color:red;">Áπî„ÇäËæº„ÇÄ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁΩÆ„ÅçÊèõ„Åà <s style="background:yellow;color:red">Áπî„ÇäËæº„ÇÄ</s> ‚Üí ÂèçÊò†„Åï„Çå</span>)`
        
        Input: Á©çÊ•µÂßøÂã¢„Å®„Åó„Åü
        Output: 
        `<span style="color:red;">Á©çÊ•µÂßøÂã¢„Å®„Åó„Åü</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û/Ë°®Áèæ <s style="background:yellow;color:red">Á©çÊ•µÂßøÂã¢„Å®„Åó„Åü</s> ‚Üí Èï∑„ÇÅ„Å®„Åó„Åü</span>)`
        
        Input: ÈôêÂÆöÁöÑ
        Output: 
        `<span style="color:red;">ÈôêÂÆöÁöÑ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÂäπÊûú„ÇÑÂΩ±Èüø„Åå„Éó„É©„Çπ„Åã„Éû„Ç§„Éä„Çπ„Åã‰∏çÊòéÁû≠„Å™„Åü„ÇÅ <s style="background:yellow;color:red">ÈôêÂÆöÁöÑ</s> ‚Üí ‰ªñ„ÅÆÈÅ©Âàá„Å™Ë°®Áèæ„Å´‰øÆÊ≠£</span>)`
        
        Input: Âà©ÁõäÁ¢∫ÂÆö„ÅÆÂ£≤„Çä
        Output: 
        `<span style="color:red;">Âà©ÁõäÁ¢∫ÂÆö„ÅÆÂ£≤„Çä</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Êñ≠ÂÆöÁöÑ„Å™Ë°®Áèæ„Åß„ÅØÊ†πÊã†„ÅåË™¨Êòé„Åß„Åç„Å™„ÅÑ„Åü„ÇÅ <s style="background:yellow;color:red">Âà©ÁõäÁ¢∫ÂÆö„ÅÆÂ£≤„Çä</s> ‚Üí „ÅåÂá∫„Åü„Å®„ÅÆË¶ãÊñπ</span>)`
        
        Input: Âà©È£ü„ÅÑÂ£≤„Çä
        Output: 
        `<span style="color:red;">Âà©È£ü„ÅÑÂ£≤„Çä</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Êñ≠ÂÆöÁöÑ„Å™Ë°®Áèæ„Åß„ÅØÊ†πÊã†„ÅåË™¨Êòé„Åß„Åç„Å™„ÅÑ„Åü„ÇÅ <s style="background:yellow;color:red">Âà©È£ü„ÅÑÂ£≤„Çä</s> ‚Üí „ÅåÂá∫„Åü„Å®„ÅÆË¶ãÊñπ</span>)`
        
        Input: ABS
        Output: 
        `<span style="color:red;">ABS</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">ABS</s> ‚Üí ABSÔºàË≥áÁî£ÊãÖ‰øùË®ºÂà∏„ÄÅÂêÑÁ®ÆË≥áÁî£ÊãÖ‰øùË®ºÂà∏Ôºâ</span>)`
        
        Input: AI
        Output: 
        `<span style="color:red;">AI</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">AI</s> ‚Üí AIÔºà‰∫∫Â∑•Áü•ËÉΩ</span>)`
        
        Input: BRICSÔºà5„ÉµÂõΩÔºâ
        Output: 
        `<span style="color:red;">BRICSÔºà5„ÉµÂõΩÔºâ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">BRICSÔºà5„ÉµÂõΩÔºâ</s> ‚Üí BRICSÔºà„Éñ„É©„Ç∏„É´„ÄÅ„É≠„Ç∑„Ç¢„ÄÅ„Ç§„É≥„Éâ„ÄÅ‰∏≠ÂõΩ„ÄÅÂçó„Ç¢„Éï„É™„Ç´Ôºâ</span>)`
        
        Input: CMBS
        Output: 
        `<span style="color:red;">CMBS</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">CMBS</s> ‚Üí CMBSÔºàÂïÜÊ•≠Áî®‰∏çÂãïÁî£„É≠„Éº„É≥ÊãÖ‰øùË®ºÂà∏Ôºâ</span>)`
        
        Input: ISM
        Output: 
        `<span style="color:red;">ISM</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">ISM</s> ‚Üí ISMÔºàÂÖ®Á±≥‰æõÁµ¶ÁÆ°ÁêÜÂçî‰ºöÔºâ</span>)`
        
        Input: IT
        Output: 
        `<span style="color:red;">IT</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">IT</s> ‚Üí ITÔºàÊÉÖÂ†±ÊäÄË°ìÔºâ</span>)`
        
        Input: MBS
        Output: 
        `<span style="color:red;">MBS</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">MBS</s> ‚Üí MBSÔºà‰ΩèÂÆÖ„É≠„Éº„É≥ÊãÖ‰øùË®ºÂà∏Ôºâ</span>)`
        
        Input: PMI
        Output: 
        `<span style="color:red;">PMI</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">PMI</s> ‚Üí PMIÔºàË≥ºË≤∑ÊãÖÂΩìËÄÖÊôØÊ∞óÊåáÊï∞Ôºâ</span>)`
        
        Input: S&P
        Output: 
        `<span style="color:red;">S&P</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ëã±Áï•Ë™û <s style="background:yellow;color:red">S&P</s> ‚Üí S&PÔºà„Çπ„Çø„É≥„ÉÄ„Éº„Éâ„Éª„Ç¢„É≥„Éâ„Éª„Éó„Ç¢„Éº„Ç∫ÔºâÁ§æ</span>)`
        
        Input: „Ç¢„Çª„ÉÉ„Éà„Ç¢„É≠„Ç±„Éº„Ç∑„Éß„É≥
        Output: 
        `<span style="color:red;">„Ç¢„Çª„ÉÉ„Éà„Ç¢„É≠„Ç±„Éº„Ç∑„Éß„É≥</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Ç¢„Çª„ÉÉ„Éà„Ç¢„É≠„Ç±„Éº„Ç∑„Éß„É≥</s> ‚Üí „Ç¢„Çª„ÉÉ„Éà„Ç¢„É≠„Ç±„Éº„Ç∑„Éß„É≥ÔºàË≥áÁî£ÈÖçÂàÜÔºâ</span>)`
        
        Input: E-„Ç≥„Éû„Éº„Çπ
        Output: 
        `<span style="color:red;">E„Ç≥„Éû„Éº„Çπ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">E-„Ç≥„Éû„Éº„Çπ</s> ‚Üí E„Ç≥„Éû„Éº„ÇπÔºàÈõªÂ≠êÂïÜÂèñÂºïÔºâ</span>)`
             
        Input: e-„Ç≥„Éû„Éº„Çπ
        Output: 
        `<span style="color:red;">e„Ç≥„Éû„Éº„Çπ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">e„Ç≥„Éû„Éº„Çπ</s> ‚Üí e„Ç≥„Éû„Éº„ÇπÔºàÈõªÂ≠êÂïÜÂèñÂºïÔºâ</span>)`
           
        Input: EC
        Output: 
        `<span style="color:red;">ECÔºàÈõªÂ≠êÂïÜÂèñÂºïÔºâ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">EC</s> ‚Üí ECÔºàÈõªÂ≠êÂïÜÂèñÂºïÔºâ</span>)`
        
        Input: „Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ
        Output: 
        `<span style="color:red;">„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ</s> ‚Üí „Ç§„Éº„É´„Éâ„Ç´„Éº„ÉñÔºàÂà©Âõû„ÇäÊõ≤Á∑öÔºâ</span>)`
        
        Input: „Ç®„ÇØ„Çπ„Éù„Éº„Ç∏„É£„Éº
        Output: 
        `<span style="color:red;">„Ç®„ÇØ„Çπ„Éù„Éº„Ç∏„É£„Éº</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Ç®„ÇØ„Çπ„Éù„Éº„Ç∏„É£„Éº</s> ‚Üí ÔºäÁ©çÊ•µÁöÑ„Å´‰ΩøÁî®„Åó„Å™„ÅÑ„ÄÇ„ÄÄÔºà‰æ°Ê†ºÂ§âÂãï„É™„Çπ„ÇØË≥áÁî£„ÅÆÈÖçÂàÜÊØîÁéá„ÄÅÂâ≤ÂêàÔºâ</span>)`
        
        Input: „ÇØ„É¨„Ç∏„ÉÉ„ÉàÔºà‰ø°Áî®ÔºâÂ∏ÇÂ†¥
        Output: 
        `<span style="color:red;">„ÇØ„É¨„Ç∏„ÉÉ„ÉàÔºà‰ø°Áî®ÔºâÂ∏ÇÂ†¥</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„ÇØ„É¨„Ç∏„ÉÉ„ÉàÔºà‰ø°Áî®ÔºâÂ∏ÇÂ†¥</s> ‚Üí ‰ø°Áî®„É™„Çπ„ÇØÔºàË≥áÈáë„ÅÆÂÄü„ÇäÊâã„ÅÆ‰ø°Áî®Â∫¶„ÅåÂ§âÂåñ„Åô„Çã„É™„Çπ„ÇØÔºâ„ÇíÂÜÖÂåÖ„Åô„ÇãÂïÜÂìÅÔºà„ÇØ„É¨„Ç∏„ÉÉ„ÉàÂïÜÂìÅÔºâ„ÇíÂèñÂºï„Åô„ÇãÂ∏ÇÂ†¥„ÅÆÁ∑èÁß∞„ÄÇ„ÄÄ‰ºÅÊ•≠„ÅÆ‰ø°Áî®„É™„Çπ„ÇØ„ÇíÂèñÂºï„Åô„ÇãÂ∏ÇÂ†¥„ÄÇ</span>)`
        
        Input: „Ç∑„Çπ„ÉÜ„Éü„ÉÉ„ÇØ„Éª„É™„Çπ„ÇØ
        Output: 
        `<span style="color:red;">„Ç∑„Çπ„ÉÜ„Éü„ÉÉ„ÇØ„Éª„É™„Çπ„ÇØ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Ç∑„Çπ„ÉÜ„Éü„ÉÉ„ÇØ„Éª„É™„Çπ„ÇØ</s> ‚Üí ÂÄãÂà•„ÅÆÈáëËûçÊ©üÈñ¢„ÅÆÊîØÊâï‰∏çËÉΩÁ≠â„ÇÑ„ÄÅÁâπÂÆö„ÅÆÂ∏ÇÂ†¥„Åæ„Åü„ÅØÊ±∫Ê∏à„Ç∑„Çπ„ÉÜ„É†Á≠â„ÅÆÊ©üËÉΩ‰∏çÂÖ®„Åå„ÄÅ‰ªñ„ÅÆÈáëËûçÊ©üÈñ¢„ÄÅ‰ªñ„ÅÆÂ∏ÇÂ†¥„ÄÅ„Åæ„Åü„ÅØÈáëËûç„Ç∑„Çπ„ÉÜ„É†ÂÖ®‰Ωì„Å´Ê≥¢Âèä„Åô„Çã„É™„Çπ„ÇØ</span>)`
        
        Input: „Éá„Ç£„Çπ„Éà„É¨„ÇπÂÇµÂà∏
        Output: 
        `<span style="color:red;">„Éá„Ç£„Çπ„Éà„É¨„ÇπÂÇµÂà∏</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Éá„Ç£„Çπ„Éà„É¨„ÇπÂÇµÂà∏</s> ‚Üí ‰ø°Áî®‰∫ãÁî±„Å™„Å©„Å´„Çà„Çä„ÄÅ‰æ°Ê†º„ÅåËëó„Åó„Åè‰∏ãËêΩ„Åó„ÅüÂÇµÂà∏</span>)`
        
        Input: „Éá„Ç£„Éï„Çß„É≥„Ç∑„Éñ
        Output: 
        `<span style="color:red;">„Éá„Ç£„Éï„Çß„É≥„Ç∑„Éñ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Éá„Ç£„Éï„Çß„É≥„Ç∑„Éñ</s> ‚Üí „Éá„Ç£„Éï„Çß„É≥„Ç∑„ÉñÔºàÊôØÊ∞ó„Å´Â∑¶Âè≥„Åï„Çå„Å´„Åè„ÅÑÔºâ</span>)`
        
        Input: „ÉÜ„ÇØ„Éã„Ç´„É´
        Output: 
        `<span style="color:red;">„ÉÜ„ÇØ„Éã„Ç´„É´</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„ÉÜ„ÇØ„Éã„Ç´„É´</s> ‚Üí „ÉÜ„ÇØ„Éã„Ç´„É´ÔºàÈÅéÂéª„ÅÆÊ†™‰æ°„ÅÆÂãï„Åç„Åã„ÇâÂà§Êñ≠„Åô„Çã„Åì„Å®</span>)`
        
        Input: „Éá„Éï„Ç©„É´„Éà
        Output: 
        `<span style="color:red;">„Éá„Éï„Ç©„É´„Éà</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Éá„Éï„Ç©„É´„Éà</s> ‚Üí „Éá„Éï„Ç©„É´„ÉàÔºàÂÇµÂãô‰∏çÂ±•Ë°åÔºâ</span>)`
        
        Input: „Éá„Éï„Ç©„É´„ÉàÂÇµ
        Output: 
        `<span style="color:red;">„Éá„Éï„Ç©„É´„ÉàÂÇµ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Éá„Éï„Ç©„É´„ÉàÂÇµ</s> ‚Üí „Éá„Éï„Ç©„É´„Éà„Å®„ÅØ‰∏ÄËà¨ÁöÑ„Å´„ÅØÂÇµÂà∏„ÅÆÂà©Êâï„ÅÑ„Åä„Çà„Å≥ÂÖÉÊú¨ËøîÊ∏à„ÅÆ‰∏çÂ±•Ë°å„ÄÅ„ÇÇ„Åó„Åè„ÅØÈÅÖÂª∂„Å™„Å©„Çí„ÅÑ„ÅÑ„ÄÅ„Åì„ÅÆ„Çà„ÅÜ„Å™Áä∂ÊÖã„Å´„ÅÇ„ÇãÂÇµÂà∏„Çí„Éá„Éï„Ç©„É´„ÉàÂÇµ„Å®„ÅÑ„ÅÑ„Åæ„Åô„ÄÇ</span>)`
        
        Input: „Éá„É•„É¨„Éº„Ç∑„Éß„É≥
        Output: 
        `<span style="color:red;">„Éá„É•„É¨„Éº„Ç∑„Éß„É≥</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Éá„É•„É¨„Éº„Ç∑„Éß„É≥</s> ‚Üí „Éá„É•„É¨„Éº„Ç∑„Éß„É≥ÔºàÈáëÂà©ÊÑüÂøúÂ∫¶Ôºâ</span>)`
        
        Input: ÊäïË≥áÈÅ©Ê†ºÂÇµ
        Output: 
        `<span style="color:red;">ÊäïË≥áÈÅ©Ê†ºÂÇµ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">ÊäïË≥áÈÅ©Ê†ºÂÇµ</s> ‚Üí Ê†º‰ªòÊ©üÈñ¢„Å´„Çà„Å£„Å¶Ê†º‰ªò„Åë„Åï„Çå„ÅüÂÖ¨Á§æÂÇµ„ÅÆ„ÅÜ„Å°„ÄÅÂÇµÂãô„ÇíÂ±•Ë°å„Åô„ÇãËÉΩÂäõ„ÅåÂçÅÂàÜ„Å´„ÅÇ„Çã„Å®Ë©ï‰æ°„Åï„Çå„ÅüÂÖ¨Á§æÂÇµ</span>)`
        
        Input: „Éï„Ç°„É≥„ÉÄ„É°„É≥„Çø„É´„Ç∫
        Output: 
        `<span style="color:red;">„Éï„Ç°„É≥„ÉÄ„É°„É≥„Çø„É´„Ç∫</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Éï„Ç°„É≥„ÉÄ„É°„É≥„Çø„É´„Ç∫</s> ‚Üí „Éï„Ç°„É≥„ÉÄ„É°„É≥„Çø„É´„Ç∫ÔºàË≥ÉÊñô„ÇÑÁ©∫ÂÆ§Áéá„ÄÅÈúÄÁµ¶Èñ¢‰øÇ„Å™„Å©„ÅÆÂü∫Á§éÁöÑÊù°‰ª∂Ôºâ‚ÄªREIT„Éï„Ç°„É≥„Éâ„Åß‰ΩøÁî®„Åô„Çã</span>)`
        
        Input: „Éï„É™„Éº„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº
        Output: 
        `<span style="color:red;">„Éï„É™„Éº„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Éï„É™„Éº„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº</s> ‚Üí Á®éÂºïÂæåÂñ∂Ê•≠Âà©Áõä„Å´Ê∏õ‰æ°ÂÑüÂç¥Ë≤ª„ÇíÂä†„Åà„ÄÅË®≠ÂÇôÊäïË≥áÈ°ç„Å®ÈÅãËª¢Ë≥áÊú¨„ÅÆÂ¢óÂä†„ÇíÂ∑Æ„ÅóÂºï„ÅÑ„Åü„ÇÇ„ÅÆ</span>)`
        
        Input: „Éô„Éº„Ç∏„É•„Éñ„ÉÉ„ÇØ
        Output: 
        `<span style="color:red;">„Éô„Éº„Ç∏„É•„Éñ„ÉÉ„ÇØ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„Éô„Éº„Ç∏„É•„Éñ„ÉÉ„ÇØ</s> ‚Üí „Çπ„Éö„Éº„Çπ„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅ„Éô„Éº„Ç∏„É•„Éñ„ÉÉ„ÇØÔºàÁ±≥Âú∞Âå∫ÈÄ£ÈäÄÁµåÊ∏àÂ†±ÂëäÔºâ</span>)`
        
        Input: „É¢„É°„É≥„Çø„É†
        Output: 
        `<span style="color:red;">„É¢„É°„É≥„Çø„É†</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„É¢„É°„É≥„Çø„É†</s> ‚Üí  Áõ∏Â†¥„ÅÆÂã¢„ÅÑ)„ÅåÂº∑„Åè„ÄÅÊäïË≥áÂÆ∂„Åü„Å°„ÅØÁü≠ÊúüÁöÑ„Å™Âà©Áõä„ÇíÁãô„Å£„Å¶„ÅÑ„Åæ„Åô„ÄÇ</span>)`
        
        Input: „É™„Ç™„Éº„Éó„É≥
        Output: 
        `<span style="color:red;">„É™„Ç™„Éº„Éó„É≥</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„É™„Ç™„Éº„Éó„É≥</s> ‚Üí „É™„Ç™„Éº„Éó„É≥/„É™„Ç™„Éº„Éó„Éã„É≥„Ç∞ÔºàÁµåÊ∏àÊ¥ªÂãïÂÜçÈñãÔºâ</span>)`
        
        Input: „É™„Çπ„ÇØ„Éó„É¨„Éü„Ç¢„É†
        Output: 
        `<span style="color:red;">„É™„Çπ„ÇØ„Éó„É¨„Éü„Ç¢„É†</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„É™„Çπ„ÇØ„Éó„É¨„Éü„Ç¢„É†</s> ‚Üí Âêå„ÅòÊäïË≥áÊúüÈñìÂÜÖ„Å´„Åä„ÅÑ„Å¶„ÄÅ„ÅÇ„Çã„É™„Çπ„ÇØË≥áÁî£„ÅÆÊúüÂæÖÂèéÁõäÁéá„Åå„ÄÅÁÑ°„É™„Çπ„ÇØË≥áÁî£ÔºàÂõΩÂÇµ„Å™„Å©Ôºâ„ÅÆÂèéÁõäÁéá„Çí‰∏äÂõû„ÇãÂπÖ„ÅÆ„Åì„Å®„ÄÇ</span>)`
        
        Input: „É™„Éï„É¨„Éº„Ç∑„Éß„É≥
        Output: 
        `<span style="color:red;">„É™„Éï„É¨„Éº„Ç∑„Éß„É≥</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â§ñÊù•Ë™û„ÉªÂ∞ÇÈñÄÁî®Ë™û <s style="background:yellow;color:red">„É™„Éï„É¨„Éº„Ç∑„Éß„É≥</s> ‚Üí „É™„Éï„É¨„Éº„Ç∑„Éß„É≥**„Éá„Éï„É¨„Éº„Ç∑„Éß„É≥„Åã„ÇâÊäú„Åë„Å¶„ÄÅ„Åæ„Å†„ÄÅ„Ç§„É≥„Éï„É¨„Éº„Ç∑„Éß„É≥„Å´„ÅØ„Å™„Å£„Å¶„ÅÑ„Å™„ÅÑÁä∂Ê≥Å„ÅÆ„Åì„Å®„ÄÇ</span>)`
        
        **„Äê‰æãÂ§ñÁî®Ë™û ‚Äì ‰øÆÊ≠£„Åó„Å™„ÅÑ„Åì„Å®„Äë**
        - „Ç≥„É≠„ÉäÁ¶ç
        - „Ç≥„É≠„Éä„Ç∑„Éß„ÉÉ„ÇØ
        - Êñ∞Âûã„Ç≥„É≠„ÉäÁ¶ç
        - ‰ΩèÂÆÖ„É≠„Éº„É≥
        - Âºï„ÅçÁ∑†„ÇÅÁ≠ñ
        - Âºï„ÅçÁ∑†„ÇÅÊîøÁ≠ñ
        - ÁµÑÂÖ•ÊØîÁéá
        - Ê†º‰ªòÊ©üÈñ¢
        - Ê†º‰ªòÂà•
        - ÂõΩÂÇµË≤∑ÂÖ•„Ç™„Éö

        **ÁâπÂÆöË°®Áèæ„ÅÆË®Ä„ÅÑÊèõ„Åà„É´„Éº„É´ÔºàÊñáËÑàÂà§Êñ≠„Çí‰º¥„ÅÜ‰øÆÊ≠£Ôºâ:
        ÊñáËÑà„Å´Âøú„Åò„Å¶„ÄÅÂÖ∑‰ΩìÁöÑ„Å™Ë°®Áèæ„Å´Ë®Ä„ÅÑÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        „Äå„Åæ„Å°„Åæ„Å°„Äç„ÅÆ‰ΩøÁî®
        „Äå„Åæ„Å°„Åæ„Å°„Äç„Å®„ÅÑ„ÅÜÊõñÊòß„Å™Ë°®Áèæ„ÅåÂá∫Áèæ„Åó„ÅüÂ†¥Âêà„ÅØ„ÄÅ„Åù„ÅÆË™û„Çí„Åù„ÅÆ„Åæ„Åæ‰øùÊåÅ„Åó„Åü‰∏ä„Åß„ÄÅÂæå„Å´„Äå‰øÆÊ≠£ÁêÜÁî±: ÊõñÊòßË°®Áèæ„ÅÆÊòéÁ¢∫Âåñ„Äç„ÇíË£úË∂≥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

        Â§âÊèõË™û„ÄåÁï∞„Å™„ÇãÂãï„Åç„Äç„ÇÇË°®Á§∫„Åó„Åæ„Åô„Åå„ÄÅÂéüÊñá„ÅØÂ§âÊõ¥„Åõ„Åö„ÄÅË£ÖÈ£æ„ÅßÁ§∫„Åô„ÅÆ„Åø„Åß„Åô„ÄÇ

        Output Format (Original term preserved, only correction reason shown):
        <span style="color:red;">„Åæ„Å°„Åæ„Å°</span>
        (<span>‰øÆÊ≠£ÁêÜÁî±: ÊõñÊòßË°®Áèæ„ÅÆÊòéÁ¢∫Âåñ <s style="background:yellow;color:red">„Åæ„Å°„Åæ„Å°</s> ‚Üí Áï∞„Å™„ÇãÂãï„Åç</span>)

        -„ÄåË°å„Å£„Å¶Êù•„ÅÑ„Äç„ÅÆË°®Áèæ

        ÊñáËÑà„Å´Âøú„Åò„Å¶„ÄÅ„Äå‰∏äÊòáÔºà‰∏ãËêΩÔºâ„Åó„Åü„ÅÆ„Å°‰∏ãËêΩÔºà‰∏äÊòáÔºâ„Äç„ÅÆ„Çà„ÅÜ„Å´ÊòéÁ¢∫„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        Exsample:

        Input: Áõ∏Â†¥„ÅØË°å„Å£„Å¶Êù•„ÅÑ„ÅÆÂ±ïÈñã„Å®„Å™„Å£„Åü
        Output: Áõ∏Â†¥„ÅØ‰∏äÊòá„Åó„Åü„ÅÆ„Å°‰∏ãËêΩ„Åô„ÇãÂ±ïÈñã„Å®„Å™„Å£„Åü
        
        Â§âÊèõË™û„ÄåË°å„Å£„Å¶Êù•„ÅÑ„Äç„ÇÇË°®Á§∫„Åó„Åæ„Åô„Åå„ÄÅÂéüÊñá„ÅØÂ§âÊõ¥„Åõ„Åö„ÄÅË£ÖÈ£æ„ÅßÁ§∫„Åô„ÅÆ„Åø„Åß„Åô„ÄÇ

        Output Format (Original term preserved, only correction reason shown):
        
        ‰øÆÊ≠£ÁêÜÁî±: Ë°®Áèæ„ÅÆÊòéÁ¢∫Âåñ
        <span style="color:red;">Ë°å„Å£„Å¶Êù•„ÅÑ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ë°®Áèæ„ÅÆÊòéÁ¢∫Âåñ <s style="background:yellow;color:red">Original Term</s> ‚Üí Ë°å„Å£„Å¶Êù•„ÅÑ</span>)

        -„ÄåÊ®™„Å∞„ÅÑ„ÄçË°®Áèæ„ÅÆÈÅ©Ê≠£‰ΩøÁî®

        Â∞èÂπÖ„Å™Â§âÂãï„Åß„ÅÇ„Çå„Å∞„ÄåÊ®™„Å∞„ÅÑ„Äç„Çí‰ΩøÁî®ÂèØËÉΩ„ÄÇ
        Â§ß„Åç„Å™Â§âÂãï„ÅÆÊú´„Å´ÂêåÊ∞¥Ê∫ñ„ÅßÁµÇ‰∫Ü„Åó„ÅüÂ†¥Âêà„ÅØ„ÄÅ„Äå„Åª„ÅºÂ§â„Çè„Çâ„Åö„Äç„ÄåÂêåÁ®ãÂ∫¶„Å®„Å™„Çã„Äç„Å™„Å©„Å´‰øÆÊ≠£„ÄÇ
        
        Â§âÊèõË™û„ÄåÊ®™„Å∞„ÅÑ„Äç„ÇÇË°®Á§∫„Åó„Åæ„Åô„Åå„ÄÅÂéüÊñá„ÅØÂ§âÊõ¥„Åõ„Åö„ÄÅË£ÖÈ£æ„ÅßÁ§∫„Åô„ÅÆ„Åø„Åß„Åô„ÄÇ
        Output Format (Original term preserved, only correction reason shown):

        ‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÈÅ©Ê≠£‰ΩøÁî®
        <span style="color:red;">Ê®™„Å∞„ÅÑ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÈÅ©Ê≠£‰ΩøÁî® <s style="background:yellow;color:red">Ê®™„Å∞„ÅÑ</s> ‚Üí „Åª„ÅºÂ§â„Çè„Çâ„Åö</span>)

        -„ÄåÔºàÂâ≤ÂÆâ„Å´ÔºâÊîæÁΩÆ„ÄçË°®Áèæ„ÅÆ‰øÆÊ≠£

        „ÄåÂâ≤ÂÆâÊÑü„ÅÆ„ÅÇ„Çã„Äç„Å™„Å©„ÄÅ„Çà„ÇäÈÅ©Âàá„Å™Ë°®Áèæ„Å´‰øÆÊ≠£„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

        Exsample:
        Input: Ê†™‰æ°„ÅØÂâ≤ÂÆâ„Å´ÊîæÁΩÆ„Åï„Çå„Åü
        Output: Ê†™‰æ°„Å´„ÅØÂâ≤ÂÆâÊÑü„Åå„ÅÇ„ÇãÁä∂ÊÖã„ÅåÁ∂ö„ÅÑ„Åü

        Â§âÊèõË™û„ÄåÔºàÂâ≤ÂÆâ„Å´ÔºâÊîæÁΩÆ„Äç„ÇÇË°®Á§∫„Åó„Åæ„Åô„Åå„ÄÅÂéüÊñá„ÅØÂ§âÊõ¥„Åõ„Åö„ÄÅË£ÖÈ£æ„ÅßÁ§∫„Åô„ÅÆ„Åø„Åß„Åô„ÄÇ
        Output Format (Original term preserved, only correction reason shown):

        ‰øÆÊ≠£ÁêÜÁî±: Ë°®Áèæ„ÅÆÊòéÁ¢∫Âåñ„Å®ÂÆ¢Ë¶≥ÊÄß„ÅÆÂêë‰∏ä
        <span style="color:red;">ÔºàÂâ≤ÂÆâ„Å´ÔºâÊîæÁΩÆ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ë°®Áèæ„ÅÆÊòéÁ¢∫Âåñ„Å®ÂÆ¢Ë¶≥ÊÄß„ÅÆÂêë‰∏ä <s style="background:yellow;color:red"ÔºàÂâ≤ÂÆâ„Å´ÔºâÊîæÁΩÆ</s> ‚Üí Ââ≤ÂÆâÊÑü„ÅÆ„ÅÇ„Çã</span>)

        """  
        # ChatCompletion Call
        response = openai.ChatCompletion.create(
        # OpenAI API Ë∞ÉÁî® asyncio
        # loop = asyncio.get_event_loop()
        # response = await loop.run_in_executor(None, lambda: openai.ChatCompletion.create(
            deployment_id=deployment_id,  # Deploy Name
            messages=[
                {"role": "system", "content": "You are a professional Japanese text proofreading assistant."
                "This includes not only Japanese text but also English abbreviations (Ëã±Áï•Ë™û), "
                "foreign terms (Â§ñÊù•Ë™û),and specialized terminology (Â∞ÇÈñÄÁî®Ë™û)."},
                {"role": "user", "content": prompt_result}
            ],
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE,
            seed=SEED
        )
        answer = response['choices'][0]['message']['content'].strip()
        re_answer = remove_code_blocks(answer)
        
        return jsonify({"success": True, "corrected_text": re_answer})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

def convert_logs(items):
    converted_data = {
        "code": 200,
        "message": "ÊàêÂäü",
        "data": []
    }
    
    for idx, item in enumerate(items):
        log_entries = item.get("logEntries", [])
        
        for log_idx, log_entry in enumerate(log_entries):
            log_parts = log_entry.split(" - ")
            timestamp_str = log_parts[0] if len(log_parts) > 1 else ""
            message = log_parts[1] if len(log_parts) > 1 else ""
            
            log_data = {
                "id": idx * len(log_entries) + log_idx + 1,  # ID
                "name": message,  # message[:30] message split pre 30 'name'
                "status": "ÂÆå‰∫Ü(‰øÆÊ≠£„ÅÇ„Çä)" if "‚úÖ SUCCESS" in message else "„Ç®„É©„Éº",
                "timeclock": timestamp_str,
                "progress": "ÊàêÂäü" if "‚úÖ SUCCESS" in message else "„Ç®„É©„Éº",
                "timestamp": timestamp_str,
                "selected": False
            }
            
            converted_data["data"].append(log_data)
    
    return converted_data

# appLog
APPLOG_CONTAINER_NAME='appLog'
@app.route('/api/applog', methods=['GET'])
def get_applog():
    # Cosmos DB ËøûÊé•
    container = get_db_connection(APPLOG_CONTAINER_NAME)

    query = "SELECT * FROM c"
    items = list(container.query_items(query=query, enable_cross_partition_query=True))

    for item in items:
        item['id'] = item['id']

    converted_logs = convert_logs(items)
    return jsonify(converted_logs)

# azure Cosmos DB
@app.route('/api/faqs', methods=['GET'])
def get_faq():
    # Cosmos DB ÈìæÊé•ÂÆ¢Êà∑Á´Ø,ENV
    container=get_db_connection()

    query = "SELECT * FROM c"
    items = list(container.query_items(query=query, enable_cross_partition_query=True))

    for item in items:
        item['id'] = item['id']

    return jsonify(items)

@app.route('/api/tenbrend', methods=['POST'])
def tenbrend():
    data = request.get_json() or {}

    raw_fcode = data.get('fcode', '').strip()
    months = data.get('month', '').strip()
    stocks = data.get('stock', '').strip()
    fund_type = data.get('fundType', 'public').strip()  # ÈªòËÆ§‰∏∫ÂÖ¨Âãü

    # Ê†πÊçÆ fundType ÈÄâÊã©ÂÆπÂô®ÔºàÂç≥ Cosmos DB ÁöÑË°®Ôºâ

    if fund_type == 'private':
        TENBREND_CONTAINER_NAME = 'tenbrend_private'
    else:
        TENBREND_CONTAINER_NAME = 'tenbrend'

    # Cosmos DB ÈìæÊé•ÂÆ¢Êà∑Á´Ø
    container = get_db_connection(TENBREND_CONTAINER_NAME)
    parameters = []
    if not raw_fcode:
        query = "SELECT * FROM c"

    else:
        # ÊûÑÂª∫ SQL Êü•ËØ¢
        if '-' in raw_fcode:
            # Â∏¶ `-` ÁöÑÁõ¥Êé•Áî®Â≠óÁ¨¶‰∏≤Ê®°Á≥äÂåπÈÖç
            query = "SELECT * FROM c WHERE CONTAINS(c.fcode, @fcode)"
            parameters.append({"name": "@fcode", "value": raw_fcode})
        else:
            try:
                fcode_num = raw_fcode
                query = "SELECT * FROM c WHERE c.fcode = @fcode"
                parameters.append({"name": "@fcode", "value": fcode_num})
            except ValueError:
                # fallback Âà∞Â≠óÁ¨¶‰∏≤Êü•ËØ¢
                query = "SELECT * FROM c WHERE CONTAINS(c.fcode, @fcode)"
                parameters.append({"name": "@fcode", "value": raw_fcode})

        if months:
            query += " AND c.months = @months"
            parameters.append({"name": "@months", "value": months})

        if stocks:
            query += " AND CONTAINS(c.stocks, @stocks)"
            parameters.append({"name": "@stocks", "value": stocks})

    items = list(container.query_items(
        query=query,
        parameters=parameters,
        enable_cross_partition_query=True
    ))

    filtered_items = [item for item in items if item.get('id')]
    return jsonify({"code": 200, "data": filtered_items})


@app.route('/api/tenbrend/months', methods=['POST'])
def tenbrend_months():
    data = request.get_json() or {}

    fcode = data.get('fcode', '').strip()
    stocks = data.get('stock', '').strip() if data.get('stock') else ''
    fund_type = data.get('fundType', 'public').strip()

    if not fcode:
        return jsonify({"code": 400, "message": "fcode is required"}), 400

    # ‚úÖ Ê†πÊçÆ fundType ÂàáÊç¢ÂÆπÂô®ÔºàË°®Ôºâ
    if fund_type == 'private':
        TENBREND_CONTAINER_NAME ='tenbrend_private'
    else:
        TENBREND_CONTAINER_NAME='tenbrend'

    # Cosmos DB ÈìæÊé•ÂÆ¢Êà∑Á´Ø
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    query = "SELECT c.months FROM c WHERE CONTAINS(c.fcode, @fcode)"
    parameters = [{"name": "@fcode", "value": fcode}]

    if stocks:
        query += " AND CONTAINS(c.stocks, @stocks)"
        parameters.append({"name": "@stocks", "value": stocks})

    try:
        items = list(container.query_items(
            query=query,
            parameters=parameters,
            enable_cross_partition_query=True
        ))

        months = sorted({item.get('months') for item in items if item.get('months')})
        return jsonify({"code": 200, "data": months})
    except Exception as e:
        print("‚ùå Cosmos DB query failed:", e)
        return jsonify({"code": 500, "message": "internal error"}), 500


@app.route('/api/tenbrend/stocks', methods=['POST'])
def tenbrend_stocks():
    data = request.get_json() or {}

    fcode = data.get('fcode', '').strip()
    months = data.get('month', '').strip() if data.get('month') else ''
    fund_type = data.get('fundType', 'public').strip()

    if not fcode:
        return jsonify({"code": 400, "message": "fcode is required"}), 400

    if fund_type == 'private':
        TENBREND_CONTAINER_NAME ='tenbrend_private'
    else:
        TENBREND_CONTAINER_NAME='tenbrend'

    # Cosmos DB ÈìæÊé•ÂÆ¢Êà∑Á´Ø
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    query = "SELECT c.stocks FROM c WHERE CONTAINS(c.fcode, @fcode)"
    parameters = [{"name": "@fcode", "value": fcode}]

    if months:
        query += " AND c.months = @months"
        parameters.append({"name": "@months", "value": months})

    try:
        items = list(container.query_items(
            query=query,
            parameters=parameters,
            enable_cross_partition_query=True
        ))

        stocks = sorted({item.get('stocks') for item in items if item.get('stocks')})
        return jsonify({"code": 200, "data": stocks})
    except Exception as e:
        print("‚ùå Cosmos DB query failed:", e)
        return jsonify({"code": 500, "message": "internal error"}), 500




@app.route('/api/tenbrend/template', methods=['GET'])
def download_excel_template():
    data = request.get_json() or {}
    # ÈªòËÆ§ÊòØ‚ÄúÂÖ¨Âãü‚Äù
    fund_type = data.get('fundType', 'public').strip()

    # Ê†πÊçÆÁ±ªÂûãÊãºÊé•Ë∑ØÂæÑ
    if fund_type == 'ÁßÅÂãü':
        file_url = ACCOUNT_URL + STORAGE_CONTAINER_NAME +"/10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÁßÅÂãü.xlsx"
    else:
        file_url = ACCOUNT_URL + STORAGE_CONTAINER_NAME +"/10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÂÖ¨Âãü.xlsx"

    try:
        # Ê≥®ÊÑè:send_file ‰∏çËÉΩÁõ¥Êé•‰∏ãËΩΩËøúÁ®ãÈìæÊé•ÔºåÊîπ‰∏∫ÈáçÂÆöÂêë
        return redirect(file_url)
    except Exception as e:
        return jsonify({"code": 500, "message": str(e)}), 500



# Data transfer
def transform_data(items,fund_type):
    menu_data = {
        "ÂÖ¨Âãü": [],
        "ÁßÅÂãü": []
    }

    for item in items:
        if fund_type == 'public':
            fund_category = menu_data["ÂÖ¨Âãü"]
        elif fund_type == 'private':
            fund_category = menu_data["ÁßÅÂãü"]
        else:
            continue  # ÏûòÎ™ªÎêú fund_typeÏùÄ Î¨¥Ïãú

        # Îç∞Ïù¥ÌÑ∞ Íµ¨Ï°∞Ïóê ÎßûÍ≤å Î≥ÄÌôò
        reference = {
            "id": "reference",
            "name": "üìÅ ÂèÇÁÖß„Éï„Ç°„Ç§„É´",
            "children": [
                {
                    "id": "report_data",
                    "name": "üìÇ „É¨„Éù„Éº„Éà„Éá„Éº„Çø",
                    "children": []
                },
                {
                    "id": "mingbing_data",
                    "name": "üìÇ 10ÈäòÊüÑËß£Ë™¨‰∏ÄË¶ßË°®",
                    "children": []
                }
            ]
        }

        # report_dataÈáåÊ∑ªÂä†item
        reference["children"][0]["children"].append({
            "id": item.get('id'),
            "name": item.get('fileName'),
            "icon": "üìÑ",
            "file": item.get('fileName'),
            "pdfPath": extract_pdf_path(item.get('link')),
        })

        # mingbing_dataÈáåÊ∑ªÂä†item
        reference["children"][1]["children"].append({
            "id": item.get('id'),
            "name": item.get('fileName'),
            "icon": "üìÑ",
            "file": item.get('fileName'),
            "pdfPath": extract_pdf_path(item.get('link'))
        })

        fund_category.append(reference)

        # checked_files ËøΩÂä†session
        checked_files = {
            "id": "checked_files",
            "name": "üìÅ „ÉÅ„Çß„ÉÉ„ÇØÂØæË±°„Éï„Ç°„Ç§„É´",
            "children": [
                {
                    "id": "individual_comments",
                    "name": "üìÇ ÂÖ±ÈÄö„Ç≥„É°„É≥„Éà„Éï„Ç°„Ç§„É´",
                    "children": []
                },
                {
                    "id": "kobetsucomment",
                    "name": "üìÇ ÂÄãÂà•„Ç≥„É°„É≥„Éà„Éï„Ç°„Ç§„É´",
                    "children": []
                }
            ]
        }

        # individual_commentsÈáåÊ∑ªÂä†item
        checked_files["children"][0]["children"].append({
            "id": item.get('id'),  
            "name": item.get('fileName'),  
            "icon": "‚ö†Ô∏è",
            "file": item.get('fileName'),  
            "status": item.get('comment_status'),  
            "readStatus": item.get('comment_readStatus'),  
            "pdfPath": extract_pdf_path(item.get('link'))  
        })

        # kobetsucommentÈáåÊ∑ªÂä†item
        checked_files["children"][1]["children"].append({
            "id": item.get('id'),  
            "name": item.get('fileName'),
            "icon": "‚ùå",
            "file": item.get('fileName'),
            "status": item.get('individual_status'),
            "readStatus": item.get('individual_readStatus'),
            "pdfPath": extract_pdf_path(item.get('link'))  
        })

        fund_category.append(checked_files)

    return menu_data

def extract_pdf_path(link):
    match = re.search(r'href="([^"]+)"', link)
    return match.group(1) if match else ""

def extract_base_name(file_path):
    file_name = os.path.basename(file_path)
    base_name, _ = os.path.splitext(file_name)
    return base_name

# public_Fund and private_Fund
@app.route('/api/fund', methods=['POST'])
def handle_fund():
    fund_type = request.json.get('type')
    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400

    container_name = f"{fund_type}_Fund"
    
    try:
        # Cosmos DB ËøûÊé•
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        query = "SELECT * FROM c"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        
        # filtered_items = [item for item in items if item and item.get('id')]
        
        # return jsonify(filtered_items)
        formatted_data = transform_data(items,fund_type)

        return jsonify(formatted_data)
        
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500

# 625 tenbrend
def convert_to_tenbrend(items):
    corrections = []

    for item in items:
        old_text = item.get("ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "").strip()
        new_text = item.get("Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "").strip()

        if old_text != new_text:
            corrections.append({
                "check_point": "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨",
                "comment": f"{old_text} ‚Üí {new_text}",
                "intgr": False,
                "locations": [],
                "original_text": new_text,
                "page": '',
                "reason_type": item.get("stocks", "")
            })

    return corrections


# 509 debug
def convert_format(filtered_items):
    checkResults = {}

    for correction in filtered_items.get("result", {}).get("corrections", []):
        page = correction["page"] + 1
        position = {}
        colorSet = "rgb(172 228 230)"

        change = {
            "before": correction["original_text"],
            "after": correction["comment"].split("‚Üí")[-1].strip(),
        }
        if correction["intgr"]:
            name = "‰∏ç‰∏ÄËá¥"
            colorSet = "rgba(172, 228, 230, 0.5)"
        else:
            name = ""
            colorSet= "rgba(255, 255, 0, 0.5)"

        if correction["locations"]:
            # for idx, loc in enumerate(correction["locations"]): 
            # checkResults
            if page not in checkResults:
                checkResults[page] = [{"title": filtered_items["fileName"], "items": []}]

            # loc = correction["locations"][0]
            for loc in correction["locations"]:
                pdf_height = loc.get("pdf_height", 792)  # PDF height (Default: A4 , 792pt)

                # x = loc["x0"] - 22 if idx == 0 else loc["x0"]
                position = {
                    "x": loc["x0"],
                    "y": pdf_height - loc["y1"] + 50,
                    "width": loc["x1"] - loc["x0"],
                    "height": loc["y1"] - loc["y0"],
                }

                if correction["intgr"]:
                    checkResults[page][0]["items"].append({
                        "name": name,
                        "color": colorSet, #"rgba(255, 255, 0, 0.5)", # green background rgba(0, 255, 0, 0.5)
                        "page": page,
                        "position": position,
                        "changes": [change],
                        "reason_type":correction["reason_type"],
                        "check_point":correction["check_point"],
                        "original_text":correction["original_text"],
                        })
                else:
                        existing_item = any(
                                item["name"] == name and
                                item["changes"] == [change] and
                                item["position"] == position
                                for item in checkResults[page][0]["items"]
                            )
                        if not existing_item:
                            checkResults[page][0]["items"].append({
                                "name": name,
                                "color": colorSet, #"rgba(255, 255, 0, 0.5)", # green background rgba(0, 255, 0, 0.5)
                                "page": page,
                                "position": position,
                                "changes": [change],
                                "reason_type":correction["reason_type"],
                                "check_point":correction["check_point"],
                                "original_text":correction["original_text"],
                                })


    return {'data': checkResults, 'code': 200}

# public_Fund and check-results
@app.route('/api/check_results', methods=['POST'])
def handle_check_results():
    fund_type = request.json.get('type')
    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400
    
    selected_id = request.json.get('selectedId')
    if not selected_id:
        return jsonify({"error": "selectedId is required"}), 400
    
    pageNumber = request.json.get('pageNumber')
    if not pageNumber:
        return jsonify({"error": "pageNumber is required"}), 400

    container_name = f"{fund_type}_Fund"
    
    try:
        # Cosmos DB ËøûÊé•
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        query = "SELECT * FROM c WHERE c.id = @id"
        parameters = [{"name": "@id", "value": selected_id}]
        items = list(container.query_items(query=query, parameters=parameters, enable_cross_partition_query=True))
        
        if not items:
            return jsonify({"error": "Item not found"}), 404

        converted_data = convert_format(items[0])

        return jsonify(converted_data)
        
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500

# get side bar
@app.route('/api/menu', methods=['POST'])
def handle_menu():
    fund_type = request.json.get('type')
    page = int(request.json.get('page', 1))
    page_size = int(request.json.get('page_size', 10))
    # user_name = request.json.get('user_name')

    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400

    # container name Setting
    container_name = f"{fund_type}_Fund"

    
    try:
        # Cosmos DB ËøûÊé•
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        # Query exe
        query = "SELECT * FROM c WHERE CONTAINS(c.id, '.pdf') OR c.upload_type='ÂèÇÁÖß„Éï„Ç°„Ç§„É´'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        
        # filter result
        filtered_items = [item for item in items if item and item.get('id')]

        # pagenations
        total = len(filtered_items)
        start = (page - 1) * page_size
        end = start + page_size
        paged_items = filtered_items[start:end]

        response = {
            "code": 200,
            "data": paged_items,
            "total": total
        }

        return jsonify(response)
        
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    
@app.route('/api/menu_all', methods=['POST'])
def handle_menu_all():
    # param check
    fund_type = request.json.get('type')

    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400

    # container name Setting
    container_name = f"{fund_type}_Fund"
    
    try:
        # Cosmos DB ËøûÊé•
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        # Query exe
        query = "SELECT * FROM c WHERE CONTAINS(c.id, '.pdf') OR c.upload_type='ÂèÇÁÖß„Éï„Ç°„Ç§„É´'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        
        # filter result
        filtered_items = [item for item in items if item and item.get('id')]
        response = {
        "code": 200,
        "data": filtered_items
        }

        return jsonify(response)
        
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    
# Cosmos DB Áä∂ÊÄÅÁ°ÆËÆ§ endpoint
MONITORING_CONTAINER_NAME = "monitoring-status"

# Cosmos DB Áä∂ÊÄÅÁ°ÆËÆ§ endpoint
@app.route('/api/monitoring-status', methods=['GET'])
def get_monitoring_status():
    try:
        # Cosmos DB ËøûÊé•
        container = get_db_connection(MONITORING_CONTAINER_NAME)
        
        # Cosmos DBÈáåÂèñÊï∞ÊçÆ
        query = "SELECT * FROM c"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        for item in items:
            item['id'] = item['id']

        return jsonify(items), 200
        
    except CosmosResourceNotFoundError:
        logging.error("Monitoring status document not found")
        return jsonify({"error": "Status document not found"}), 404
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500

# Áä∂ÊÄÅÊõ¥Êñ∞
@app.route('/api/monitoring-status', methods=['PUT'])
def update_monitoring_status():
    try:
        # Cosmos DB ËøûÊé•
        container = get_db_connection(MONITORING_CONTAINER_NAME)
        
        new_status = request.json.get('status', 'off')
        
        status_item = {
            'id': 'monitoring_status',
            'type': 'control',
            'status': new_status,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        container.upsert_item(body=status_item)
        logging.info(f"Monitoring status updated to {new_status}")
        return jsonify({'message': 'Status updated', 'new_status': new_status, 'code': 200}), 200
        
    except CosmosResourceNotFoundError as e:
        logging.error(f"Cosmos DB error: {str(e)}")
        return jsonify({"error": "Database operation failed"}), 500
    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500

# read/unread status change
# Cosmos DB Áä∂ÊÄÅÁ°ÆËÆ§ endpoint
@app.route('/api/update_read_status', methods=['POST'])
def get_read_status():
    fund_type = request.json.get('type')
    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400
    
    selected_id = request.json.get('selectedId')
    if not selected_id:
        return jsonify({"error": "selectedId is required"}), 400

    # container name Setting
    container_name = f"{fund_type}_Fund"
    try:
        # Cosmos DB ËøûÊé•
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        query = "SELECT * FROM c WHERE c.id = @id"
        parameters = [{"name": "@id", "value": selected_id}]
        items = list(container.query_items(query=query, parameters=parameters, enable_cross_partition_query=True))
        
        if not items:
            return jsonify({"error": "Item not found"}), 404

        return jsonify(items[0]), 200
        
    except CosmosResourceNotFoundError:
        logging.error("read status document not found")
        return jsonify({"error": "read Status document not found"}), 404
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    
@app.route('/api/update_read_status', methods=['PUT'])
def update_read_status():
    selected_id = request.json.get('selectedId')
    if not selected_id:
        return jsonify({"error": "selectedId is required"}), 400

    mark = request.json.get('mark')
    if mark not in ['read', 'unread']:
        return jsonify({"error": "Invalid mark value"}), 400

    fund_type = request.json.get('type')
    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400

    # container name Setting
    container_name = f"{fund_type}_Fund"
    
    try:
        # Cosmos DB ËøûÊé•
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        query = "SELECT * FROM c WHERE c.id = @id"
        parameters = [{"name": "@id", "value": selected_id}]
        items = list(container.query_items(query=query, parameters=parameters, enable_cross_partition_query=True))

        if not items:
            return jsonify({"error": "Item not found"}), 404

        status_item = items[0]

        # readStatus Âíå timestamp
        status_item['readStatus'] = mark
        status_item['timestamp'] = datetime.utcnow().isoformat()
        
        container.upsert_item(body=status_item)
        logging.info(f"readStatus updated to {mark} for item {selected_id}")
        return jsonify({'message': 'Status updated', 'new_status': mark, 'code': 200}), 200
        
    except CosmosResourceNotFoundError as e:
        logging.error(f"Cosmos DB error: {str(e)}")
        return jsonify({"error": "Item not found"}), 404
    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500


@app.route("/api/health")
def health_check():
    return "OK", 200

logging.basicConfig(level=logging.INFO)

def get_storage_container():
    """
    Azure AD RBAC ÊñπÂºè Azure Blob StorageÏóê ËøûÊé•, ËøîÂõûContainerClient .
    :return: ContainerClient
    """
    try:
        # BlobServiceClient 
        blob_service_client = BlobServiceClient(account_url=ACCOUNT_URL, credential=credential)
        
        container_client = blob_service_client.get_container_client(STORAGE_CONTAINER_NAME)
        
        print("Connected to Azure Blob Storage via Azure AD RBAC")
        logging.info("Connected to Azure Blob Storage via Azure AD RBAC")
        
        return container_client
    except Exception as e:
        logging.error(f"Azure Blob Storage Connection Error: {e}")
        print(f"Azure Blob Storage Connection Error: {e}")
        raise e
    
def allowed_file(filename):
    """    
    :param filename:
    :return: bool
    """
    ALLOWED_EXTENSIONS = {'pdf', 'xlsx','txt','xls','XLSX','xlm','xlsm','xltx','xltm','xlsb','doc','docx'}   # PDF Âíå Excel  
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/api/test_token', methods=['GET'])
def test_token():
    try:
        token_cache._refresh_token()
        token = token_cache.get_token()
        return jsonify({"access_token": token}), 200
    except Exception as e:
        logging.exception("Token Get Error")
        return jsonify({"message": f"Token Get Error: {str(e)}"}), 500

# uploadpdf,api/brand
def parse_escaped_json(raw_text: str):
    text = raw_text.strip()
    if text.startswith('"') and text.endswith('"'):
        text = text[1:-1]
    
    text = text.replace('```json', '')
    text = text.replace('```', '')

    text = text.replace('""', '"')

    parsed = json.loads(text)
    return parsed

def parse_gpt_response(answer):
    try:
        json_str = re.search(r'\{[\s\S]*?\}', answer).group()
        return json.loads(json_str)
    except (AttributeError, json.JSONDecodeError):
        dict_str = re.search(r'corrected_map\s*=\s*\{[\s\S]*?\}', answer, re.DOTALL)
        if dict_str:
            dict_str = dict_str.group().split('=', 1)[1].strip()
            return ast.literal_eval(dict_str)
        return {}

def detect_corrections(original, corrected):
    matcher = SequenceMatcher(None, original, corrected)
    corrections = {}
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'replace':
            orig_part = original[i1:i2].strip()
            corr_part = corrected[j1:j2].strip()
            if orig_part and corr_part:
                corrections[orig_part] = corr_part
    return corrections

def filter_corrected_map(corrected_map):
    keys_to_remove = [" ", "  "]
    for key in keys_to_remove:
        if key in corrected_map:
            del corrected_map[key]
    return corrected_map

# 512 debug
def apply_corrections(input_text, corrected_map):
    result = input_text


    for original, corrected in corrected_map.items():

        if result == corrected:
            continue

        if re.search(re.escape(corrected), result):
            continue

        pattern_already_corrected = re.compile(
            rf"<span style=\"color:red;\">{re.escape(corrected)}</span>\s*"
            rf"\(<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁµ±‰∏Ä\s*<s style=\"background:yellow;color:red\">{re.escape(original)}</s>\s*‚Üí\s*{re.escape(corrected)}</span>\)"
        )
        if pattern_already_corrected.search(result):
            continue

        # original
        if re.search(original, result):
            replacement = (
                f'<span style="color:red;">{corrected}</span> '
                f'(<span>‰øÆÊ≠£ÁêÜÁî±: Áî®Ë™û„ÅÆÁµ±‰∏Ä '
                f'<s style="background:yellow;color:red">{original}</s> ‚Üí {corrected}</span>)'
            )
            result = result.replace(original, replacement)

    return result


DICTIONARY_CONTAINER_NAME = "dictionary"
def fetch_and_convert_to_dict():
    try:
        container = get_db_connection(DICTIONARY_CONTAINER_NAME)
        query = "SELECT c.original, c.corrected FROM c"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        corrected_dict = {item["original"]: item["corrected"] for item in items if "original" in item and "corrected" in item}

        return corrected_dict

    except CosmosHttpResponseError as e:
        print(f"‚ùå DB error: {e}")
        return {}
    
@app.route('/api/check_upload', methods=['POST'])
def check_upload():
    if 'files' not in request.files:
        return jsonify({"success": False, "message": "No files part"}), 400

    files = request.files.getlist('files')
    file_type = request.form.get("fileType")
    fund_type = request.form.get("fundType")

    for file in files:
        if file.filename == '':
            return jsonify({"success": False, "error": "No selected file"}), 400

        file_bytes = file.read()

        if file and allowed_file(file.filename):
            try:
                if file.filename.endswith('.pdf'):  
                    tenbrend_data = check_tenbrend(file.filename,fund_type)
                    reader = PdfReader(io.BytesIO(file_bytes))
                    text = ""
                    for page in reader.pages:
                        text += page.extract_text()

                    # Encode the PDF bytes to Base64
                    file_base64 = base64.b64encode(file_bytes).decode('utf-8')

                    return jsonify({
                        "success": True,
                        "original_text": extract_text_from_base64_pdf(file_bytes),  # file_bytesPDF text  , input = extract_text_from_base64_pdf(pdf_base64)
                        "pdf_bytes": file_base64,  # PDF Base64 
                        "file_name": file.filename,
                        "tenbrend_data":tenbrend_data,
                        # "fund_type": fund_type
                    })
                
                elif file.filename.endswith('.txt'):
                    text = file_bytes.decode('utf-8')  # UTF-8 

                    return jsonify({
                        "success": True,
                        "prompt_text": text
                    })

                elif file.filename.endswith(('.doc', '.docx')):
                    # Just Only DOCX format
                    # docx = Document(io.BytesIO(file_bytes))
                    # text = "\n".join([para.text for para in docx.paragraphs])

                    file_base64 = base64.b64encode(file_bytes).decode('utf-8')

                    return jsonify({
                        "success": True,
                        "original_text": "",
                        "docx_bytes": file_base64,
                        "file_name": file.filename
                    })

                elif regcheck.search(r'\.(xls|xlsx|XLSX|xlsm|xlm|xltx|xltm|xlsb)$',file.filename):
                    """
                    :param file_bytes: ‰∏ä‰º†ÁöÑbase64Êñá‰ª∂
                    :return: ‰øÆÊîπÂÆåÁöÑbase64 encoding
                    """
                    #--------------excel start------------------------------------------
                    # üîπ 1Ô∏è‚É£ corrected_map init
                    # corrected_map = fetch_and_convert_to_dict()
                    # all_text=[]

                    # # üîπ 2Ô∏è‚É£ ‰∏¥Êó∂‰øùÂ≠òÂÜÖÂ≠òÈáå in-memory zip)
                    # in_memory_zip = zipfile.ZipFile(io.BytesIO(file_bytes), 'r')

                    # # new ZIP ÁöÑ BytesIO
                    # output_buffer = io.BytesIO()
                    # new_zip = zipfile.ZipFile(output_buffer, 'w', zipfile.ZIP_DEFLATED, allowZip64=True)

                    # # üîπ 3Ô∏è‚É£ Âæ™ÁéØÊñá‰ª∂             
                    # for item in in_memory_zip.infolist():
                    #     file_data = in_memory_zip.read(item.filename)
                    #     # üîπ 4Ô∏è‚É£ ÊòØÂê¶drawingN.xml Ê£ÄÊü• (Â§ÑÁêÜÊñáÊú¨Ê°Ü)
                    #     if item.filename.startswith("xl/drawings/drawing") and item.filename.endswith(".xml"):
                    #         try:
                    #             tree = ET.fromstring(file_data)
                    #             ns = {'a': 'http://schemas.openxmlformats.org/drawingml/2006/main'}
                    #             # ÊâÄÊúâÁöÑ <a:t> 
                    #             text_elements = []
                    #             for t_element in tree.findall(".//a:t", ns):
                    #                 original_text = t_element.text
                    #                 if original_text:
                    #                     parent = t_element.getparent()
                    #                     if parent is not None:
                    #                         x = parent.attrib.get('x', 0)
                    #                         y = parent.attrib.get('y', 0)
                    #                         text_elements.append((float(y), float(x), original_text.strip()))
                    #             text_elements.sort(key=lambda item: (item[0], item[1]))
                    #             for _, _, text in text_elements:
                    #                 all_text.append(text)
                    #             file_data = ET.tostring(tree, encoding='utf-8', standalone=False)
                    #         except Exception as e:
                    #             print(f"Warning: Parsing {item.filename} failed - {e}")

                    #         try:
                    #             tree = ET.fromstring(file_data)
                    #             ns = {'ss': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'}

                    #             for row in tree.findall(".//ss:Row", ns):
                    #                 for cell in row.findall("ss:Cell", ns):
                    #                     value_element = cell.find("ss:Data", ns)
                    #                     if value_element is not None and value_element.text:
                    #                         all_text.append(value_element.text.strip())

                    #                     if cell.attrib.get('ss:MergeAcross') is not None:
                    #                         merged_value = value_element.text.strip() if value_element is not None else ""
                    #                         for _ in range(int(cell.attrib['ss:MergeAcross'])):
                    #                             all_text.append(merged_value)

                    #         except Exception as e:
                    #             print(f"Warning: Parsing {item.filename} failed - {e}")
                                
                    #     new_zip.writestr(item, file_data)

                    # # merge all text one string
                    # combined_text = ''.join(all_text)
                    
                    # # 612 debug
                    # # if file_type != "ÂèÇÁÖß„Éï„Ç°„Ç§„É´":
                    # #     result_map = gpt_correct_text(combined_text)
                    # #     corrected_map.update(result_map)  # Í≤∞Í≥º Îßµ Î≥ëÌï©
                    # # else:
                    # #     corrected_map = ""


                    # in_memory_zip.close()
                    # new_zip.close()

                    # output_buffer.seek(0)
                    #--------------excel end------------------------------------------
                    # excel_base64 = base64.b64encode(output_buffer.getvalue()).decode('utf-8')
                    excel_base64 = base64.b64encode(file_bytes).decode('utf-8')

                    return jsonify({
                        "success": True,
                        "original_text": "",# combined_text,
                        "excel_bytes": excel_base64,
                        "combined_text": "",# combined_text,
                        "file_name": file.filename
                    })

            except Exception as e:
                logging.error(f"Error processing file {file.filename}: {str(e)}")
                return jsonify({"success": False, "error": str(e)}), 500

    return jsonify({"success": False, "error": "Invalid file type"}), 400


# 5007 debug
def remove_correction_blocks(html_text):
    pattern = re.compile(
        r'<span[^>]*?>.*?<\/span>\s*\(<span>ÊèêÁ§∫:<s[^>]*?>.*?<\/s><\/span>\)',
        re.DOTALL
    )
    return pattern.sub('', html_text)

half_to_full_dict = {
    "ÔΩ¶": "„É≤", "ÔΩß": "„Ç°", "ÔΩ®": "„Ç£", "ÔΩ©": "„Ç•", "ÔΩ™": "„Çß", "ÔΩ´": "„Ç©",
    "ÔΩ¨": "„É£", "ÔΩ≠": "„É•", "ÔΩÆ": "„Éß", "ÔΩØ": "„ÉÉ", "ÔΩ∞": "„Éº",
    "ÔΩ±": "„Ç¢", "ÔΩ≤": "„Ç§", "ÔΩ≥": "„Ç¶", "ÔΩ¥": "„Ç®", "ÔΩµ": "„Ç™",
    "ÔΩ∂": "„Ç´", "ÔΩ∑": "„Ç≠", "ÔΩ∏": "„ÇØ", "ÔΩπ": "„Ç±", "ÔΩ∫": "„Ç≥",
    "ÔΩª": "„Çµ", "ÔΩº": "„Ç∑", "ÔΩΩ": "„Çπ", "ÔΩæ": "„Çª", "ÔΩø": "„ÇΩ",
    "ÔæÄ": "„Çø", "ÔæÅ": "„ÉÅ", "ÔæÇ": "„ÉÑ", "ÔæÉ": "„ÉÜ", "ÔæÑ": "„Éà",
    "ÔæÖ": "„Éä", "ÔæÜ": "„Éã", "Ôæá": "„Éå", "Ôæà": "„Éç", "Ôæâ": "„Éé",
    "Ôæä": "„Éè", "Ôæã": "„Éí", "Ôæå": "„Éï", "Ôæç": "„Éò", "Ôæé": "„Éõ",
    "Ôæè": "„Éû", "Ôæê": "„Éü", "Ôæë": "„É†", "Ôæí": "„É°", "Ôæì": "„É¢",
    "Ôæî": "„É§", "Ôæï": "„É¶", "Ôæñ": "„É®",
    "Ôæó": "„É©", "Ôæò": "„É™", "Ôæô": "„É´", "Ôæö": "„É¨", "Ôæõ": "„É≠",
    "Ôæú": "„ÉØ", "Ôæù": "„É≥",
    "%": "ÔºÖ", "@": "Ôº†"
}

full_to_half_dict = {
    'Ôºê': '0', 'Ôºë': '1', 'Ôºí': '2', 'Ôºì': '3', 'Ôºî': '4',
    'Ôºï': '5', 'Ôºñ': '6', 'Ôºó': '7', 'Ôºò': '8', 'Ôºô': '9',
    'Ôº°': 'A', 'Ôº¢': 'B', 'Ôº£': 'C', 'Ôº§': 'D', 'Ôº•': 'E',
    'Ôº¶': 'F', 'Ôºß': 'G', 'Ôº®': 'H', 'Ôº©': 'I', 'Ôº™': 'J',
    'Ôº´': 'K', 'Ôº¨': 'L', 'Ôº≠': 'M', 'ÔºÆ': 'N', 'ÔºØ': 'O',
    'Ôº∞': 'P', 'Ôº±': 'Q', 'Ôº≤': 'R', 'Ôº≥': 'S', 'Ôº¥': 'T',
    'Ôºµ': 'U', 'Ôº∂': 'V', 'Ôº∑': 'W', 'Ôº∏': 'X', 'Ôºπ': 'Y', 
    'Ôº∫': 'Z','Ôºã':'+','Ôºç':'-'
}

# ÂçäËßí‚Üí,-ÂÖ®Ëßí
def half_and_full_process(text, mapping):
    return ''.join(mapping.get(c, c) for c in text)

replace_rules = {
    # 'AAA': 'AAAÔºàÂÖ®Á±≥Ëá™ÂãïËªäÂçî‰ºöÔºâ', # 729 fix bug
    'ABS': 'ABSÔºàË≥áÁî£ÊãÖ‰øùË®ºÂà∏„ÄÅÂêÑÁ®ÆË≥áÁî£ÊãÖ‰øùË®ºÂà∏Ôºâ',
    'ADB': 'ADBÔºà„Ç¢„Ç∏„Ç¢ÈñãÁô∫ÈäÄË°åÔºâ',
    'ADR': 'ADRÔºàÁ±≥ÂõΩÈ†êË®óË®ºÂà∏Ôºâ',
    'AI': 'AIÔºà‰∫∫Â∑•Áü•ËÉΩÔºâ',
    'AIIB': 'AIIBÔºà„Ç¢„Ç∏„Ç¢„Ç§„É≥„Éï„É©ÊäïË≥áÈäÄË°åÔºâ',
    'APEC': 'APECÔºà„Ç¢„Ç∏„Ç¢Â§™Âπ≥Ê¥ãÁµåÊ∏àÂçîÂäõ‰ºöË≠∞Ôºâ',
    'API': 'APIÔºàÂÖ®Á±≥Áü≥Ê≤πÂçî‰ºöÔºâ',
    'BIS': 'BISÔºàÂõΩÈöõÊ±∫Ê∏àÈäÄË°åÔºâ',
    'BOE': 'BOEÔºàËã±‰∏≠Â§ÆÈäÄË°å„ÄÅ„Ç§„É≥„Ç∞„É©„É≥„ÉâÈäÄË°åÔºâ',
    'BRICSÔºà5„ÉµÂõΩÔºâ': 'BRICSÔºà„Éñ„É©„Ç∏„É´„ÄÅ„É≠„Ç∑„Ç¢„ÄÅ„Ç§„É≥„Éâ„ÄÅ‰∏≠ÂõΩ„ÄÅÂçó„Ç¢„Éï„É™„Ç´Ôºâ',
    'CDSÂ∏ÇÂ†¥': 'CDSÔºà„ÇØ„É¨„Ç∏„ÉÉ„Éà„Éª„Éá„Éï„Ç©„É´„Éà„Éª„Çπ„ÉØ„ÉÉ„ÉóÔºâÂ∏ÇÂ†¥',
    'CFROIC': 'CFROICÔºàÊäï‰∏ãË≥áÊú¨„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„ÉºÂà©ÁõäÁéáÔºâ',
    'Chat GPT': 'Chat GPTÔºàAI„Çí‰Ωø„Å£„ÅüÂØæË©±Âûã„Çµ„Éº„Éì„ÇπÔºâ',
    'CMBS': 'CMBSÔºàÂïÜÊ•≠Áî®‰∏çÂãïÁî£„É≠„Éº„É≥ÊãÖ‰øùË®ºÂà∏Ôºâ',
    'COP26': 'COP26ÔºàÂõΩÈÄ£Ê∞óÂÄôÂ§âÂãïÊû†ÁµÑ„ÅøÊù°Á¥ÑÁ¨¨26ÂõûÁ∑†Á¥ÑÂõΩ‰ºöË≠∞Ôºâ',
    'CPI': 'CPIÔºàÊ∂àË≤ªËÄÖÁâ©‰æ°ÊåáÊï∞Ôºâ',
    'CSR': 'CSRÔºà‰ºÅÊ•≠„ÅÆÁ§æ‰ºöÁöÑË≤¨‰ªªÔºâ',
    'DR': 'DRÔºàÈ†êË®óË®ºÊõ∏Ôºâ',
    'DRAM': 'DRAMÔºàÂçäÂ∞é‰ΩìÁ¥†Â≠ê„ÇíÂà©Áî®„Åó„ÅüË®òÊÜ∂Ë£ÖÁΩÆ„ÅÆ„Å≤„Å®„Å§Ôºâ',
    'DX': 'DXÔºà„Éá„Ç∏„Çø„É´„Éà„É©„É≥„Çπ„Éï„Ç©„Éº„É°„Éº„Ç∑„Éß„É≥Ôºâ',
    'EC': 'ECÔºàÈõªÂ≠êÂïÜÂèñÂºïÔºâ',
    'ECB': 'ECBÔºàÊ¨ßÂ∑û‰∏≠Â§ÆÈäÄË°åÔºâ',
    'EIA': 'EIAÔºàÁ±≥„Ç®„Éç„É´„ÇÆ„ÉºÁúÅ„Ç®„Éç„É´„ÇÆ„ÉºÊÉÖÂ†±Â±ÄÔºâ',
    'EMEA': 'EMEAÔºàÊ¨ßÂ∑û„Éª‰∏≠Êù±„Éª„Ç¢„Éï„É™„Ç´Ôºâ',
    'EPA': 'EPAÔºàÁ±≥Áí∞Â¢É‰øùË≠∑Â±ÄÔºâ',
    'EPS': 'EPSÔºà‰∏ÄÊ†™ÂΩì„Åü„ÇäÂà©ÁõäÔºâ',
    'ESM': 'ESMÔºàÊ¨ßÂ∑ûÂÆâÂÆö„É°„Ç´„Éã„Ç∫„É†Ôºâ',
    'ESG': 'ESGÔºàÁí∞Â¢É„ÉªÁ§æ‰ºö„Éª‰ºÅÊ•≠Áµ±Ê≤ªÔºâ',
    'EU': 'EUÔºàÊ¨ßÂ∑ûÈÄ£ÂêàÔºâ',
    'EV': 'EVÔºàÈõªÊ∞óËá™ÂãïËªäÔºâ',
    'EVA': 'EVAÔºàÁµåÊ∏àÁöÑ‰ªòÂä†‰æ°ÂÄ§Ôºâ',
    'FASB': 'FASBÔºàÁ±≥Ë≤°Âãô‰ºöË®àÂü∫Ê∫ñÂØ©Ë≠∞‰ºöÔºâ',
    'FDA': 'FDAÔºàÁ±≥ÂõΩÈ£üÂìÅÂåªËñ¨ÂìÅÂ±ÄÔºâ',
    'FF„É¨„Éº„ÉàÔºàÁ±≥ÂõΩ„ÅÆÂ†¥ÂêàÔºâ': 'ÊîøÁ≠ñÈáëÂà©ÔºàFF„É¨„Éº„ÉàÔºâ',
    'FOMC': 'FOMCÔºàÁ±≥ÈÄ£ÈÇ¶ÂÖ¨ÈñãÂ∏ÇÂ†¥ÂßîÂì°‰ºöÔºâ',
    'FRB': 'FRBÔºàÁ±≥ÈÄ£ÈÇ¶Ê∫ñÂÇôÂà∂Â∫¶ÁêÜ‰∫ã‰ºöÔºâ',
    'FTA': 'FTAÔºàËá™Áî±Ë≤øÊòìÂçîÂÆöÔºâ',
    'G7': 'G7Ôºà‰∏ªË¶Å7„ÉµÂõΩ‰ºöË≠∞Ôºâ',
    'G8': 'G8Ôºà‰∏ªË¶Å8„ÉµÂõΩÈ¶ñËÑ≥‰ºöË≠∞Ôºâ',
    'G20': 'G20Ôºà20„ÉµÂõΩ„ÉªÂú∞ÂüüÔºâË≤°ÂãôÁõ∏„Éª‰∏≠Â§ÆÈäÄË°åÁ∑èË£Å‰ºöË≠∞„ÄÅÈ¶ñËÑ≥‰ºöË≠∞',
    'GDP': 'GDPÔºàÂõΩÂÜÖÁ∑èÁîüÁî£Ôºâ',
    'GPIF': 'Âπ¥ÈáëÁ©çÁ´ãÈáëÁÆ°ÁêÜÈÅãÁî®Áã¨Á´ãË°åÊîøÊ≥ï‰∫∫ÔºàGPIFÔºâ',
    'GNP': 'GNPÔºàÂõΩÊ∞ëÁ∑èÁîüÁî£Ôºâ',
    'GST„ÄÄ‚Äª„Ç§„É≥„Éâ„ÅÆÂ†¥Âêà': 'GSTÔºàÁâ©ÂìÅ„Éª„Çµ„Éº„Éì„ÇπÁ®éÔºâ',
    'IEA': 'IEAÔºàÂõΩÈöõ„Ç®„Éç„É´„ÇÆ„ÉºÊ©üÈñ¢Ôºâ',
    'IMF': 'IMFÔºàÂõΩÈöõÈÄöË≤®Âü∫ÈáëÔºâ',
    'IoT': 'IoTÔºà„É¢„Éé„ÅÆ„Ç§„É≥„Çø„Éº„Éç„ÉÉ„ÉàÔºâ',
    'IPEF': 'IPEFÔºà„Ç§„É≥„ÉâÂ§™Âπ≥Ê¥ãÁµåÊ∏àÊû†ÁµÑ„ÅøÔºâ',
    'IPO': 'IPOÔºàÊñ∞Ë¶èÊ†™ÂºèÂÖ¨ÈñãÔºâ',
    'ISMÈùûË£ΩÈÄ†Ê•≠ÊôØÊ≥Å': 'ISMÈùûË£ΩÈÄ†Ê•≠ÊôØÊ≥ÅÊåáÊï∞',
    'IT': 'ITÔºàÊÉÖÂ†±ÊäÄË°ìÔºâ',
    'LBO': 'LBOÔºà„É¨„Éê„É¨„ÉÉ„Ç∏„Éâ„Éª„Éê„Ç§„Ç¢„Ç¶„ÉàÔºöÂØæË±°‰ºÅÊ•≠„ÅÆË≥áÁî£„ÇíÊãÖ‰øù„Å´Ë≥áÈáëË™øÈÅî„Åô„ÇãË≤∑ÂèéÔºâ',
    'LED': 'LEDÔºàÁô∫ÂÖâ„ÉÄ„Ç§„Ç™„Éº„ÉâÔºâ',
    'LME': 'LMEÔºà„É≠„É≥„Éâ„É≥ÈáëÂ±ûÂèñÂºïÊâÄÔºâ',
    'LNG': 'LNGÔºàÊ∂≤ÂåñÂ§©ÁÑ∂„Ç¨„ÇπÔºâ',
    'M&A': 'M&AÔºà‰ºÅÊ•≠„ÅÆÂêà‰Ωµ„ÉªË≤∑ÂèéÔºâ',
    'MAS': 'MASÔºà„Ç∑„É≥„Ç¨„Éù„Éº„É´ÈáëËûçÈÄöË≤®Â∫ÅÔºâ',
    'MBA': 'MBAÔºàÂÖ®Á±≥ÊäµÂΩìË≤∏‰ªòÈäÄË°åÂçî‰ºöÔºâ',
    'MBO': 'MBOÔºàÁµåÂñ∂Èô£„Å´„Çà„ÇãË≤∑ÂèéÔºâ',
    'MBS': 'MBSÔºà‰ΩèÂÆÖ„É≠„Éº„É≥ÊãÖ‰øùË®ºÂà∏Ôºâ',
    'NAFTA': 'NAFTAÔºàÂåóÁ±≥Ëá™Áî±Ë≤øÊòìÂçîÂÆöÔºâ',
    'NAHB': 'NAHBÔºàÂÖ®Á±≥‰ΩèÂÆÖÂª∫Ë®≠Ê•≠ËÄÖÂçî‰ºöÔºâ',
    'NAIC': 'NAICÔºàÂÖ®Á±≥‰øùÈô∫Áõ£Áù£ÂÆòÂçî‰ºöÔºâ',
    'NAR': 'NARÔºàÂÖ®Á±≥‰∏çÂãïÁî£Ê•≠ËÄÖÂçî‰ºöÔºâ',
    'NDF': 'NDFÔºàÁÇ∫ÊõøÂÖàÊ∏°ÂèñÂºï„ÅÆ„Å≤„Å®„Å§Ôºâ',
    'NISA': 'NISAÔºàÂ∞ëÈ°çÊäïË≥áÈùûË™≤Á®éÂà∂Â∫¶Ôºâ',
    'OECD': 'OECDÔºàÁµåÊ∏àÂçîÂäõÈñãÁô∫Ê©üÊßãÔºâ',
    'OEM': 'OEMÔºàÁõ∏ÊâãÂÖà„Éñ„É©„É≥„Éâ„Å´„Çà„ÇãÁîüÁî£Ôºâ',
    'OPEC': 'OPECÔºàÁü≥Ê≤πËº∏Âá∫ÂõΩÊ©üÊßãÔºâ',
    'OPEC„Éó„É©„Çπ': 'OPEC„Éó„É©„ÇπÔºàOPECÔºàÁü≥Ê≤πËº∏Âá∫ÂõΩÊ©üÊßãÔºâ„Å®ÈùûÂä†ÁõüÁî£Ê≤πÂõΩ„ÅßÊßãÊàê„Åô„ÇãOPEC„Éó„É©„ÇπÔºâ',
    'PBR': 'PBRÔºàÊ†™‰æ°Á¥îË≥áÁî£ÂÄçÁéáÔºâ',
    'PCE': 'PCEÔºàÂÄã‰∫∫Ê∂àË≤ªÊîØÂá∫Ôºâ',
    'PCFR': 'PCFRÔºàÊ†™‰æ°„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„ÉºÂÄçÁéáÔºâ',
    'PER': 'PERÔºàÊ†™‰æ°ÂèéÁõäÁéáÔºâ',
    'PMI': 'PMIÔºàË≥ºË≤∑ÊãÖÂΩìËÄÖÊôØÊ∞óÊåáÊï∞Ôºâ',
    'PPI': 'PPIÔºàÁîüÁî£ËÄÖÁâ©‰æ°ÊåáÊï∞Ôºâ',
    'QE': 'QEÔºàÈáèÁöÑÈáëËûçÁ∑©ÂíåÔºâ',
    'QT': 'QTÔºàÈáèÁöÑÂºï„ÅçÁ∑†„ÇÅÔºâ',
    'Quad': 'QuadÔºàÊó•Á±≥Ë±™Âç∞Êà¶Áï•ÂØæË©±Ôºâ',
    'RBA': 'RBAÔºàË±™Â∑ûÊ∫ñÂÇôÈäÄË°åÔºâ',
    'RCEP': 'RCEPÔºàÂú∞ÂüüÁöÑ„Å™ÂåÖÊã¨ÁöÑÁµåÊ∏àÈÄ£Êê∫ÂçîÂÆöÔºâ',
    'RBI': 'RBIÔºà„Ç§„É≥„ÉâÊ∫ñÂÇôÈäÄË°åÔºâ',
    'ROA': 'ROAÔºàÁ∑èË≥áÁî£Âà©ÁõäÁéáÔºâ',
    'ROE': 'ROEÔºàËá™Â∑±Ë≥áÊú¨Âà©ÁõäÁéáÔºâ',
    'S&L': 'S&LÔºàË≤ØËìÑË≤∏‰ªòÁµÑÂêàÔºâ',
    'SDGs': 'SDGsÔºàÊåÅÁ∂öÂèØËÉΩ„Å™ÈñãÁô∫ÁõÆÊ®ôÔºâ',
    'SEC': 'SECÔºàÁ±≥Ë®ºÂà∏ÂèñÂºïÂßîÂì°‰ºöÔºâ',
    'SQ': 'SQÔºàÁâπÂà•Ê∏ÖÁÆóÊåáÊï∞Ôºâ',
    'SRI': 'SRIÔºàÁ§æ‰ºöÁöÑË≤¨‰ªªÊäïË≥áÔºâ',
    'SUV': 'SUVÔºà„Çπ„Éù„Éº„ÉÑÁî®Â§öÁõÆÁöÑËªäÔºâ',
    'TALF': 'TALFÔºà„Çø„Éº„É†Áâ©Ë≥áÁî£ÊãÖ‰øùË®ºÂà∏Ë≤∏Âá∫Âà∂Â∫¶Ôºâ',
    'TOB': 'TOBÔºàÊ†™ÂºèÂÖ¨ÈñãË≤∑‰ªò„ÅëÔºâ',
    'TPP': 'TPPÔºàÁí∞Â§™Âπ≥Ê¥ãÁµåÊ∏àÈÄ£Êê∫ÂçîÂÆöÔºâ',
    'UAE': 'UAEÔºà„Ç¢„É©„ÉñÈ¶ñÈï∑ÂõΩÈÄ£ÈÇ¶Ôºâ',
    'UAW': 'UAWÔºàÂÖ®Á±≥Ëá™ÂãïËªäÂä¥ÂÉçÁµÑÂêàÔºâ',
    'USDA': 'USDAÔºàÁ±≥ÂõΩËæ≤ÂãôÁúÅÔºâ',
    'USMCA': 'USMCAÔºàÁ±≥ÂõΩ„Éª„É°„Ç≠„Ç∑„Ç≥„Éª„Ç´„Éä„ÉÄÂçîÂÆöÔºâ',
    'USTR': 'USTRÔºàÁ±≥ÈÄöÂïÜ‰ª£Ë°®ÈÉ®Ôºâ',
    'VAT': 'VATÔºà‰ªòÂä†‰æ°ÂÄ§Á®éÔºâ',
    'WTI': 'WTIÔºà„Ç¶„Ç®„Çπ„Éà„Éª„ÉÜ„Ç≠„Çµ„Çπ„Éª„Ç§„É≥„Çø„Éº„Éü„Éá„Ç£„Ç®„Éº„ÉàÔºâ',
    'WTO': 'WTOÔºà‰∏ñÁïåË≤øÊòìÊ©üÈñ¢Ôºâ',
    '„Ç¢„Çª„ÉÉ„Éà„Ç¢„É≠„Ç±„Éº„Ç∑„Éß„É≥': '„Ç¢„Çª„ÉÉ„Éà„Ç¢„É≠„Ç±„Éº„Ç∑„Éß„É≥ÔºàË≥áÁî£ÈÖçÂàÜÔºâ',
    '„Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„Éà': '„Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„ÉàÔºà„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Å´ÊØî„Åπ‰Ωé„ÇÅ„ÅÆÊäïË≥áÊØîÁéáÔºâ',
    '„Ç™„Éº„Éê„Éº„Ç¶„Ç®„Ç§„Éà': '„Ç™„Éº„Éê„Éº„Ç¶„Ç®„Ç§„ÉàÔºà„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Å´ÊØî„ÅπÈ´ò„ÇÅ„ÅÆÊäïË≥áÊØîÁéáÔºâ',
    'E-„Ç≥„Éû„Éº„Çπ': 'E„Ç≥„Éû„Éº„ÇπÔºàÈõªÂ≠êÂïÜÂèñÂºïÔºâ',
    'e-„Ç≥„Éû„Éº„Çπ': 'e„Ç≥„Éû„Éº„ÇπÔºàÈõªÂ≠êÂïÜÂèñÂºïÔºâ',
    '„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ': '„Ç§„Éº„É´„Éâ„Ç´„Éº„ÉñÔºàÂà©Âõû„ÇäÊõ≤Á∑öÔºâ',
    '„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ„Éª„Ç≥„É≥„Éà„É≠„Éº„É´': '„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ„Éª„Ç≥„É≥„Éà„É≠„Éº„É´ÔºàÈï∑Áü≠ÈáëÂà©Êìç‰ΩúÔºâ',
    '„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ„ÅÆ„Çπ„ÉÜ„Ç£„Éº„ÉóÂåñ': '„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ„ÅÆ„Çπ„ÉÜ„Ç£„Éº„ÉóÂåñÔºàÈï∑„ÉªÁü≠ÈáëÂà©Ê†ºÂ∑Æ„ÅÆÊã°Â§ßÔºâ',
    '„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ„ÅÆ„Éï„É©„ÉÉ„ÉàÂåñ': '„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ„ÅÆ„Éï„É©„ÉÉ„ÉàÂåñÔºàÈï∑„ÉªÁü≠ÈáëÂà©Ê†ºÂ∑Æ„ÅÆÁ∏ÆÂ∞èÔºâ',
    '„Ç§„É≥„Ç´„É†„Ç≤„Ç§„É≥': '„Ç§„É≥„Ç´„É†„Ç≤„Ç§„É≥ÔºàÂà©Â≠êÂèéÂÖ•Ôºâ',
    '„Ç§„É≥„Çø„É©„ÇØ„ÉÜ„Ç£„Éñ': '„Ç§„É≥„Çø„É©„ÇØ„ÉÜ„Ç£„ÉñÔºàÂèåÊñπÂêëÊÄßÔºâ',
    '„Ç®„ÇØ„Ç§„ÉÜ„Ç£„Éª„Éï„Ç°„Ç§„Éä„É≥„Çπ': '„Ç®„ÇØ„Ç§„ÉÜ„Ç£„Éª„Éï„Ç°„Ç§„Éä„É≥„ÇπÔºàÊñ∞Ê†™Áô∫Ë°åÁ≠â„Å´„Çà„ÇãË≥áÈáëË™øÈÅîÔºâ',
    '„Ç™„Éê„Éû„Ç±„Ç¢': '„Ç™„Éê„Éû„Ç±„Ç¢ÔºàÂåªÁôÇ‰øùÈô∫Âà∂Â∫¶ÊîπÈù©Ê≥ïÔºâ',
    '„Ç™„É≥„Éá„Éû„É≥„Éâ': '„Ç™„É≥„Éá„Éû„É≥„ÉâÔºàÊ≥®ÊñáÁîüÁî£Ôºâ',
    '„Ç´„É≥„Éà„É™„ÉºÔΩ•„Ç¢„É≠„Ç±„Éº„Ç∑„Éß„É≥': '„Ç´„É≥„Éà„É™„ÉºÔΩ•„Ç¢„É≠„Ç±„Éº„Ç∑„Éß„É≥ÔºàÂõΩÂà•Ë≥áÁî£ÈÖçÂàÜÔºâ',
    'ÈÄÜ„Ç§„Éº„É´„Éâ': 'ÈÄÜ„Ç§„Éº„É´„ÉâÔºàÁü≠ÊúüÂÇµÂà∏„ÅÆÂà©Âõû„Çä„ÅåÈï∑ÊúüÂÇµÂà∏„ÅÆÂà©Âõû„Çä„Çí‰∏äÂõû„Å£„Å¶„ÅÑ„ÇãÁä∂ÊÖãÔºâ',
    # '„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº': '„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„ÉºÔºàÁèæÈáëÂèéÊîØÔºâ',
    '„Ç≠„É£„Éî„Çø„É´„Ç≤„Ç§„É≥': '„Ç≠„É£„Éî„Çø„É´„Ç≤„Ç§„É≥ÔºàÂÄ§‰∏ä„Åå„ÇäÁõäÔºâ',
    '„Ç≠„É£„É™„Éº„Éà„É¨„Éº„Éâ': '„Ç≠„É£„É™„Éº„Éà„É¨„Éº„ÉâÔºà‰ΩéÈáëÂà©„ÅÆË≥áÈáë„ÇíË™øÈÅî„Åó„Å¶„ÄÅÈ´òÈáëÂà©„ÅÆË≥áÁî£„ÅßÈÅãÁî®„Åô„ÇãÂèñÂºïÔºâ',
    '„ÇØ„É¨„Ç∏„ÉÉ„Éà„Çπ„Éó„É¨„ÉÉ„Éâ': '„ÇØ„É¨„Ç∏„ÉÉ„Éà„Çπ„Éó„É¨„ÉÉ„ÉâÔºà‰ºÅÊ•≠„ÅÆ‰ø°Áî®Âäõ„ÅÆÂ∑Æ„Å´„Çà„ÇãÂà©Âõû„Çä„ÅÆÂ∑ÆÔºâ',
    '„Ç∞„É≠„Éº„Éê„É™„Çº„Éº„Ç∑„Éß„É≥': '„Ç∞„É≠„Éº„Éê„É™„Çº„Éº„Ç∑„Éß„É≥ÔºàÂú∞ÁêÉË¶èÊ®°ÂåñÔºâ',
    '„Ç≥„Ç∏„Çß„Éç„É¨„Éº„Ç∑„Éß„É≥': '„Ç≥„Ç∏„Çß„Éç„É¨„Éº„Ç∑„Éß„É≥ÔºàÁÜ±Èõª‰æõÁµ¶„Ç∑„Çπ„ÉÜ„É†Ôºâ',
    '„Ç≥„Éº„Éù„É¨„Éº„Éà„Éª„Ç¨„Éê„Éä„É≥„Çπ': '„Ç≥„Éº„Éù„É¨„Éº„Éà„Éª„Ç¨„Éê„Éä„É≥„ÇπÔºà‰ºÅÊ•≠Áµ±Ê≤ªÔºâ',
    '„Ç≥„É≥„Ç∞„É≠„Éû„É™„ÉÉ„Éà': '„Ç≥„É≥„Ç∞„É≠„Éû„É™„ÉÉ„ÉàÔºàË§áÂêà‰ºÅÊ•≠Ôºâ',
    '„Ç≥„É≥„ÇΩ„Éº„Ç∑„Ç¢„É†': '„Ç≥„É≥„ÇΩ„Éº„Ç∑„Ç¢„É†ÔºàÂÖ±Âêå‰∫ãÊ•≠Ôºâ',
    '„Çµ„Éº„Éô„Ç§„É©„É≥„Çπ': '„Çµ„Éº„Éô„Ç§„É©„É≥„ÇπÔºàË™øÊüªÁõ£Ë¶ñÔºâ',
    '„Çµ„Çπ„ÉÜ„Éä„Éì„É™„ÉÜ„Ç£': '„Çµ„Çπ„ÉÜ„Éä„Éì„É™„ÉÜ„Ç£ÔºàÊåÅÁ∂öÂèØËÉΩÊÄßÔºâ',
    '„Çµ„Éñ„Éó„É©„Ç§„É†„É≠„Éº„É≥': '„Çµ„Éñ„Éó„É©„Ç§„É†„É≠„Éº„É≥Ôºà‰ø°Áî®Â∫¶„ÅÆ‰Ωé„ÅÑÂÄã‰∫∫Âêë„Åë‰ΩèÂÆÖËûçË≥áÔºâ',
    '„Çµ„Éó„É©„Ç§„ÉÅ„Çß„Éº„É≥': '„Çµ„Éó„É©„Ç§„ÉÅ„Çß„Éº„É≥Ôºà‰æõÁµ¶Á∂≤Ôºâ',
    '„Ç∏„Çß„Éç„É™„ÉÉ„ÇØÂåªËñ¨ÂìÅ': '„Ç∏„Çß„Éç„É™„ÉÉ„ÇØÂåªËñ¨ÂìÅÔºàÂæåÁô∫Ëñ¨Ôºâ',
    '„Ç∑„ÇØ„É™„Ç´„É´': '„Ç∑„ÇØ„É™„Ç´„É´ÔºàÊôØÊ∞óÊïèÊÑüÔºâ',
    '„Ç∑„É£„Éâ„Éº„Éê„É≥„Ç≠„É≥„Ç∞': '„Ç∑„É£„Éâ„Éº„Éê„É≥„Ç≠„É≥„Ç∞ÔºàÂΩ±„ÅÆÈäÄË°åÔºâ',
    '„Ç∑„Éß„Éº„Éà„Éù„Ç∏„Ç∑„Éß„É≥': '„Ç∑„Éß„Éº„Éà„Éù„Ç∏„Ç∑„Éß„É≥ÔºàÂ£≤„ÇäÊåÅ„Å°Ôºâ',
    '‰ø°Áî®Â∏ÇÂ†¥': '‰ºÅÊ•≠„ÅÆ‰ø°Áî®„É™„Çπ„ÇØ„ÇíÂèñÂºï„Åô„ÇãÂ∏ÇÂ†¥',
    '„Çπ„ÉÜ„Ç£„Éº„ÉóÂåñ': '„Çπ„ÉÜ„Ç£„Éº„ÉóÂåñÔºàÈï∑Áü≠ÈáëÂà©Ê†ºÂ∑Æ„ÅÆÊã°Â§ßÔºâ',
    '„Çπ„Éà„É¨„Çπ„ÉÜ„Çπ„Éà': '„Çπ„Éà„É¨„Çπ„ÉÜ„Çπ„ÉàÔºàÂÅ•ÂÖ®ÊÄßÂØ©ÊüªÔºâ',
    '„Çπ„Éó„É¨„ÉÉ„Éâ': '„Çπ„Éó„É¨„ÉÉ„ÉâÔºàÂà©Âõû„ÇäÊ†ºÂ∑ÆÔºâ',
    '„Çπ„Éû„Éº„Éà„Ç∑„ÉÜ„Ç£„Éº': '„Çπ„Éû„Éº„Éà„Ç∑„ÉÜ„Ç£„ÉºÔºàIT„ÇíÊ¥ªÁî®„Åó„ÅüÊ¨°‰∏ñ‰ª£Âûã„ÅÆÈÉΩÂ∏ÇÔºâ',
    '„Çπ„Éû„Éº„Éà„É¢„Éì„É™„ÉÜ„Ç£': '„Çπ„Éû„Éº„Éà„É¢„Éì„É™„ÉÜ„Ç£ÔºàÂæìÊù•„ÅÆ‰∫§ÈÄö„ÉªÁßªÂãï„ÇíÂ§â„Åà„ÇãÊñ∞„Åü„Å™„ÉÜ„ÇØ„Éé„É≠„Ç∏„ÉºÔºâ',
    '„Çª„Éº„Éï„ÉÜ„Ç£„Éç„ÉÉ„Éà': '„Çª„Éº„Éï„ÉÜ„Ç£„Éç„ÉÉ„ÉàÔºàÂÆâÂÖ®Á∂≤Ôºâ',
    'ÂÖ®‰∫∫‰ª£': 'ÂÖ®‰∫∫‰ª£ÔºàÂÖ®ÂõΩ‰∫∫Ê∞ë‰ª£Ë°®Â§ß‰ºöÔºâ',
    '„ÇΩ„Éï„Éà„É©„É≥„Éá„Ç£„É≥„Ç∞': '„ÇΩ„Éï„Éà„É©„É≥„Éá„Ç£„É≥„Ç∞ÔºàËªüÁùÄÈô∏Ôºâ',
    '„ÉÄ„Ç§„Éê„Éº„Ç∑„ÉÜ„Ç£': '„ÉÄ„Ç§„Éê„Éº„Ç∑„ÉÜ„Ç£ÔºàÂ§öÊßòÊÄßÔºâ',
    '„Çø„Éº„Éü„Éä„É´„É¨„Éº„Éà': '„Çø„Éº„Éü„Éä„É´„É¨„Éº„ÉàÔºàÊîøÁ≠ñÈáëÂà©„ÅÆÊúÄÁµÇÂà∞ÈÅîÊ∞¥Ê∫ñÔºâ',
    '„ÉÜ„Éº„Éë„É™„É≥„Ç∞': '„ÉÜ„Éº„Éë„É™„É≥„Ç∞ÔºàÈáèÁöÑÈáëËûçÁ∑©Âíå„ÅÆÁ∏ÆÂ∞èÔºâ',
    '„Éá„Ç£„Éï„Çß„É≥„Ç∑„Éñ': '„Éá„Ç£„Éï„Çß„É≥„Ç∑„ÉñÔºàÊôØÊ∞ó„Å´Â∑¶Âè≥„Åï„Çå„Å´„Åè„ÅÑÔºâ',
    '„Éá„Éï„Ç©„É´„Éà': '„Éá„Éï„Ç©„É´„ÉàÔºàÂÇµÂãô‰∏çÂ±•Ë°åÔºâ',
    '„Éá„É•„É¨„Éº„Ç∑„Éß„É≥': '„Éá„É•„É¨„Éº„Ç∑„Éß„É≥ÔºàÈáëÂà©ÊÑüÂøúÂ∫¶Ôºâ', 
    '„Éá„É™„Éê„ÉÜ„Ç£„Éñ': '„Éá„É™„Éê„ÉÜ„Ç£„ÉñÔºàÈáëËûçÊ¥æÁîüÂïÜÂìÅÔºâ',
    '„Éâ„É´„Éö„ÉÉ„Ç∞Âà∂': '„Éâ„É´„Éö„ÉÉ„Ç∞ÔºàÈÄ£ÂãïÔºâÂà∂',
    '„Éê„Ç§„Ç™„Ç∑„Éü„É©„Éº': '„Éê„Ç§„Ç™„Ç∑„Éü„É©„ÉºÔºàÂæåÁ∂öËñ¨Ôºâ',
    '„Éê„Ç§„Ç™„Éû„Çπ': '„Éê„Ç§„Ç™„Éû„ÇπÔºàÁîüÁâ©„ÇíÂà©Áî®„Åó„Å¶Áâ©Ë≥™„ÇÑ„Ç®„Éç„É´„ÇÆ„Éº„ÇíÂæó„Çã„Åì„Å®Ôºâ',
    '„Éê„Éº„ÉÅ„É£„É´': '„Éê„Éº„ÉÅ„É£„É´Ôºà‰ªÆÊÉ≥Ôºâ',
    '„Éê„É™„É•„Ç®„Éº„Ç∑„Éß„É≥': '„Éê„É™„É•„Ç®„Éº„Ç∑„Éß„É≥ÔºàÊäïË≥á‰æ°ÂÄ§Ë©ï‰æ°Ôºâ',
    '„Éê„É™„É•„Éº': '„Éê„É™„É•„ÉºÔºàÂâ≤ÂÆâÔºâ',
    '5G': '5GÔºàÁ¨¨5‰∏ñ‰ª£ÁßªÂãïÈÄö‰ø°„Ç∑„Çπ„ÉÜ„É†Ôºâ',
    '„Éï„Ç£„É≥„ÉÜ„ÉÉ„ÇØ': '„Éï„Ç£„É≥„ÉÜ„ÉÉ„ÇØÔºàÈáëËûç„Å®ÊäÄË°ì„ÅÆËûçÂêàÔºâ', 
    '„Éï„Çß„Ç¢„Éê„É™„É•„Éº': '„Éï„Çß„Ç¢„Éê„É™„É•„ÉºÔºàÈÅ©Ê≠£‰æ°Ê†ºÔºâ',
    '„Éï„Çß„Éº„Ç∫2': '„Éï„Çß„Éº„Ç∫2ÔºàËá®Â∫äË©¶È®ì„ÅÆ‰∏≠ÈñìÊÆµÈöéÔºâ',
    '„Éï„Çß„Éº„Ç∫3': '„Éï„Çß„Éº„Ç∫3ÔºàËá®Â∫äË©¶È®ì„ÅÆÊúÄÁµÇÊÆµÈöéÔºâ',
    '„Éï„Éº„Éâ„Éá„É™„Éê„É™„Éº': '„Éï„Éº„Éâ„Éá„É™„Éê„É™„ÉºÔºàÊñôÁêÜÁ≠â„ÅÆÂÆÖÈÖç„Çµ„Éº„Éì„ÇπÔºâ',
    '„Éï„É´„Ç§„É≥„Éô„Çπ„Éà„É°„É≥„Éà': '„Éï„É´„Ç§„É≥„Éô„Çπ„Éà„É°„É≥„ÉàÔºàÈ´ò‰ΩçÁµÑÂÖ•Ôºâ',
    '„Éñ„É≠„Éº„Éâ„Éê„É≥„Éâ': '„Éñ„É≠„Éº„Éâ„Éê„É≥„ÉâÔºàÂ§ßÂÆπÈáèÔΩ•È´òÈÄüÈÄö‰ø°Ôºâ',
    '„Éù„ÉÜ„É≥„Ç∑„É£„É´': '„Éù„ÉÜ„É≥„Ç∑„É£„É´ÔºàÊΩúÂú®ÂäõÔºâ',
    '„Éù„Éî„É•„É™„Ç∫„É†': '„Éù„Éî„É•„É™„Ç∫„É†ÔºàÂ§ßË°ÜËøéÂêà‰∏ªÁæ©Ôºâ',
    '„É¢„Éº„Ç≤„Éº„Ç∏': '„É¢„Éº„Ç≤„Éº„Ç∏Ôºà‰∏çÂãïÁî£ÊãÖ‰øù„É≠„Éº„É≥Ôºâ',
    '„É¢„Éº„Ç≤„Éº„Ç∏ÂÇµ': '„É¢„Éº„Ç≤„Éº„Ç∏ÂÇµÔºà‰∏çÂãïÁî£„É≠„Éº„É≥ÊãÖ‰øùÂÇµÂà∏Ôºâ',
    '„É¢„É©„É´„Éè„Ç∂„Éº„Éâ': '„É¢„É©„É´„Éè„Ç∂„Éº„ÉâÔºàÂÄ´ÁêÜÂ¥©Â£äÔºâ',
    '„É™„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞': '„É™„Ç®„É≥„Ç∏„Éã„Ç¢„É™„É≥„Ç∞ÔºàÊ•≠Âãô„ÅÆÊäúÊú¨ÁöÑÈù©Êñ∞Ôºâ',
    '„É™„Ç™„Éº„Éó„É≥': '„É™„Ç™„Éº„Éó„Éã„É≥„Ç∞ÔºàÁµåÊ∏àÊ¥ªÂãïÂÜçÈñãÔºâ',
    '„É™„Çª„ÉÉ„Ç∑„Éß„É≥': '„É™„Çª„ÉÉ„Ç∑„Éß„É≥ÔºàÊôØÊ∞óÂæåÈÄÄÔºâ',
    '„É™„Çø„Éº„É≥„É™„Éê„Éº„Çµ„É´': '„É™„Çø„Éº„É≥„É™„Éê„Éº„Çµ„É´ÔºàÈÅéÂâ∞ÂèçÂøúÂäπÊûúÔºâ',
    '„É™„Éê„Ç¶„É≥„Éâ': '„É™„Éê„Ç¶„É≥„ÉâÔºàÂèçÁô∫Ôºâ',
    '„É™„Éê„É©„É≥„Çπ': '„É™„Éê„É©„É≥„ÇπÔºàÊäïË≥áÊØîÁéá„ÅÆÂÜçË™øÊï¥Ôºâ',
    '„É¨„Éë„Éà„É™Ê∏õÁ®é': '„É¨„Éë„Éà„É™ÔºàÊµ∑Â§ñÂèéÁõä„ÅÆÊú¨ÂõΩÈÇÑÊµÅÔºâÊ∏õÁ®é',
    '„É¨„Éê„É¨„ÉÉ„Ç∏„Éâ„É≠„Éº„É≥': '‰ΩéÊ†º‰ªò„ÅëÁ≠â„ÅÆÂÄü„ÇäÊâãÂêë„ÅëËûçË≥á',
    '„É¨„É©„ÉÜ„Ç£„Éñ„Éª„Éê„É™„É•„Éº': '„É¨„É©„ÉÜ„Ç£„Éñ„Éª„Éê„É™„É•„ÉºÔºàÁõ∏ÂØæ‰æ°ÂÄ§Ôºâ',
    '„É≠„ÉÉ„ÇØ„ÉÄ„Ç¶„É≥': '„É≠„ÉÉ„ÇØ„ÉÄ„Ç¶„É≥ÔºàÈÉΩÂ∏ÇÂ∞ÅÈéñÔºâ',
    '„É≠„É≥„Ç∞„Éù„Ç∏„Ç∑„Éß„É≥': '„É≠„É≥„Ç∞„Éù„Ç∏„Ç∑„Éß„É≥ÔºàË≤∑„ÅÑÊåÅ„Å°Ôºâ',
    'EDA„ÉÑ„Éº„É´': 'EDA„ÉÑ„Éº„É´ÔºàÈõªÂ≠êË®≠Ë®àËá™ÂãïÂåñ„ÉÑ„Éº„É´Ôºâ', #623
    'TOPIX':'TOPIXÔºàÊù±Ë®ºÊ†™‰æ°ÊåáÊï∞Ôºâ', #63207
    'Âà©Âõû„Çä„ÅØ‰∏äÊòá': 'Âà©Âõû„Çä„ÅØ‰∏äÊòáÔºà‰æ°Ê†º„ÅØ‰∏ãËêΩÔºâ', #730
    'Âà©Âõû„Çä„ÅØ‰Ωé‰∏ã': 'Âà©Âõû„Çä„ÅØ‰Ωé‰∏ãÔºà‰æ°Ê†º„ÅØ‰∏äÊòáÔºâ', #730

    'Âà©Âõû„Çä„ÅÆ‰∏äÊòá': 'Âà©Âõû„Çä„ÅÆ‰∏äÊòá(‰æ°Ê†º„ÅØ‰Ωé‰∏ã)', #730
    'Âà©Âõû„Çä„ÅÆ‰Ωé‰∏ã': 'Âà©Âõû„Çä„ÅÆ‰Ωé‰∏ã(‰æ°Ê†º„ÅØ‰∏äÊòá)', #730

    'ÂõΩÂÇµÂà©Âõû„Çä„ÅØ„ÄÅÊúàÈñì„Åß‰∏äÊòá': 'ÂõΩÂÇµÂà©Âõû„Çä„ÅØ„ÄÅÊúàÈñì„Åß‰∏äÊòá(‰æ°Ê†º„ÅØ‰Ωé‰∏ã)', #730
    'ÂõΩÂÇµÂà©Âõû„Çä„ÅØ„ÄÅÊúàÈñì„Åß‰∏ãËêΩ': 'ÂõΩÂÇµÂà©Âõû„Çä„ÅØ„ÄÅÊúàÈñì„Åß‰∏ãËêΩ(‰æ°Ê†º„ÅØ‰∏äÊòá)', #730

    'ÂÇµÂà∏Âà©Âõû„Çä„ÅØ‰∏äÊòá': 'ÂÇµÂà∏Âà©Âõû„Çä„ÅØ‰∏äÊòá(‰æ°Ê†º„ÅØ‰Ωé‰∏ã)', #730
    'ÂÇµÂà∏Âà©Âõû„Çä„ÅØ‰∏ãËêΩ': 'ÂÇµÂà∏Âà©Âõû„Çä„ÅØ‰∏äÊòá(‰æ°Ê†º„ÅØ‰∏äÊòá)', #730
    
    '„Ç∑„É£„É™„Ç¢': '„Ç∑„É£„É™„Éº„Ç¢',
    'TTM': '‰ª≤ÂÄ§',
    'ÊÆÜ„Å©': '„Åª„Å®„Çì„Å©',
    'Áúü‰ºº': '„Åæ„Å≠',
    '‰∫ò„Çã': '„Çè„Åü„Çã',
    '‰ΩÜ„Åó': '„Åü„Å†„Åó',
    'ÁâΩÂà∂': '„Åë„ÇìÂà∂',
    'ÁâΩÂºï': '„Åë„ÇìÂºï',
    'ÁµÇÁÑâ': 'ÁµÇ„Åà„Çì',
    'ÂèéÊñÇ': 'Âèé„Çå„Çì',
    'ÈÄºËø´': '„Å≤„Å£Ëø´',
    '„É∂Êúà': '„ÉµÊúà',
    'ÂÖ•Êõø„Åà': 'ÂÖ•„ÇåÊõø„Åà',
    'ÂÖ•Êõø': 'ÂÖ•„ÇåÊõø',
    'Â£≤‰ªò':'Â£≤„Çä‰ªò„Åë',
    'Â£≤‰ªò„Åë':'Â£≤„Çä‰ªò„Åë',
    'Ê†º‰ªò': 'Ê†º‰ªò„Åë', 
    'Ë≤∑Âª∫„Å¶': 'Ë≤∑„ÅÑÂª∫„Å¶',
    'Â£≤Âª∫„Å¶': 'Â£≤„ÇäÂª∫„Å¶',
    'Âàá‰∏ä„Åí': 'Âàá„Çä‰∏ä„Åí',
    'ÂàáÊç®„Å¶': 'Âàá„ÇäÊç®„Å¶',
    'ÁµÑÂÖ•„Çå': 'ÁµÑ„ÅøÂÖ•„Çå', 
    'Áπ∞‰∏ä„ÅíÂÑüÈÇÑ': 'Áπ∞‰∏äÂÑüÈÇÑ',
    'ÂÖà„ÅçË°å„Åç': 'ÂÖàË°å„Åç',
    '‰∏ãÊîØ„Åà„Çã': '‰∏ãÊîØ„Åà„Åô„Çã',
    'Âèñ„ÇäÂºï„Åç': 'ÂèñÂºï',
    'Âºï‰∏ä„Åí': 'Âºï„Åç‰∏ä„Åí',
    'Âºï‰∏ã„Åí': 'Âºï„Åç‰∏ã„Åí',
    'ÂºïÁ∂ö„Åç': 'Âºï„ÅçÁ∂ö„Åç',
    'ÂºïÁ∑†„ÇÅ': 'Âºï„ÅçÁ∑†„ÇÅ',
    'ËñÑÂïÜ„ÅÑ': 'ÂèñÂºïÈáè„ÅåÂ∞ë„Å™„Åè',
    '„Ç≥„Ç¢ÈäòÊüÑ': '‰∏≠Ê†∏ÈäòÊüÑ„ÄÅ„Ç≥„Ç¢Ôºà‰∏≠Ê†∏ÔºâÈäòÊüÑ',
    '„Éà„É™„Ç¨„Éº': '„Åç„Å£„Åã„Åë',
    '„Éñ„É´„Éº„ÉÅ„ÉÉ„Éó‰ºÅÊ•≠': 'ÂÑ™ËâØ‰ºÅÊ•≠',
    '„Éè„ÉàÊ¥æ': 'ÈáëËûçÁ∑©Âíå„Å´ÂâçÂêë„Åç',
    '„Çø„Ç´Ê¥æ': 'ÈáëËûçÂºï„ÅçÁ∑†„ÇÅÈáçË¶ñ',
    'Áõ∏Â†¥': 'Â∏ÇÂ†¥',
    'ÈÄ£„ÇåÈ´ò': 'ÂΩ±Èüø„ÇíÂèó„Åë„Å¶‰∏äÊòá',
    '‰ºùÊí≠': 'Â∫É„Åå„Çã',
    '„Åß„Çì„Å±': 'Â∫É„Åå„Çã',
    '„É¨„É≥„Ç∏': 'ÁØÑÂõ≤',
    'ÂõûÈáë': 'ÂÜÜËª¢',
    '„É≠„Éº„É≥': 'Ë≤∏„Åó‰ªò„Åë',
    'ÊâÄË¨Ç': '„ÅÑ„Çè„ÇÜ„Çã',
    'Êö´„Åè': '„Åó„Å∞„Çâ„Åè',
    'Áïô„Åæ„Çã': '„Å®„Å©„Åæ„Çã',
    'Ê≠¢„Åæ„Çã': '„Å®„Å©„Åæ„Çã',
    'Â∞ö': '„Å™„Åä',
    'Á≠à': '„ÅØ„Åö',
    'ËìãÁÑ∂ÊÄß': 'ÂèØËÉΩÊÄß',
    'ÂïÜ„ÅÑ': 'Âá∫Êù•È´ò',
    'ÂæåÂÄí„Åó': 'Âª∂Êúü',
    'ÁµåÊ∏àÊ≠£Â∏∏Âåñ': 'ÁµåÊ∏àÊ¥ªÂãïÊ≠£Â∏∏Âåñ',
    'ÈáëËûçÊ≠£Â∏∏Âåñ': 'ÈáëËûçÊîøÁ≠ñÊ≠£Â∏∏Âåñ',
    'Êó•Êú¨ÈäÄË°å': 'Êó•ÈäÄ',
    'ÊîøÊ≤ªÁöÑ„É™„Çπ„ÇØ': 'ÊîøÊ≤ª„É™„Çπ„ÇØ',
    'Âú∞ÊîøÂ≠¶„É™„Çπ„ÇØ': 'Âú∞ÊîøÂ≠¶ÁöÑ„É™„Çπ„ÇØ',
    '„Å∏„ÅÆÁµÑ„ÅøÂÖ•„Çå': '„ÅÆÁµÑ„ÅøÂÖ•„Çå',
    '„Éû„Ç§„Éä„Çπ„Å´ÂØÑ‰∏é': '„Éû„Ç§„Éä„Çπ„Å´ÂΩ±Èüø',
    '„Éû„Ç§„Éä„ÇπÂØÑ‰∏é': '„Éû„Ç§„Éä„ÇπÂΩ±Èüø', #829
    'Á±≥ÂõΩÂõΩÂÇµ': 'Á±≥ÂõΩÂÇµ',
    'Êñ∞Âûã„Ç≥„É≠„Éä': 'Êñ∞Âûã„Ç≥„É≠„Éä„Ç¶„Ç§„É´„Çπ',
    '„Ç≥„É≠„Éä„Ç¶„Ç§„É´„Çπ': 'Êñ∞Âûã„Ç≥„É≠„Éä„Ç¶„Ç§„É´„Çπ',
    'Á´ã„Å°Âæå„Çå': 'Á´ã„Å°ÈÅÖ„Çå',
    '‰º∏Âºµ': '‰º∏Èï∑',
    '„ÉÄ„Ç¶Âπ≥Âùá': '„ÉÄ„Ç¶Âπ≥ÂùáÊ†™‰æ°',
    'NY„ÉÄ„Ç¶': '„ÉÄ„Ç¶Âπ≥ÂùáÊ†™‰æ°',
    '‰∏≠ÈäÄ': '‰∏≠Â§ÆÈäÄË°å', #623
    'Ë°å„Çè„Çå': 'Ë°å„Å™„Çè„Çå', #623
    'Ë°å„ÅÑ': 'Ë°å„Å™„ÅÑ', #623
    'Ë°å„Çè„Å™„ÅÑ': 'Ë°å„Å™„Çè„Å™„ÅÑ', #821
    'Ë°å„Å£„Åü':'Ë°å„Å™„Å£„Åü',
    'Ë°å„ÅÜ': 'Ë°å„Å™„ÅÜ', #623
    'Ë°å„Å£„Å¶': 'Ë°å„Å™„Å£„Å¶', #623
    'Ë°å„Çè„Çå„Çã': 'Ë°å„Å™„Çè„Çå„Çã',
    '„Å™„Çä„Åó„Åæ„Åó„Åü': '„Å™„Åó„Åæ„Åó„Åü', #180015,628
    'Ë≤∑„ÅÑ‰ªò„Åë„Åæ„Åó„Åü': 'Ë≤∑„ÅÑ‰ªò„Åë„Åó„Åæ„Åó„Åü',
    # 'Ë≤∑„ÅÑ‰ªò„Åë': 'Ë≤∑„ÅÑ‰ªò„Åë„Åó', #64977 , 829 fix
    'Ë≤∑‰ªò':'Ë≤∑„ÅÑ‰ªò„Åë',
    'Ë≤∑‰ªò„Åë':'Ë≤∑„ÅÑ‰ªò„Åë„Åó', #63207
    'Â£≤„Çä‰ªò„Åë„Åæ„Åó„Åü':'Â£≤„Çä‰ªò„Åë„Åó„Åæ„Åó„Åü', #628
    'Â£≤„ÇäÁ´ã„Å¶„Åæ„Åó„Åü':'Â£≤„ÇäÁ´ã„Å¶„Åó„Åæ„Åó„Åü', #628
    'Ââ≤ÂÆâ„Å´': 'Ââ≤ÂÆâÊÑü„ÅÆ„ÅÇ„Çã',
    'MSCI„Ç§„É≥„ÉâÊåáÊï∞': 'MSCI„Ç§„É≥„Éâ„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ',
    '„Çµ„Çπ„ÉÜ„Éä„Éñ„É´': '„Çµ„Çπ„ÉÜ„Ç£„Éä„Éñ„É´',
    '„Ç®„É≥„Çø„Éº„ÉÜ„Ç§„É°„É≥„Éà': '„Ç®„É≥„Çø„Éº„ÉÜ„Ç§„É≥„É°„É≥„Éà',
    '‰∫ò': '„Çè„Åü',
    'REIT': '„É™„Éº„Éà', #629
    'Ááª': '„Åè„Åô„Å∂',
    '„Éà„É©„É≥„ÉóÊîøÊ®©': '„Éà„É©„É≥„ÉóÁ±≥ÊîøÊ®©',
    '„Éà„É©„É≥„ÉóÂ§ßÁµ±È†ò': '„Éà„É©„É≥„ÉóÁ±≥Â§ßÁµ±È†ò',
    'Á±≥„Éà„É©„É≥„ÉóÂ§ßÁµ±È†ò': 'Á±≥„Éà„É©„É≥„ÉóÂ§ßÁµ±È†ò',
    'Â•ΩÊÑü„Åï„Çå„ÄÅ‰∏ãËêΩ„Åó': 'Â•ΩÊÑü„Åï„Çå‰∏ãËêΩ„Åó',
    'Â´åÊ∞ó„Åï„Çå„ÄÅ‰∏ãËêΩ„Åó': 'Â´åÊ∞ó„Åï„Çå‰∏ãËêΩ„Åó',
    'Â•ΩÊÑü„Åï„Çå„ÄÅ‰∏äÊòá„Åó': 'Â•ΩÊÑü„Åï„Çå‰∏äÊòá„Åó',
    'Â´åÊ∞ó„Åï„Çå„ÄÅ‰∏äÊòá„Åó': 'Â´åÊ∞ó„Åï„Çå‰∏äÊòá„Åó',
    'Áïô': '„Å®„Å©', #629
    'ÂΩìÁ§æ': 'ÂêåÁ§æ', #629
    'ÁâΩ': '„Åë„Çì', #629
    '„Åì„Å®ÁõÆÊåá„Åó„Å¶„ÅÑ„Çã': '„Åì„Å®„ÇíÁõÆÊåá„Åó„Å¶„ÅÑ„Çã', #630
    '„Ç∞„É≠„Éº„Éê„É´„Åß‰∫ãÊ•≠': '„Ç∞„É≠„Éº„Éê„É´„Å´‰∫ãÊ•≠', #630
    'Á©ç„ÅøÂ¢ó„Åô': 'Á©ç„ÅøÂ¢ó„Åó„Åô„Çã', #630 
    'ÂèñÁµÑ„Åø': 'Âèñ„ÇäÁµÑ„Åø',
    'È≠ÖÂäõÂ∫¶': '<sup>‚Äª</sup>È≠ÖÂäõÂ∫¶',
    '„Éï„É™„Éº„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº': '„Éï„É™„Éº„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº(Á®éÂºïÂæåÂñ∂Ê•≠Âà©Áõä„Å´Ê∏õ‰æ°ÂÑüÂç¥Ë≤ª„ÇíÂä†„Åà„ÄÅË®≠ÂÇôÊäïË≥áÈ°ç„Å®ÈÅãËª¢Ë≥áÊú¨„ÅÆÂ¢óÂä†„ÇíÂ∑Æ„ÅóÂºï„ÅÑ„Åü„ÇÇ„ÅÆ )', #726
    '„Éï„É™„Éº„Éª„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº': '„Éï„É™„Éº„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº(Á®éÂºïÂæåÂñ∂Ê•≠Âà©Áõä„Å´Ê∏õ‰æ°ÂÑüÂç¥Ë≤ª„ÇíÂä†„Åà„ÄÅË®≠ÂÇôÊäïË≥áÈ°ç„Å®ÈÅãËª¢Ë≥áÊú¨„ÅÆÂ¢óÂä†„ÇíÂ∑Æ„ÅóÂºï„ÅÑ„Åü„ÇÇ„ÅÆ )', #726

    '„Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£': '„Éú„É©„ÉÜ„Ç£„É™„ÉÜ„Ç£Ôºà‰æ°Ê†ºÂ§âÂãïÊÄßÔºâ', #829
    '„Éï„Ç°„É≥„ÉÄ„É°„É≥„Çø„É´„Ç∫': '„Éï„Ç°„É≥„ÉÄ„É°„É≥„Çø„É´„Ç∫ÔºàÁµåÊ∏à„ÅÆÂü∫Á§éÁöÑÊù°‰ª∂Ôºâ', #829

    
}

replace_rules1 ={
    '„Ç∑„É£„É™„Ç¢': '„Ç∑„É£„É™„Éº„Ç¢',
    'TTM': '‰ª≤ÂÄ§',
    'ÊÆÜ„Å©': '„Åª„Å®„Çì„Å©',
    'Áúü‰ºº': '„Åæ„Å≠',
    '‰∫ò„Çã': '„Çè„Åü„Çã',
    '‰ΩÜ„Åó': '„Åü„Å†„Åó',
    'ÁâΩÂà∂': '„Åë„ÇìÂà∂',
    'ÁâΩÂºï': '„Åë„ÇìÂºï',
    'ÁµÇÁÑâ': 'ÁµÇ„Åà„Çì',
    'ÂèéÊñÇ': 'Âèé„Çå„Çì',
    'ÈÄºËø´': '„Å≤„Å£Ëø´',
    '„É∂Êúà': '„ÉµÊúà',
    'ÂÖ•Êõø„Åà': 'ÂÖ•„ÇåÊõø„Åà',
    'ÂÖ•Êõø': 'ÂÖ•„ÇåÊõø',
    'Â£≤‰ªò':'Â£≤„Çä‰ªò„Åë',
    'Â£≤‰ªò„Åë':'Â£≤„Çä‰ªò„Åë',
    'Ê†º‰ªò': 'Ê†º‰ªò„Åë', 
    'Ë≤∑Âª∫„Å¶': 'Ë≤∑„ÅÑÂª∫„Å¶',
    'Â£≤Âª∫„Å¶': 'Â£≤„ÇäÂª∫„Å¶',
    'Âàá‰∏ä„Åí': 'Âàá„Çä‰∏ä„Åí',
    'ÂàáÊç®„Å¶': 'Âàá„ÇäÊç®„Å¶',
    'ÁµÑÂÖ•„Çå': 'ÁµÑ„ÅøÂÖ•„Çå', 
    'Áπ∞‰∏ä„ÅíÂÑüÈÇÑ': 'Áπ∞‰∏äÂÑüÈÇÑ',
    'ÂÖà„ÅçË°å„Åç': 'ÂÖàË°å„Åç',
    '‰∏ãÊîØ„Åà„Çã': '‰∏ãÊîØ„Åà„Åô„Çã',
    'Âèñ„ÇäÂºï„Åç': 'ÂèñÂºï',
    'Âºï‰∏ä„Åí': 'Âºï„Åç‰∏ä„Åí',
    'Âºï‰∏ã„Åí': 'Âºï„Åç‰∏ã„Åí',
    'ÂºïÁ∂ö„Åç': 'Âºï„ÅçÁ∂ö„Åç',
    'ÂºïÁ∑†„ÇÅ': 'Âºï„ÅçÁ∑†„ÇÅ',
    'ËñÑÂïÜ„ÅÑ': 'ÂèñÂºïÈáè„ÅåÂ∞ë„Å™„Åè',
    '„Ç≥„Ç¢ÈäòÊüÑ': '‰∏≠Ê†∏ÈäòÊüÑ„ÄÅ„Ç≥„Ç¢Ôºà‰∏≠Ê†∏ÔºâÈäòÊüÑ',
    '„Éà„É™„Ç¨„Éº': '„Åç„Å£„Åã„Åë',
    '„Éñ„É´„Éº„ÉÅ„ÉÉ„Éó‰ºÅÊ•≠': 'ÂÑ™ËâØ‰ºÅÊ•≠',
    '„Éè„ÉàÊ¥æ': 'ÈáëËûçÁ∑©Âíå„Å´ÂâçÂêë„Åç',
    '„Çø„Ç´Ê¥æ': 'ÈáëËûçÂºï„ÅçÁ∑†„ÇÅÈáçË¶ñ',
    'Áõ∏Â†¥': 'Â∏ÇÂ†¥',
    'ÈÄ£„ÇåÈ´ò': 'ÂΩ±Èüø„ÇíÂèó„Åë„Å¶‰∏äÊòá',
    '‰ºùÊí≠': 'Â∫É„Åå„Çã',
    '„Åß„Çì„Å±': 'Â∫É„Åå„Çã',
    '„É¨„É≥„Ç∏': 'ÁØÑÂõ≤',
    'ÂõûÈáë': 'ÂÜÜËª¢',
    '„É≠„Éº„É≥': 'Ë≤∏„Åó‰ªò„Åë',
    'ÊâÄË¨Ç': '„ÅÑ„Çè„ÇÜ„Çã',
    'Êö´„Åè': '„Åó„Å∞„Çâ„Åè',
    'Áïô„Åæ„Çã': '„Å®„Å©„Åæ„Çã',
    'Ê≠¢„Åæ„Çã': '„Å®„Å©„Åæ„Çã',
    'Â∞ö': '„Å™„Åä',
    'Á≠à': '„ÅØ„Åö',
    'ËìãÁÑ∂ÊÄß': 'ÂèØËÉΩÊÄß',
    'ÂïÜ„ÅÑ': 'Âá∫Êù•È´ò',
    'ÂæåÂÄí„Åó': 'Âª∂Êúü',
    'ÁµåÊ∏àÊ≠£Â∏∏Âåñ': 'ÁµåÊ∏àÊ¥ªÂãïÊ≠£Â∏∏Âåñ',
    'ÈáëËûçÊ≠£Â∏∏Âåñ': 'ÈáëËûçÊîøÁ≠ñÊ≠£Â∏∏Âåñ',
    'Êó•Êú¨ÈäÄË°å': 'Êó•ÈäÄ',
    'ÊîøÊ≤ªÁöÑ„É™„Çπ„ÇØ': 'ÊîøÊ≤ª„É™„Çπ„ÇØ',
    'Âú∞ÊîøÂ≠¶„É™„Çπ„ÇØ': 'Âú∞ÊîøÂ≠¶ÁöÑ„É™„Çπ„ÇØ',
    '„Å∏„ÅÆÁµÑ„ÅøÂÖ•„Çå': '„ÅÆÁµÑ„ÅøÂÖ•„Çå',
    '„Éû„Ç§„Éä„Çπ„Å´ÂØÑ‰∏é': '„Éû„Ç§„Éä„Çπ„Å´ÂΩ±Èüø',
    '„Éû„Ç§„Éä„ÇπÂØÑ‰∏é': '„Éû„Ç§„Éä„ÇπÂΩ±Èüø', #829
    'Á±≥ÂõΩÂõΩÂÇµ': 'Á±≥ÂõΩÂÇµ',
    'Êñ∞Âûã„Ç≥„É≠„Éä': 'Êñ∞Âûã„Ç≥„É≠„Éä„Ç¶„Ç§„É´„Çπ',
    '„Ç≥„É≠„Éä„Ç¶„Ç§„É´„Çπ': 'Êñ∞Âûã„Ç≥„É≠„Éä„Ç¶„Ç§„É´„Çπ',
    'Á´ã„Å°Âæå„Çå': 'Á´ã„Å°ÈÅÖ„Çå',
    '‰º∏Âºµ': '‰º∏Èï∑',
    '„ÉÄ„Ç¶Âπ≥Âùá': '„ÉÄ„Ç¶Âπ≥ÂùáÊ†™‰æ°',
    'NY„ÉÄ„Ç¶': '„ÉÄ„Ç¶Âπ≥ÂùáÊ†™‰æ°',
    '‰∏≠ÈäÄ': '‰∏≠Â§ÆÈäÄË°å', #623
    'Ë°å„Çè„Çå': 'Ë°å„Å™„Çè„Çå', #623
    'Ë°å„ÅÑ': 'Ë°å„Å™„ÅÑ', #623
    'Ë°å„Çè„Å™„ÅÑ': 'Ë°å„Å™„Çè„Å™„ÅÑ', #821
    'Ë°å„Å£„Åü':'Ë°å„Å™„Å£„Åü',
    'Ë°å„ÅÜ': 'Ë°å„Å™„ÅÜ', #623
    'Ë°å„Å£„Å¶': 'Ë°å„Å™„Å£„Å¶', #623
    'Ë°å„Çè„Çå„Çã': 'Ë°å„Å™„Çè„Çå„Çã',
    '„Å™„Çä„Åó„Åæ„Åó„Åü': '„Å™„Åó„Åæ„Åó„Åü', #180015,628
    'Ë≤∑„ÅÑ‰ªò„Åë„Åæ„Åó„Åü': 'Ë≤∑„ÅÑ‰ªò„Åë„Åó„Åæ„Åó„Åü',
    # 'Ë≤∑„ÅÑ‰ªò„Åë': 'Ë≤∑„ÅÑ‰ªò„Åë„Åó', #64977 , 824fix
    'Ë≤∑‰ªò':'Ë≤∑„ÅÑ‰ªò„Åë',
    'Ë≤∑‰ªò„Åë':'Ë≤∑„ÅÑ‰ªò„Åë„Åó', #63207
    'Â£≤„Çä‰ªò„Åë„Åæ„Åó„Åü':'Â£≤„Çä‰ªò„Åë„Åó„Åæ„Åó„Åü', #628
    'Â£≤„ÇäÁ´ã„Å¶„Åæ„Åó„Åü':'Â£≤„ÇäÁ´ã„Å¶„Åó„Åæ„Åó„Åü', #628
    'Ââ≤ÂÆâ„Å´': 'Ââ≤ÂÆâÊÑü„ÅÆ„ÅÇ„Çã',
    'MSCI„Ç§„É≥„ÉâÊåáÊï∞': 'MSCI„Ç§„É≥„Éâ„Éª„Ç§„É≥„Éá„ÉÉ„ÇØ„Çπ',
    '„Çµ„Çπ„ÉÜ„Éä„Éñ„É´': '„Çµ„Çπ„ÉÜ„Ç£„Éä„Éñ„É´',
    '„Ç®„É≥„Çø„Éº„ÉÜ„Ç§„É°„É≥„Éà': '„Ç®„É≥„Çø„Éº„ÉÜ„Ç§„É≥„É°„É≥„Éà',
    '‰∫ò': '„Çè„Åü',
    'REIT': '„É™„Éº„Éà', #629
    'Ááª': '„Åè„Åô„Å∂',
    '„Éà„É©„É≥„ÉóÊîøÊ®©': '„Éà„É©„É≥„ÉóÁ±≥ÊîøÊ®©',
    '„Éà„É©„É≥„ÉóÂ§ßÁµ±È†ò': '„Éà„É©„É≥„ÉóÁ±≥Â§ßÁµ±È†ò',
    'Á±≥„Éà„É©„É≥„ÉóÂ§ßÁµ±È†ò': 'Á±≥„Éà„É©„É≥„ÉóÂ§ßÁµ±È†ò',
    'Â•ΩÊÑü„Åï„Çå„ÄÅ‰∏ãËêΩ„Åó': 'Â•ΩÊÑü„Åï„Çå‰∏ãËêΩ„Åó',
    'Â´åÊ∞ó„Åï„Çå„ÄÅ‰∏ãËêΩ„Åó': 'Â´åÊ∞ó„Åï„Çå‰∏ãËêΩ„Åó',
    'Â•ΩÊÑü„Åï„Çå„ÄÅ‰∏äÊòá„Åó': 'Â•ΩÊÑü„Åï„Çå‰∏äÊòá„Åó',
    'Â´åÊ∞ó„Åï„Çå„ÄÅ‰∏äÊòá„Åó': 'Â´åÊ∞ó„Åï„Çå‰∏äÊòá„Åó',
    'Áïô': '„Å®„Å©', #629
    'ÂΩìÁ§æ': 'ÂêåÁ§æ', #629
    'ÁâΩ': '„Åë„Çì', #629
    '„Åì„Å®ÁõÆÊåá„Åó„Å¶„ÅÑ„Çã': '„Åì„Å®„ÇíÁõÆÊåá„Åó„Å¶„ÅÑ„Çã', #630
    '„Ç∞„É≠„Éº„Éê„É´„Åß‰∫ãÊ•≠': '„Ç∞„É≠„Éº„Éê„É´„Å´‰∫ãÊ•≠', #630
    'Á©ç„ÅøÂ¢ó„Åô': 'Á©ç„ÅøÂ¢ó„Åó„Åô„Çã', #630 
    'ÂèñÁµÑ„Åø': 'Âèñ„ÇäÁµÑ„Åø',
    'È≠ÖÂäõÂ∫¶': '<sup>‚Äª</sup>È≠ÖÂäõÂ∫¶'
}


replace_rules2 ={
    'ÊîøÊ≤ªÁöÑ„É™„Çπ„ÇØ': 'ÊîøÊ≤ª„É™„Çπ„ÇØ',
    'Âú∞ÊîøÂ≠¶„É™„Çπ„ÇØ': 'Âú∞ÊîøÂ≠¶ÁöÑ„É™„Çπ„ÇØ',
}

def merge_brackets(content: str) -> str:
    """
    Êã¨Âè∑ÂÜÖÊç¢Ë°åÁ¨¶: 'CPIÔºàÊ∂àË≤ªËÄÖÁâ©\n‰æ°ÊåáÊï∞Ôºâ' -> 'CPIÔºàÊ∂àË≤ªËÄÖÁâ©‰æ°ÊåáÊï∞Ôºâ'
    """
    # return regcheck.sub(r'Ôºà[^Ôºâ\n\r]*[\n\r]+[^Ôºâ]*Ôºâ', lambda m: m.group(0).replace("\n", "").replace("\r", ""), content)
    content = regcheck.sub(r'([^\s\n\r])[\s\n\r]+Ôºà', r'\1Ôºà', content)

    def replacer(match):
        inside = match.group(1)
        cleaned = regcheck.sub(r'[\s\u3000]+', '', inside)
        return f'Ôºà{cleaned}Ôºâ'

    return regcheck.sub(r'Ôºà(.*?)Ôºâ', replacer, content, flags=regcheck.DOTALL)


# (4Êúà30Êó• ‚Üí 2025Âπ¥4Êúà30Êó•)
def insert_year_by_regex(date_str: str, full_text: str, date_pos: int) -> str:
    year_matches = list(regcheck.finditer(r'(\d{4})Âπ¥', full_text[:date_pos]))
    if year_matches:
        last_year = year_matches[-1].group(1)
        return f'{last_year}Âπ¥{date_str}'
    return date_str

# (4Êúà30Êó• ‚Üí 2025Âπ¥4Êúà30Êó•)
def year_half_dict(text: str) -> str:
    full_half = {
        'Ôºê': '0', 'Ôºë': '1', 'Ôºí': '2', 'Ôºì': '3', 'Ôºî': '4',
        'Ôºï': '5', 'Ôºñ': '6', 'Ôºó': '7', 'Ôºò': '8', 'Ôºô': '9'
    }
    return ''.join(full_half.get(c, c) for c in text)


def opt_check_eng(content, rules):
    if not isinstance(rules, dict):
        raise TypeError(f"`rules` must be a dict, got {type(rules)}")
    
    content = merge_brackets(content)
    content = content.replace("(", "Ôºà").replace(")", "Ôºâ")
    lines = content.strip().splitlines()

    seen_raw = set()
    seen_full = set()
    results = []

    for line in lines:
        result = []
        normalized_line = line.replace("\n", "").replace(" ", "")

        for k, v in rules.items():
            raw_key = k.replace("(", "Ôºà").replace(")", "Ôºâ")
            full_key = v.replace("(", "Ôºà").replace(")", "Ôºâ")

            if '(' not in full_key and 'Ôºà' not in full_key:
                continue
            
            escaped_k = regcheck.escape(raw_key)
            escaped_v = regcheck.escape(full_key)

            # ------------------------------
            # keyword Ê≤°ÊúâÂØπÂ∫îÁöÑpattern
            # ------------------------------
            new_k = escaped_k
            paren_pattern = f"{escaped_k}Ôºà[^Ôºâ]+Ôºâ"

            if raw_key.isalpha() or raw_key in ["S&L", "M&A"]:
                if raw_key == "OPEC":
                    new_k = f"(?<![a-zA-Z]){escaped_k}(?!„Éó„É©„Çπ|[a-zA-Z])"
                elif raw_key == "„Çπ„ÉÜ„Ç£„Éº„ÉóÂåñ":
                    new_k = f"(?<!„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ„ÅÆ){escaped_k}"
                elif raw_key == "„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ":
                    new_k = f"{escaped_k}(?!„Éª„Ç≥„É≥„Éà„É≠„Éº„É´|„ÅÆ„Çπ„ÉÜ„Ç£„Éº„ÉóÂåñ|„ÅÆ„Éï„É©„ÉÉ„ÉàÂåñ)"
                elif raw_key == "„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº":
                    new_k = f"(?<!„Éï„É™„Éº){escaped_k}"
                elif raw_key == "„Ç≠„É£„É™„Éº„Éà„É¨„Éº„Éâ":
                    new_k = f"(?<!ÂÜÜ){escaped_k}"
                elif raw_key == "„Çπ„Éó„É¨„ÉÉ„Éâ":
                    new_k = f"(?<!„ÇØ„É¨„Ç∏„ÉÉ„Éà){escaped_k}"
                elif raw_key == "„Éê„É™„É•„Éº":
                    new_k = f"(?<!„É¨„É©„ÉÜ„Ç£„Éñ„Éª|„Éï„Çß„Ç¢){escaped_k}"
                elif raw_key == "„É¢„Éº„Ç≤„Éº„Ç∏":
                    new_k = f"{escaped_k}(?!ÂÇµ)"
                elif raw_key == "ÂïÜ„ÅÑ":
                    new_k = f"(?<!ËñÑ){escaped_k}"
                else:
                    new_k = f"(?<![a-zA-Z]){escaped_k}(?![a-zA-Z])"

            matched_full = regcheck.search(escaped_v, normalized_line)
            matched_raw_with_paren = regcheck.search(paren_pattern, normalized_line)
            matched_raw = regcheck.search(new_k, normalized_line)

            # ‚úÖ Ê†°È™åfull_key,Á¨¨‰∏ÄÊ¨°Âá∫Áé∞
            if matched_full and full_key not in seen_full:
                seen_raw.add(raw_key)
                seen_full.add(full_key)
                continue

            # ‚úÖ full_key ,Á¨¨‰∫åÊ¨°Âá∫Áé∞
            elif matched_full and full_key in seen_full:
                result.append({full_key: "Âà†Èô§"})
            
            elif matched_raw_with_paren:
                result.append({matched_raw_with_paren.group(): full_key})
                seen_raw.add(raw_key)
                seen_full.add(full_key)

            elif matched_raw and raw_key not in seen_raw:
                result.append({raw_key: full_key})
                seen_raw.add(raw_key)
                seen_full.add(full_key)

        results.append(result)

    return results

def opt_check_ruru1(content, rules):
    content = merge_brackets(content)

    result = []
    for k, v in rules.items():
        raw_key = k.replace("(", "Ôºà").replace(")", "Ôºâ")
        full_key = v.replace("(", "Ôºà").replace(")", "Ôºâ")

        escaped_k = regcheck.escape(raw_key)
        escaped_v = regcheck.escape(full_key)

        new_k = escaped_k
        if raw_key.isalpha() or raw_key in ["S&L", "M&A"]:
            if raw_key == "OPEC":
                new_k = f"(?<![a-zA-Z]){escaped_k}(?!„Éó„É©„Çπ|[a-zA-Z])"
            elif raw_key == "„Çπ„ÉÜ„Ç£„Éº„ÉóÂåñ":
                new_k = f"(?<!„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ„ÅÆ){escaped_k}"
            elif raw_key == "„Ç§„Éº„É´„Éâ„Ç´„Éº„Éñ":
                new_k = f"{escaped_k}(?!„Éª„Ç≥„É≥„Éà„É≠„Éº„É´|„ÅÆ„Çπ„ÉÜ„Ç£„Éº„ÉóÂåñ|„ÅÆ„Éï„É©„ÉÉ„ÉàÂåñ)"
            elif raw_key == "„Ç≠„É£„ÉÉ„Ç∑„É•„Éï„É≠„Éº":
                new_k = f"(?<!„Éï„É™„Éº){escaped_k}"
            elif raw_key == "„Ç≠„É£„É™„Éº„Éà„É¨„Éº„Éâ":
                new_k = f"(?<!ÂÜÜ){escaped_k}"
            elif raw_key == "„Çπ„Éó„É¨„ÉÉ„Éâ":
                new_k = f"(?<!„ÇØ„É¨„Ç∏„ÉÉ„Éà){escaped_k}"
            elif raw_key == "„Éê„É™„É•„Éº":
                new_k = f"(?<!„É¨„É©„ÉÜ„Ç£„Éñ„Éª|„Éï„Çß„Ç¢){escaped_k}"
            elif raw_key == "„É¢„Éº„Ç≤„Éº„Ç∏":
                new_k = f"{escaped_k}(?!ÂÇµ)"
            elif raw_key == "ÂïÜ„ÅÑ":
                new_k = f"(?<!ËñÑ){escaped_k}"
            else:
                new_k = f"(?<![a-zA-Z]){escaped_k}(?![a-zA-Z])"
        #  ‰∏≠ÈäÄ
        elif raw_key == "‰∏≠ÈäÄ":
            matches = regcheck.finditer(escaped_v, content)
            exclude = False
            for m in matches:
                prefix = content[max(0, m.start() - 2): m.start()]
                if prefix and not regcheck.match(r"[ \t\n\r]", prefix):
                    exclude = True
                    break
            if exclude:
                new_k = escaped_k
                full_match = None
            else:
                full_match = regcheck.search(escaped_v, content)

            
        raw_match = regcheck.search(new_k, content)
        full_match = regcheck.search(escaped_v, content)

        if raw_key != "‰∏≠ÈäÄ":
            if full_match and raw_match:
                if full_match.start() <= raw_match.start():
                    continue
            elif full_match and not raw_match:
                continue

        if raw_match:
            result.append({raw_key: full_key})
        
    return result

def keyword_pair_exists(content, keyword_a, keyword_b):
    return keyword_a in content and keyword_b in content

# Âú∞ÊîøÂ≠¶„É™„Çπ„ÇØ/ÊîøÊ≤ªÁöÑ„É™„Çπ„ÇØ
def opt_check_ruru2(content, replace_rules2):
    content = merge_brackets(content)

    result = []

    keyword_pairs = [
        ("Âú∞ÊîøÂ≠¶„É™„Çπ„ÇØ", "Âú∞ÊîøÂ≠¶ÁöÑ„É™„Çπ„ÇØ"),
        ("ÊîøÊ≤ªÁöÑ„É™„Çπ„ÇØ", "ÊîøÊ≤ª„É™„Çπ„ÇØ")
    ]

    for a, b in keyword_pairs:
        if keyword_pair_exists(content, a, b):
            result.append({a: b})

    return result

# 0501 debug
def find_corrections(corrected_text,input_text,pageNumber):
    corrections = []
    pattern = r'<span\s+style="color:red;">([\s\S]*?)<\/span>\s*\(<span>\s*‰øÆÊ≠£ÁêÜÁî±[::]\s*([\s\S]*?)\s*<s[^>]*>([\s\S]*?)<\/s>\s*‚Üí\s*([\s\S]*?)<\/span>\)'
    matches = re.findall(pattern, corrected_text)

    print("Matches found:", matches)
    # <span style="color:red;">‰∏äÂçà12Êó∂00ÂàÜ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ‰∏çË¶Å„Å™‰∏≠ÂõΩË™ûË°®Ë®ò <s style="background:yellow;color:red">‰∏äÂçà12Êó∂00ÂàÜ</s> ‚Üí ÔºàÂâäÈô§Ôºâ</span>)

    for match in matches:
        if len(match) == 4:
            corrected_text_re = match[0]  #610 debug
            reason_type = match[1].strip()
            original_text = match[2].strip()
            target_text = match[3].strip()

            comment = f"{reason_type} {original_text} ‚Üí {target_text}"

            corrections.append({
                "page": pageNumber,
                "original_text": corrected_text_re,
                "comment": comment,
                "reason_type":reason_type,
                "check_point": input_text.strip(),
                "locations": [],
                "intgr": False, 
            })
    
    return corrections

# 814 ,add dotfind Âè•Ë™≠ÁÇπ
#------------------------------------------------------------
def check_fullwidth_period(sentence):
    return sentence.endswith("„ÄÇ")

#---------------------------------------------------------------------------

# 0623 debug
def find_corrections_wording(input_text,pageNumber,tenbrend,fund_type,input_list):
    corrections = []

#-------------------
    #Â∏∏Áî®Â§ñÊ±âÂ≠ó
    common_par = r"((Âïû|Ëõô|È¥â|ÂüÉ|Êå®|Êõñ|ÈùÑ|Ëªã|Êñ°|Êåâ|Â∫µ|Èûç|Èóá|Â∑≤|Â§∑|Áïè|Èüã|Â∏∑|Ëêé|Ê§Ö|Ëë¶|ÂΩô|È£¥|Ë¨Ç|Èñæ|Ê∫¢|È∞Ø|Â∞π|ÂíΩ|ÊÆ∑|Ê∑´|Èöï|Ëî≠|‰∫é|ËøÇ|ÁõÇ|ÁÉè|È¨±|‰∫ë|Êöà|Á©¢|Êõ≥|Ê¥©|Ë£î|Á©é|Â¨∞|Áø≥|ËÖã|Êõ∞|Â•Ñ|ÂÆõ|ÊÄ®|‰ø∫|ÂÜ§|Ë¢Å|Â©â|ÁÑâ|Â†∞|Ê∑µ|ÁÑ∞|Á≠µ|Âé≠|È≥∂|Ááï|Èñª|Âö•|Âóö|Âá∞|Âòî|È¥®|Áîï|Ë•ñ|Ë¨≥|È∂Ø|È∑ó|È∏ö|ËáÜ|‰ø§|Áìú|Âëµ|Ëãõ|ÁèÇ|Ëø¶|Ë®õ|Ë®∂|Ë∑è|Âò©|Áëï|Ê¶é|Á™©|Ëù¶|Ëù∏|Èçã|È°Ü|Áâô|Áì¶|Ëá•|‰øÑ|Â≥®|Ë®ù|Ëõæ|Ë°ô|Èßï|Ëä•|‰πñ|Âªª|Âæä|ÊÅ¢|Êô¶|Â†∫|ÊΩ∞|Èûã|Ë´ß|Ê™ú|Ëüπ|Âí≥|Â¥ñ|Ëìã|Êºë|È™∏|Èéß|ÂñÄ|Âªì|Êëë|Êî™|ÊÑï|Ëêº|Ë´§|È°é|È∞ê|Ê®´|Áµ£|Á≠à|Ëëõ|Èóä|È∞π|Ëê±|Â•∏|‰∏≤|Êó±|ÂáΩ|Âí∏|Âß¶|ÂÆ¶|Êüë|Á´ø|ÊÇç|Ê°ì|Ê∂µ|ËèÖ|Âµå|Èâó|Êæó|Áø∞|Ë´´|Áû∞|Ê™ª|ÁÅå|Áé©|ÈõÅ|Áø´|È†∑|Áôå|Ë¥ã|Âá†|Âçâ|ÂÖ∂|Á•Å|ËÄÜ|Âüº|ÊÇ∏|ÊèÜ|ÊØÄ|ÁÆï|Áïø|Á™∫|Ë´±|ÂæΩ|Ê´É|Â¶ì|Á•á|È≠è|Ëüª|Êé¨|È∫¥|ÂêÉ|Â±π|ÊãÆ|Ë¨î|‰ªá|Ëáº|Ê±≤|ÁÅ∏|Âíé|ÈÇ±|Êü©|Á¨à|Ë∫¨|Âé©|ÂóÖ|ËàÖ|ÁÇ¨|Ê∏†|Ë£æ|Âôì|Â¢ü|Èã∏|ÈÅΩ|Ê¨Ö|Âåà|ÊÄØ|‰ø†|ËÑá|Ëé¢|Á´ü|Âçø|ÂÉë|Â¨å|Ëïé|Èãè|È†∞|Ê©ø|ÁñÜ|È•ó|Ê£ò|È´∑|Â∑æ|ÂÉÖ|Á¶Ω|È•â|Áãó|ÊÉß|ËªÄ|Êáº|‰ø±|Âñ∞|ÂØì|Á™ü|Á≤Ç|ÂÅà|Ëçä|Áè™|Áï¶|ËÑõ|È†É|Áóô|Ë©£|Á¶ä|Èñ®|Á®Ω|È†∏|È´ª|Ëπä|ÈÆ≠|Áπ´|Áù®|Êàü|Èöô|Êäâ|È†Å|Ë®£|Ëï®|Âß∏|ÂÄ¶|Ëôî|Êç≤|ÁâΩ|Âñß|Á°Ø|ËÖ±|Èçµ|Áûº|Èπº|Âëü|Áú©|Ëà∑|Ë´∫|‰πé|Âßë|Áãê|ËÇ°|Ê∂∏|Ëè∞|Ë¢¥|Â£∫|Ë∑®|Á≥ä|ÈÜê|ÈΩ¨|‰∫¢|Âãæ|Âè©|Â∞ª|Âêº|ËÇõ|Â≤°|Â∫ö|Êù≠|ËÇ¥|Âí¨|Âû¢|Â∑∑|ÊÅç|ÊÅ∞|Áã°|Ê°Å|ËÉ±|Â¥ó|Ê¢ó|Âñâ|ËÖî|Ëõ§|Âπå|ÁÖå|Èâ§|Êï≤|Áùæ|ËÜè|Èñ§|ËÜ†|ÁØù|Á∏û|Ëñ®|Á≥†|ËóÅ|ÈÆ´|Â£ô|Êõ†|Âä´|ÊØ´|ÂÇ≤|Â£ï|Êø†|Âöô|ËΩü|Ââã|Âì≠|Èµ†|‰πû|ÂøΩ|ÊÉö|Êòè|Áóï|Ê∏æ|Ë§å|Âèâ|‰∫õ|Âóü|Ëìë|Á£ã|Âùê|Êå´|Êôí|Êü¥|Á†¶|ÁäÄ|Ë≥Ω|È∞ì|Ê¶ä|Êüµ|ÁÇ∏|Á™Ñ|Á∞Ä|Âàπ|Êã∂|Á¥Æ|Êíí|Ëñ©|Áèä|È§ê|Á∫Ç|Èú∞|Êî¢|ËÆÉ|Êñ¨|Êá∫|‰ªî|Âºõ|Ê≠§|ÂùÄ|Á•Ä|Â±ç|Â±é|Êüø|Ëå®|ÊÅ£|Á†•|Á•†|ÁøÖ|Ëàê|Áñµ|Ë∂æ|ÊñØ|Ë¶ó|Âóú|Êªì|ÁçÖ|Âπü|ÊëØ|Âò¥|ÁÜæ|È´≠|Ë¥Ñ|ËÄå|Â≥ô|Áóî|È§å|Á´∫|Èõ´|†Æü|ÊÇâ|Ëõ≠|Â´â|ËÜù|Ê´õ|Êüò|Ê¥í|Â®ë|ÈÄô|Â•¢|Èóç|Êùì|ÁÅº|Á∂Ω|Èå´|ÈõÄ|ÊÉπ|Â®∂|ËÖ´|Ë´è|È¨ö|Âë™|Á´™|Á∂¨|ËÅö|Êø°|Ë•¶|Â∏ö|ÈÖã|Ë¢ñ|Áæû|Ëë∫|Ëíê|ÁÆí|Áö∫|ËºØ|Èç¨|Áπ°|Ëπ¥|ËÆê|È∑≤|Âªø|Êèâ|Áµ®|Á≤•|Êàå|Èñè|Ê•Ø|È¶¥|Êùµ|ËñØ|Ëó∑|Ê±ù|Êäí|Èã§|Â¶æ|Âì®|Áß§|Â®º|ÈÄç|ÂªÇ|Ê§í|Êπò|Á´¶|Èàî|Áù´|Ëõ∏|Èâ¶|Êë∫|Ëî£|Ë£≥|Ë™¶|Êºø|Ëï≠|Ë∏µ|Èûò|ÁØ†|ËÅ≥|Èçæ|ÈÜ¨|ÂõÅ|Êùñ|Ëå∏|Âòó|Êìæ|Êîò|È•í|Êã≠|Âü¥|ËúÄ|Ëùï|Áá≠|Ë§•|Ê≤Å|ËäØ|Âëª|ÂÆ∏|Áñπ|ËúÉ|Êª≤|Ë≥ë|Èçº|Â£¨|Ë®ä|ËÖé|Èù±|Â°µ|ÂÑò|Á¨•|Á•ü|ËÜµ|Ë™∞|Èåê|Èõñ|Èöã|Èöß|Ëäª|Ë∂®|ÈÆ®|‰∏º|ÂáÑ|Ê†ñ|Ê£≤|Áî•|Ë≤∞|Ëúª|ÈÜí|ÈåÜ|Ëáç|ÁÄû|ÈØñ|ËÑÜÂº±?|Ë¥Ö|ËÑä|Êàö|Êô∞|Ëπü|Ê≥Ñ|Â±ë|Êµô|Âïú|Ê•î|Êà™|Â∞ñ|Ëã´|Á©ø|ÈñÉ|Èôù|Èáß|ÊèÉ|ÁÖé|Áæ®|ËÖ∫|Ë©Æ|ÁÖΩ|ÁÆã|Êí∞|ÁÆ≠|Ë≥§|Ëü¨|Áô¨|Âñò|ËÜ≥|Áãô|ÁñΩ|Áñè|Áî¶|Ê•ö|Èº†|ÈÅ°|Ëòá|ÈΩü|Áà™|ÂÆã|ÁÇí|Âèü|Ëö§|Êõæ|Êπä|Ëë±|Êêî|Êßç|Êºï|ÁÆè|Âôå|Áò°|Áò¶|Ë∏™|Ëâò|Ëñî|Áîë|Âè¢|Ëó™|Ë∫Å|ÂõÉ|Á´à|È∞∫|‰ªÑ|Êçâ|Â°û|Á≤ü|Êù£|ÈÅú|ÂôÇ|Ê®Ω|È±í|‰æò|Âí§|Ë©´|ÈôÄ|Êãø|Ëçº|Âîæ|Ëàµ|Ê•ï|È©í|Ëãî|ÊÆÜ|Â†Ü|Á¢ì|ËÖø|È†Ω|Êà¥|ÈÜç|Êâò|Èê∏|Âáß|Ë•∑|Ááµ|Âù¶|Áñ∏|ËÄΩ|Âïñ|Ëõã|ÊØØ|Êπõ|Áó∞|Á∂ª|ÊÜö|Ê≠é|Á∞û|Ë≠ö|ÁÅò|Èõâ|È¶≥|Ëúò|Á∑ª|Á≠ë|ËÜ£|ËÇò|ÂÜë|Á¥ê|ÈÖé|Âé®|Ëõõ|Ë®ª|Ë™Ö|Áñá|Ë∫ä|‰Ωá|Ê•Æ|ÁÆ∏|ÂÑ≤|ÁÄ¶|Ë∫á|Âêä|Â∏ñ|Âñã|Ë≤º|Áâí|Ë∂ô|Èäö|Âò≤|Ë´ú|ÂØµ|Êçó|Êûï|Êßå|Èéö|Ëæª|ÂâÉ|Êå∫|Èáò|Êéü|Ê¢Ø|ÈÄû|Âïº|Á¢á|Èºé|Á∂¥|ÈÑ≠|Ëñô|Ë´¶|ËπÑ|Èµú|Ëçª|Êì¢|Ê∫∫|Âß™|ËΩç|Ëæø|Âî∏|Â°°|ÁØÜ|È°ö|ÂõÄ|Á∫è|‰ΩÉ|Ê∑Ä|Êæ±|ËáÄ|ÂÖé|Â¶¨|ÂÖú|Â†µ|Â±†|Ë≥≠|ÂÆï|Ê≤ì|Â•ó|Áñº|Ê°∂|Ê∑ò|ËêÑ|ÈÄó|Ê£π|Ê®ã|Ëï©|ÈÑß|Ê©ô|Êø§|Ê™Æ|Ê´Ç|Á¶±|Êíû|Á¶ø|ÁÄÜ|Ê†É|ÂíÑ|Ê≤å|ÈÅÅ|È†ì|Âêû|Ë≤™|ÈÇá|ÂåÇ|ÈüÆ|Ê∂Ö|Á¶∞|Êçè|Êçª|Êíö|ËÜø|Âõä|Êù∑|Áà¨|Áê∂|È†ó|Êí≠|Ëä≠|ÁΩµ|Ëüá|ËÉö|Âæò|Áâå|Á®ó|ÁãΩ|ÁÖ§|Â∏õ|Êüè|Ââù|Á≤ï|ÁÆî|Ëé´|ÈßÅ|ÁÄë|Êõù|Áï†|Êçå|Êí•|ÊΩë|ÈÜ±|Á≠è|Ë∑ã|Âô∫|Ê∞æ|Ê±é|Âèõ|Ë¢¢|ÁµÜ|Êñë|ÊßÉ|Âπ°|ÊîÄ|ÊåΩ|Á£ê|ËïÉ|Â±Å|Â∫á|Á†í|ËÑæ|Áó∫|ÈÑô|Ë™π|ËáÇ|Êûá|ÊØò|Ê¢∂|Â™ö|Áêµ|Ëñá|Èù°|Áñã|Áï¢|ÈÄº|Ë¨¨|Ë±π|ÊÜë|Áì¢|Â±õ|Âªü|Áâù|ÁÄï|ÊÜ´|È¨¢|Êñß|Èòú|Ë®É|‰øØ|Èáú|ËÖë|Â≠µ|ÈÆí|Â∑´|Ëë°|Êí´|Ëï™|Ë´∑|Á•ì|Âêª|ÊâÆ|ÁÑö|Á≥û|Âπ∑|ËÅò|ËîΩ|È§Ö|ÊñÉ|Ë¢Ç|ÂÉª|Áíß|Ë•û|Ëîë|Áû•|ÊâÅ|ÁØá|È®ô|Â®©|Èû≠|Âì∫|ÂúÉ|Ëí≤|Êàä|Áâ°|Âß•|Ëè©|ÂëÜ|ÂΩ∑|Â∫ñ|Ëãû|Áñ±|Êçß|ÈÄ¢|ËúÇ|Ëì¨|ÈûÑ|Èãí|Áâü|Ëäí|Ëå´|Ëôª|Ê¶ú|ËÜÄ|Ë≤å|Èâæ|Ë¨ó|Âê†|Âçú|ÂãÉ|Ê¢µ|Êòß|ÈÇÅ|Êû°|‰ø£|Ê≤´|ËøÑ|Êõº|Ëîì|Áûû|È•Ö|È¨ò|È∞ª|Ëúú|Èµ°|ÂÜ•|Áûë|Ë¨é|È∫µ|Ëíô|Êú¶|Âãø|Á±æ|ÊÇ∂|Êè∂|Áà∫|Èëì|Âñ©|ÊèÑ|ÊÑà|Ê•°|Â∞§|Èáâ|Ê•¢|Áå∑|È£´|Ëºø|Â≠ï|Â¶ñ|Êãó|Ê∂å|Áóí|ÂÇ≠|ÁÜî|Áòç|Ë†Ö|Ê≤É|Ëû∫|Ëêä|Ëïæ|Ê¥õ|Âüí|Êãâ|Ëæ£|ÁÄæ|Áàõ|È∏û|Áã∏|Ë£°|ÁΩπ|Á±¨|ÊàÆ|ÊÖÑ|Êé†|Á¨†|Ê∫ú|Ê¶¥|Âäâ|Áò§|‰æ∂|Ê¢Å|ËÅä|Ëè±|ÂØ•|Ëìº|Ê∑ã|Ááê|È±ó|Â±¢|Ëõâ|Ë†£|Ê´ü|Á§´|ËΩ¢|ÁÖâ|Êº£|ÊÜê|Á∞æ|È∞ä|Êî£|Ë≥Ç|È≠Ø|Êøæ|Âª¨|Ê´ì|ËòÜ|È∑∫|ÂºÑ|Áâ¢|Áãº|Ê¶î|Áòª|Ô®ü|Ëáò|Êúß|Ë†ü|Á±†|ËÅæ|ËÇã|Âãí|Êºâ|È∫ì|Á™™|Ê≠™|Áå•|Èöà|Êàñ|ÁΩ†|Ê§Ä|Á¢ó|ÂΩé|‰∏ÄÊó¶).{,5})"
    common_list = regcheck.findall(common_par, input_text)
    
    for word in common_list:
        reason_type = "Â∏∏Áî®Â§ñÊº¢Â≠ó„ÅÆ‰ΩøÁî®"
        corrections.append({
            "page": pageNumber,
            "original_text": word[0],  # original_text,
            "comment": word[0],
            "reason_type": reason_type,
            "check_point": word[1],
            "locations": [],
            "intgr": False,  
        })
#-------------------
    if fund_type == 'public':
        # ÔºàÂçäËßí‚ÜíÂÖ®ËßíÔºâ -0.09% ‚Üí -0.09ÔºÖ
        pattern_half_width_katakana = r"[ÔΩ¶-Ôæù%Ôº†]+"
        half_width_katakana_matches = regcheck.findall(pattern_half_width_katakana, input_text)

        for match in half_width_katakana_matches:
            corrected_text_re = half_and_full_process(match,half_to_full_dict)  # ÂçäËßí‚ÜíÂÖ®Ëßí
            reason_type = "ÂçäËßí„ÇíÂÖ®ËßíÁµ±‰∏Ä"
            original_text = match
            target_text = corrected_text_re
            # „ÄåÔºÖ„ÄçË°®Ë®ò„ÅÆÁµ±‰∏ÄÔºàÂçäËßí‚ÜíÂÖ®ËßíÔºâ -0.09% ‚Üí -0.09ÔºÖ
            comment = f"{reason_type} {original_text} ‚Üí {target_text}"

            corrections.append({
                "page": pageNumber,
                "original_text": original_text,#corrected_text_re
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False, 
            })

        # # ÔºàÂçäËßíÊã¨Âºß ‚Üí ÂÖ®ËßíÊã¨ÂºßÔºâ -() ‚Üí () ,with date format: \((?!\d{4}Âπ¥\d{1,2}Êúà\d{1,2}Êó•)([^)]+)\)
        # pattern_half_width_kuohao = r"\(([^)]+)\)"
        # half_width_kuohao_matches = regcheck.findall(pattern_half_width_kuohao, input_text)

        # for match in half_width_kuohao_matches:
        #     corrected_text_re = half_and_full_process(match,half_to_full_dict)  # ÂçäËßí‚ÜíÂÖ®Ëßí
        #     reason_type = "ÂçäËßíÊã¨Âºß„ÇíÂÖ®ËßíÊã¨Âºß„Å´Áµ±‰∏Ä"
        #     original_text = match
        #     converted = corrected_text_re
        #     target_text = re.sub(r'\(([^)]+)\)', r'Ôºà\1Ôºâ', converted)
        #     # ()Ë°®Ë®ò„ÅÆÁµ±‰∏Ä(ÂàÜÈÖçÈáëÂÜçÊäïË≥á)Ôºâ -(ÂàÜÈÖçÈáëÂÜçÊäïË≥á) ‚Üí ÔºàÂàÜÈÖçÈáëÂÜçÊäïË≥áÔºâ
        #     comment = f"{reason_type} {original_text} ‚Üí {target_text}"

        #     corrections.append({
        #         "page": pageNumber,
        #         "original_text": original_text,#corrected_text_re
        #         "comment": comment,
        #         "reason_type": reason_type,
        #         "check_point": input_text.strip(),
        #         "locations": [],
        #         "intgr": False, 
        #     })

        # ÂçäËßí‚ÜíÂÖ®Ëßí
        pattern_full_width_numbers_and_letters = r"[Ôºê-ÔºôÔº°-Ôº∫ÔºãÔºç]+"
        full_width_matches = regcheck.findall(pattern_full_width_numbers_and_letters, input_text)

        for match in full_width_matches:
            corrected_text_re = half_and_full_process(match,full_to_half_dict)  # ÂÖ®Ëßí‚ÜíÂçäËßí
            reason_type = "ÂÖ®Ëßí„ÇíÂçäËßíÁµ±‰∏Ä"
            original_text = match
            target_text = corrected_text_re

            comment = f"{reason_type} {original_text} ‚Üí {target_text}"

            corrections.append({
                "page": pageNumber,
                "original_text": original_text,
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False, 
            })
            
        # ÔºàÊ≥®0-9Ôºâ--Âà†Èô§
        pattern_full_delete = r"ÔºàÊ≥®[0-9]+Ôºâ"
        full_width_matches_delete = regcheck.findall(pattern_full_delete, input_text)

        for match in full_width_matches_delete:
            corrected_text_re = match
            reason_type = "Âà†Èô§"
            original_text = match
            target_text = corrected_text_re

            comment = f"{reason_type} {original_text} ‚Üí {target_text}"

            corrections.append({
                "page": pageNumber,
                "original_text": original_text,
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False,
            })
#-------------------
    # Âπ¥Â∫¶
    # if fund_type == 'public':
    #     cleaned_text = regcheck.sub(r'\n\s*', '', input_text)
    #     date_pattern = r'(?<!\d{4}Âπ¥)(\d{1,2})Êúà(\d{1,2})Êó•'

    #     for match in regcheck.finditer(date_pattern, cleaned_text):
    #         date_str = match.group(0)               # '4Êúà30Êó•'
    #         date_pos = match.start()            
    #         full_date = insert_year_by_regex(date_str, cleaned_text, date_pos)
    #         half_date = year_half_dict(full_date)

    #         context_pattern = r'.{0,8}' + regcheck.escape(date_str)
    #         context_match = regcheck.search(context_pattern, cleaned_text)
    #         original_text = context_match.group() if context_match else date_str

    #         comment = f"{original_text} ‚Üí {half_date}"
    #         corrections.append({
    #             "page": pageNumber,
    #             "original_text": original_text,
    #             "comment": comment,
    #             "reason_type": 'Âπ¥Â∫¶Áî®Ë™û„ÅÆÁµ±‰∏Ä',
    #             "check_point": 'Âπ¥Â∫¶Áî®Ë™û„ÅÆÁµ±‰∏Ä',
    #             "locations": [],
    #             "intgr": False,  # for debug
    #         })
#-------------------
    # Ëã±Áï•ËØç
    if fund_type == 'public':
        results = opt_check_eng(input_text, replace_rules)

        for line_result in results:
            if line_result:
                for item in line_result:
                    if isinstance(item, dict):
                        for original_text, corrected_text_re in item.items():
                            reason_type = "Áî®Ë™û„ÅÆÁµ±‰∏Ä"
                        
                            if corrected_text_re == "Âà†Èô§":
                                comment = f"{original_text} ‚Üí „Éà„É´„ÅØ‰∏çË¶Å"
                            else:
                                comment = f"{original_text} ‚Üí {corrected_text_re}"

                            corrections.append({
                                "page": pageNumber,
                                "original_text": original_text,
                                "comment": comment,
                                "reason_type": reason_type,
                                "check_point": reason_type,
                                "locations": [],
                                "intgr": False,
                            })


        results_ruru1 = opt_check_ruru1(input_text, replace_rules1)
    
        for item in results_ruru1:
            for k, v in item.items():
                original_text = k
                corrected_text_re = v
                reason_type = "Áî®Ë™û„ÅÆÁµ±‰∏Ä"

                comment = f"{reason_type} {original_text} ‚Üí {corrected_text_re}"

            corrections.append({
                "page": pageNumber,
                "original_text": extract_text(input_text, original_text),# original_text,
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False,  
            })

# Ëã±Áï•ËØçÔºåonly Âú∞ÊîøÂ≠¶
    if fund_type == 'private':
        results_ruru2 = opt_check_ruru2(input_text, replace_rules2)
    
        for item in results_ruru2:
            for k, v in item.items():
                original_text = k  # original_text save to AI
                corrected_text_re = v  # value(v)ÏùÑ corrected_text_re save to AIÔºà‰∫∫Â∑•Áü•ËÉΩÔºâ
                reason_type = "Áî®Ë™û„ÅÆÁµ±‰∏Ä"

                comment = f"{reason_type} {original_text} ‚Üí {corrected_text_re}"

            corrections.append({
                "page": pageNumber,
                "original_text": extract_text(input_text, original_text),# original_text,
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False,
            })

# -----------------
    if fund_type == 'public':
        word_re = regcheck.findall(r"Â§ñÂõΩ‰∫∫ÊäïË≥áÂÆ∂„Åã„Çâ„ÅÆË≥áÈáëÊµÅÂÖ•|Â§ñÂõΩ‰∫∫ÊäïË≥áÂÆ∂„ÅÆË≥áÈáëÊµÅÂá∫|Âä†ÈÄü", input_text)
        for word_result in word_re:
            corrections.append({
                "page": pageNumber,
                "original_text": word_result,
                "comment": f"{word_result} ‚Üí ", #word_result,
                "reason_type": "„É°„ÉÉ„Çª„Éº„Ç∏„ÅÆË°®Á§∫",
                "check_point": word_result,
                "locations": [],  
                "intgr": False,  
            })

        day_re = regcheck.findall(r"\d{1,2}[~ÔΩû]\d{1,2}ÊúàÊúü|\d{1,2}Êúà\d{1,2}Êó•[~ÔΩû]\d{1,2}Êúà\d{1,2}Êó•", input_text)
        for day_result in day_re:
            cor_day = day_result.replace("~", "-").replace("ÔΩû", "-")
            corrections.append({
                "page": pageNumber,
                "original_text": day_result,
                "comment": f"{day_result} ‚Üí {cor_day}",
                "reason_type": "Ê≥¢„ÉÄ„ÉÉ„Ç∑„É•„ÅÆ‰øÆÊ≠£",
                "check_point": day_result,
                "locations": [],  
                "intgr": False,  
            })

        score_re = regcheck.findall(r"[\d.]+?[ÔΩû~][\d.]+?[%ÔºÖ]", input_text)
        for score_result in score_re:
            cor_score = score_result.replace("ÔΩû", "ÔºÖÔΩû").replace("~", "ÔºÖ~")
            corrections.append({
                "page": pageNumber,
                "original_text": score_result,
                "comment": f"{score_result} ‚Üí {cor_score}",
                "reason_type": "Ê≥¢„ÉÄ„ÉÉ„Ç∑„É•„ÅÆ‰øÆÊ≠£",
                "check_point": score_result,
                "locations": [],  
                "intgr": False,  
            })

    half_re = regcheck.findall(r"\d{2,4}Âπ¥Á¨¨[1-4‰∏Ä‰∫å‰∏âÂõõ]ÂõõÂçäÊúü", input_text)
    for half_result in half_re:
        half_num = half_result[-3]
        if half_num in ["1", "‰∏Ä"]:
            time_range = "1-3"
        elif half_num in ["2", "‰∫å"]:
            time_range = "4-6"
        elif half_num in ["3", "‰∏â"]:
            time_range = "7-9"
        else:
            time_range = "10-12"
        cor_half = half_result[: -4] + time_range + half_result[-3:]
        cor_half_len = len(cor_half.split("Âπ¥", 1)[0])
        if cor_half_len < 4:
            cor_half = "20" + cor_half[cor_half_len - 2:]
        corrections.append({
            "page": pageNumber,
            "original_text": half_result,
            "comment": f"{half_result} ‚Üí {cor_half}",
            "reason_type": "Êó•‰ªò„ÅÆ‰øÆÊ≠£",
            "check_point": half_result,
            "locations": [],  
            "intgr": False,  
        })


#-------------------
    # tenbrend
    if isinstance(tenbrend, list):
        for item in tenbrend:
            if not isinstance(item, dict):
                continue

            old_text = item.get("ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "").strip()
            new_text = item.get("Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "").strip()

            corrections.append({
                "check_point": "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨",
                "comment": f"{old_text} ‚Üí {new_text}",
                "intgr": False,
                "locations": [],
                "original_text": new_text[:20],
                "page": pageNumber,
                "reason_type": item.get("ÂàÜÈ°û", "")
            })

#---------------------
# dotfind Âè•ÁÇπ„ÅÆËøΩÂä†  Âè•Ë™≠ÁÇπ  
    for sentence in input_list:
        if "ÔºàÂá∫ÊâÄÔºâ" in sentence:
            continue

        if not check_fullwidth_period(sentence):
            sentence_split = re.sub(r"\s+$", "", sentence)[-30:]
            corrections.append({
                "check_point": "Âè•ÁÇπ„ÅÆËøΩÂä†",
                "comment": f"{sentence_split} ‚Üí {sentence_split}„ÄÇ",
                "intgr": False,
                "locations": [],
                "original_text": sentence_split,
                "page": pageNumber,
                "reason_type": "Âè•ÁÇπ„ÅÆËøΩÂä†",
            })
#--------------------add--0905--
            # ‰∏ªË™ûÊ¨†ËêΩ„ÇíÊ§úÁü•„Åô„ÇãÊ≠£Ë¶èË°®Áèæ pattern
            # „Äå„Äú„Å®Á§∫ÂîÜ„Åó„Åü„Äç „ÅÆÁõ¥Ââç„Å´„Äå„Åå|„ÅØ„Äç„Å™„Å©„ÅÆ‰∏ªË™û 
            pattern = r"([^„ÄÅ„ÄÇ]+?„Å®Á§∫ÂîÜ„Åó„Åü)"

            matches = re.finditer(pattern, input_text)

            for match in matches:
                original_text = match.group(0)
                # ‰øÆÊ≠£Ê°à: „ÄåÂêåÁ§æ„Åå„Äú„Åì„Å®„ÇíÁ§∫ÂîÜ„Åó„Åü„Äç
                corrected_text_re = f"ÂêåÁ§æ„Åå{match.group(1)}„Åì„Å®„ÇíÁ§∫ÂîÜ„Åó„Åü"
                reason_type = "‰∏ªË™û„ÅÆÊ¨†ËêΩ"

                comment = f"{reason_type}: {original_text} ‚Üí {corrected_text_re}"

                corrections.append({
                    "page": pageNumber,
                    "original_text": original_text,
                    "comment": comment,
                    "reason_type": reason_type,
                    "check_point": reason_type,
                    "locations": [],   # ÂøÖË¶Å„Å™„Çâ‰ΩçÁΩÆÊÉÖÂ†±„ÇíËøΩÂä†
                    "intgr": False,
                })
#--------------------
    return corrections

def extract_text(input_text, original_text):
    pattern = rf"{original_text}Ôºà[^Ôºâ]*Ôºâ|{original_text}"

    match = regcheck.search(pattern, input_text)
    
    if match:
        return match.group(0)
    else:
        return None

def clean_percent_prefix(value: str):
    if not isinstance(value, str):
        return None
    for symbol in ['ÔºÖ', '%', '„Éù„Ç§„É≥„Éà']:
        if symbol in value:
            value = value.split(symbol)[0].strip()
            return f"{value}{symbol}"
    return value.strip()
                
def extract_parts_with_direction(text: str):
    parts = re.split(r'[„ÄÅ„ÄÇ\n]', text)
    
    segments = []

    for part in parts:
        part = part.strip()
        if not part:
            continue

        # pattern = r'[^ÔºÖ%„Éù„Ç§„É≥„Éà‰∏ä‰∏ã„ÄÅ„ÄÇ\n]*[+-‚àí]?\d+(?:\.\d+)?(?:ÔºÖ|%|„Éù„Ç§„É≥„Éà)'
        pattern = r'[^ÔºÖ%„ÄÅ„ÄÇ\n]*[+-‚àí]{0,2}\d+(?:\.\d+)?(?:ÔºÖ|%|„Éù„Ç§„É≥„Éà)'
        segments.extend(re.findall(pattern, part))

        # ‰∏ä‰∏ãÊñπÂêë
        # direction_match = re.findall(r'(‰∏äÂõû„Çä„Åæ„Åó„Åü|‰∏ãÂõû„Çä„Åæ„Åó„Åü)', part)
        # segments.extend(direction_match)

    return segments

def extract_corrections(corrected_text, input_text,pageNumber):
    corrections = []
    
    # correction span
    pattern_alt = re.compile(
        r'<span.*?>(.*?)<\/span>\s*'
        r'\(<span>ÊèêÁ§∫:\s*(.*?)\s*<s.*?>(.*?)<\/s>\s*‚Üí\s*(.*?)<\/span>\)',
        re.DOTALL
    )

    matches = pattern_alt.findall(corrected_text)

    for match in matches:
        original = match[0].strip()
        reason = match[2].strip()
        reason_type = match[1].strip()
        corrected = match[3].strip()

        comment = f"{reason} ‚Üí {corrected}" if corrected else reason
        # "%": "ÔºÖ"
        corrections.append({
            "page": pageNumber,
            "original_text": clean_percent_prefix(reason),
            "comment": comment, # +0.2% ‚Üí 0.85% , ‰∏äÂçá -> ‰∏ãËêΩ
            "reason_type": reason_type, # „Éï„Ç°„É≥„Éâ„ÅÆÈ®∞ËêΩÁéáÔºåB-xxx

            "check_point": input_text.strip(), # ÂΩìÊúà„ÅÆ„Éï„Ç°„É≥„Éâ„ÅÆÈ®∞ËêΩÁéá„ÅØ+0.2%„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ A B -xxx
            "locations": [],
            "intgr": True,
        })

    return corrections

    
def add_comments_to_pdf(pdf_bytes, corrections):
    if not isinstance(pdf_bytes, bytes):
        raise ValueError("pdf_bytes must be a bytes object.")
    if not isinstance(corrections, list):
        raise ValueError("corrections must be a list of dictionaries.")
    for correction in corrections:
        if not all(key in correction for key in ["page", "original_text", "comment"]):
            raise ValueError("Each correction must contain 'page', 'original_text', and 'comment' keys.")

    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    except Exception as e:
        raise ValueError(f"Invalid PDF file: {str(e)}")

    for idx, correction in enumerate(corrections):
        page_num = correction["page"]
        comment = correction["comment"]
        reason_type = correction["reason_type"]
        locations = correction["locations"][0]
        text_instances = [fitz.Rect(locations["x0"], locations["y0"], locations["x1"], locations["y1"])]
        if int(text_instances[0][0]) == 0:
            continue
        colorSetFill= (1, 1, 0)

        if page_num < 0 or page_num >= len(doc):
            raise ValueError(f"Invalid page number: {page_num}")

        page = doc.load_page(page_num)

        if correction["intgr"]:
            colorSetFill = (172/255, 228/255, 230/255)
        else:
            colorSetFill= (1, 1, 0)

        for rect in text_instances:
            highlight = page.add_rect_annot(rect)
            highlight.set_colors(stroke=None, fill=colorSetFill)
            highlight.set_opacity(0.5)
            highlight.set_info({
                "title": reason_type,  # ÂèØÈÄâÔºöÊòæÁ§∫Âú®Ê≥®ÈáäÊ°ÜÊ†áÈ¢òÊ†è
                "content": comment
            })
            highlight.update()

    output = io.BytesIO()
    doc.save(output)
    output.seek(0)
    doc.close()

    return output


def add_comments_to_excel(excel_bytes, corrections):
    excel_file = io.BytesIO(excel_bytes)
    workbook = load_workbook(excel_file)  # openpyxl

    for sheet_name in workbook.sheetnames:
        sheet = workbook[sheet_name]
        for correction in corrections:
            if correction["sheet"] == sheet_name:
                cell = correction["cell"]  # : "A1", "B2"
                original_text = correction["original_text"]
                comment = correction["comment"]

                if sheet[cell].value and original_text in str(sheet[cell].value):
                    fill = PatternFill(start_color="FFFF0000", end_color="FFFF0000", fill_type="solid")
                    sheet[cell].fill = fill

                    sheet[cell].comment = Comment(comment, "Author")

    output = io.BytesIO()
    workbook.save(output)
    output.seek(0)
    return output

# pre processing
def normalize_text_for_search(text: str) -> str:
    import re
    replacements = {
        "Ôºà": "(", "Ôºâ": ")", "„Äê": "[", "„Äë": "]",
        "„Äå": "\"", "„Äç": "\"", "„Äé": "\"", "„Äè": "\"",
        "„ÄÄ": " ", "‚óã": "„Äá", "„Éª": "ÔΩ•", 
        "‚Äì": "-", "‚Äï": "-", "‚àí": "-", "„Éº": "-"
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    text = text.replace("\n", " ").replace("\r", " ")
    text = re.sub(r"[\u200b\u200c\u200d\u00a0]", "", text)
    return re.sub(r"\s+", " ", text).strip()


#0617 debug
def find_locations_in_pdf(pdf_bytes, corrections):
    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    except Exception as e:
        raise ValueError(f"Invalid PDF file: {str(e)}")

    for idx, correction in enumerate(corrections):
        page_num = correction.get("page", 0)
        original_text = correction["original_text"]

        if page_num < 0 or page_num >= len(doc):
            print(f"Warning: Invalid page number: {page_num}")
            continue

        page = doc[page_num]
        found_locations = []
        # pre processing
        text_instances = page.search_for(original_text)

        if not text_instances:
            print(f"Warning: Text '{original_text}' not found on page {page_num}.")
            found_locations.append({
                "x0": 0,
                "y0": 0,
                "x1": 0,
                "y1": 0
            })
            
        else:
            for inst in text_instances:
                rect = fitz.Rect(inst)
                found_locations.append({
                    "x0": rect.x0,
                    "y0": rect.y0,
                    "x1": rect.x1,
                    "y1": rect.y1
                })

        if "locations" not in corrections[idx]:
            corrections[idx]["locations"] = []
        corrections[idx]["locations"].extend(found_locations)

    doc.close()
    return corrections


# db and save blob
PUBLIC_FUND_CONTAINER_NAME = "public_Fund"
PRIVATE_FUND_CONTAINER_NAME = "private_Fund"
CHECKED_PDF_CONTAINER = "checked_pdf"

public_container = get_db_connection(PUBLIC_FUND_CONTAINER_NAME)
private_container = get_db_connection(PRIVATE_FUND_CONTAINER_NAME)
checked_pdf_container = get_db_connection(CHECKED_PDF_CONTAINER)

def upload_to_azure_storage(pdf_bytes, file_name, fund_type):
        """Azure Blob Storage PDF"""
        container_name = PUBLIC_FUND_CONTAINER_NAME if fund_type == 'public' else PRIVATE_FUND_CONTAINER_NAME
        
        container_client = get_storage_container()

        try:
            blob_client = container_client.get_blob_client(file_name)
            blob_client.upload_blob(pdf_bytes, overwrite=True)
            logging.info(f"‚úÖ Blob uploaded: {file_name} to {container_name}")
            return blob_client.url
        except Exception as e:
            logging.error(f"‚ùå Storage Upload error: {e}")
            return None
def upload_checked_pdf_to_azure_storage(pdf_bytes, file_name, fund_type):
        """Azure Blob Storage PDF"""
        container_name = CHECKED_PDF_CONTAINER

        container_client = get_storage_container()

        try:
            blob_client = container_client.get_blob_client(file_name)
            blob_client.upload_blob(pdf_bytes, overwrite=True)
            logging.info(f"‚úÖ Blob uploaded: {file_name} to {container_name}")
            return blob_client.url
        except Exception as e:
            logging.error(f"‚ùå Storage Upload error: {e}")
            return None
def download_checked_pdf_from_azure_storage(file_name: str, fund_type: str = None) -> bytes:
    """
    ‰ªé Azure Blob Storage ‰∏ãËΩΩ PDF
    :param file_name: Êñá‰ª∂ÂêçÔºå‰æãÂ¶Ç "a_checked.pdf"
    :param fund_type: ÂÖ¨ÂãüÊàñËÄÖÁßÅÂãü
    :return: PDF Êñá‰ª∂ÁöÑÂ≠óËäÇÊµÅÔºàbytesÔºâÔºåÂ§±Ë¥•Êó∂ËøîÂõû None
    """
    container_name = CHECKED_PDF_CONTAINER
    container_client = get_storage_container()

    try:
        blob_client = container_client.get_blob_client(file_name)
        # ‰∏ãËΩΩ blob Âà∞ÂÜÖÂ≠ò
        download_stream = blob_client.download_blob()
        pdf_bytes = download_stream.readall()
        logging.info(f"üì• Blob downloaded: {file_name} from {container_name}")
        return pdf_bytes
    except Exception as e:
        logging.error(f"‚ùå Storage Download error: {e}")
        return None

def save_to_cosmos(file_name, response_data, link_url, fund_type, upload_type='', comment_type='',icon=''):
    """Cosmos DB Save"""
    # Cosmos DB ËøûÊé•
    container = public_container if fund_type == 'public' else private_container

    # match = re.search(r'(\d{0,}(?:-\d+)?_M\d{4})', file_name)
    # if match:
    #     file_id = match.group(1)
    # else:
    #     file_id = file_name

    item = {
        'id': file_name,
        'fileName': file_name,
        'result': response_data,
        'link': link_url,
        'updateTime': datetime.utcnow().isoformat(),
        'status': "issue", 
        'readStatus': "unread",
        'icon': icon,
    }
    if upload_type:
        item.update(upload_type=upload_type)
    if comment_type:
        item.update(comment_type=comment_type)


    try:
        existing_item = list(container.query_items(
                query="SELECT * FROM c WHERE c.id = @id",
                parameters=[{"name": "@id", "value": file_name}],
                enable_cross_partition_query=True
            ))

        if not existing_item:
                container.create_item(body=item)
                logging.info(f"‚úÖ Cosmos DB „ÅØ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô: {file_name}")
        else:
            existing_item[0].update(item)
            container.upsert_item(existing_item[0])

            logging.info(f"üîÑ Cosmos DB Êõ¥Êñ∞ÂÆå‰∫Ü: {file_name}")
                
    except CosmosHttpResponseError as e:
        logging.error(f"‚ùåCosmos DB save error: {e}")

def save_checked_pdf_cosmos(file_name, response_data, link_url, fund_type, upload_type='', comment_type='',icon=''):
    """Cosmos DB Save"""
    # Cosmos DB ËøûÊé•
    container = 'checked_pdf'

    item = {
        'id': file_name,
        'fileName': file_name,
        'result': response_data,
        'link': link_url,
        'updateTime': datetime.utcnow().isoformat(),
        'status': "checked", 
        'readStatus': "unread",
        'icon': icon,
    }
    if upload_type:
        item.update(upload_type=upload_type)
    if comment_type:
        item.update(comment_type=comment_type)


    try:
        existing_item = list(container.query_items(
                query="SELECT * FROM c WHERE c.id = @id",
                parameters=[{"name": "@id", "value": file_name}],
                enable_cross_partition_query=True
            ))

        if not existing_item:
                container.create_item(body=item)
                logging.info(f"‚úÖ Cosmos DB „ÅØ‰øùÂ≠ò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô: {file_name}")
        else:
            existing_item[0].update(item)
            container.upsert_item(existing_item[0])

            logging.info(f"üîÑ Cosmos DB Êõ¥Êñ∞ÂÆå‰∫Ü: {file_name}")
                
    except CosmosHttpResponseError as e:
        logging.error(f"‚ùåCosmos DB save error: {e}")

@app.route('/api/file_status', methods=['POST'])
def get_file_status():
    data = request.json
    fund_type = data.get("fund_type", "public_Fund")
    file_name = data.get("file_name")
    container = public_container if fund_type == 'public_Fund' else private_container
    if file_name:
        query = f"SELECT * FROM c WHERE c.fileName = '{file_name}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        if items:
            return jsonify({"success": True, "status": True}), 200
    return jsonify({"success": True, "status": False}), 200


@app.route('/api/download_checked_pdf', methods=['POST'])
def download_checked_pdf():
    try:
        data = request.json
        fund_type = data.get("fund_type", "public_Fund")
        file_name = data.get("file_name")
        root, ext = os.path.splitext(file_name)
        if ext.lower() == ".pdf":
            file_name = root + "_checked" + ext
        container = get_db_connection(CHECKED_PDF_CONTAINER)

        query = f"SELECT link_url FROM c WHERE c.file_name = '{file_name}' AND c.fund_type = '{fund_type}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        if items:
            link_url = items[0].get("link_url")
            return jsonify({"success": True, "status": True,"link_url":link_url}), 200
    


    except Exception as e:
        logging.error(f"‚ùå Error in write_checked_pdf: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/write_upload_save', methods=['POST'])
def write_upload_save():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("‚úÖ token upload")

        data = request.json
        pdf_base64 = data.get("pdf_bytes", "")
        excel_base64 = data.get("excel_bytes", "")
        docx_base64 = data.get("docx_bytes", "")
        resutlmap = data.get("original_text", "")
        fund_type = data.get("fund_type", "public")  # 'public'
        file_name_decoding = data.get("file_name", "")
        upload_type = data.get("upload_type", "")
        comment_type = data.get("comment_type", "")
        icon = data.get("icon", "")
        change_flag = data.get("change_flag", "")

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        #---------EXCEL-----------
        if excel_base64:
            try:
                excel_bytes = base64.b64decode(excel_base64)
                response_data = {
                    "success": True,
                    "corrections": []
                }

                # Blob Upload
                link_url = upload_to_azure_storage(excel_bytes, file_name, fund_type)
                if not link_url:
                    return jsonify({"success": False, "error": "Blob upload failed"}), 500

                # Cosmos DB Save
                save_to_cosmos(file_name, response_data, link_url, fund_type, upload_type, comment_type,icon)
                if upload_type != "ÂèÇÁÖß„Éï„Ç°„Ç§„É´" and change_flag == "change":
                    container = get_db_connection(FILE_MONITOR_ITEM)
                    container.upsert_item({"id": str(uuid.uuid4()), "file_name": file_name, "flag": "wait",
                                            "link": link_url, "fund_type": fund_type})

            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500

            # 3) return xlsx
            return jsonify({
                "success": True,
                "corrections": [],
                "code": 200,
            })
        # ---------PDF -----------
        if pdf_base64:
            try:
                pdf_bytes = base64.b64decode(pdf_base64)

                response_data = {
                    "corrections": []
                }

                # Blob Upload
                link_url = upload_to_azure_storage(pdf_bytes, file_name, fund_type)
                if not link_url:
                    return jsonify({"success": False, "error": "Blob upload failed"}), 500

                # Cosmos DB Save
                save_to_cosmos(file_name, response_data, link_url, fund_type, upload_type, comment_type,icon)

            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500
            
        # ---------DOCX -----------
        if docx_base64:
            try:
                docx_bytes = base64.b64decode(docx_base64)

                response_data = {
                    "corrections": []
                }

                # Blob Upload
                link_url = upload_to_azure_storage(docx_bytes, file_name, fund_type)
                if not link_url:
                    return jsonify({"success": False, "error": "Blob upload failed"}), 500

                # Cosmos DB Save
                save_to_cosmos(file_name, response_data, link_url, fund_type, upload_type, comment_type,icon)
                if upload_type != "ÂèÇÁÖß„Éï„Ç°„Ç§„É´" and change_flag == "change":
                    container = get_db_connection(FILE_MONITOR_ITEM)
                    container.upsert_item({"id": str(uuid.uuid4()), "file_name": file_name, "flag": "wait",
                                            "link": link_url, "fund_type": fund_type})

            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500
            
            # return JSON
            return jsonify({
                "success": True,
                "corrections": [],
                "code": 200,
            })

        # return JSON
        return jsonify({
            "success": True,
            "corrections": [],
            "code": 200,
        })

    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

def apply_manual_corrections(text, correction_map):
    if text in correction_map:
        result = correction_map[text]
    # for old_text, new_text in correction_map.items():
    #     if old_text in text:
    #         text = text.replace(old_text, new_text)
    return result

def gpt_correct_text(prompt):
    token = token_cache.get_token()
    openai.api_key = token
    print("‚úÖ Token Update SUCCESS")
    
    if not prompt.strip():
        return prompt
    hyogaiKanjiList = []
    
    corrected_text = detect_hyogai_kanji(prompt, hyogaiKanjiList)
    prompt_result = f"""
    You are a professional Japanese text proofreading assistant. Please carefully proofread the following Japanese text and provide corrections in a structured `corrected_map` format.

    **Text to Proofread:**
    - {prompt}

    **Proofreading Requirements:**
    1. **Check for typos and missing characters (Ë™§Â≠óËÑ±Â≠ó„Åå„Å™„ÅÑ„Åì„Å®):**
    - Ensure there are no spelling errors or missing characters in the content of the report.
    - If errors are found, add them to the `corrected_map` in the format: "incorrect": "correct".

    2. **Follow the Fund Manager Comment Terminology Guide („Éï„Ç°„É≥„Éâ„Éû„Éç„Éº„Ç∏„É£„Ç≥„É°„É≥„ÉàÁî®Ë™ûÈõÜ„Å´Ê≤ø„Å£„ÅüË®òËºâ„Å®„Å™„Å£„Å¶„ÅÑ„Çã„Åì„Å®):**
    - **Consistent Terminology (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä):**
        - Ensure the writing format of terms is consistent throughout the report.
    - **Prohibited Words and Phrases (Á¶ÅÊ≠¢ÔºàNGÔºâ„ÉØ„Éº„ÉâÂèä„Å≥ÊñáÁ´†„ÅÆÊ≥®ÊÑè‰∫ãÈ†Ö):**
        - Check if any prohibited words or phrases are used in the report and correct them as per the guidelines.
    - **Replaceable and Recommended Terms/Expressions (ÁΩÆ„ÅçÊèõ„Åà„ÅåÂøÖË¶Å„Å™Áî®Ë™û/Ë°®Áèæ„ÄÅÁΩÆ„ÅçÊèõ„Åà„ÇíÊé®Â•®„Åô„ÇãÁî®Ë™û/Ë°®Áèæ):**
        - If you find terms or expressions that need to be replaced, revise them according to the provided rules.
    - **Use of Hiragana („Å≤„Çâ„Åå„Å™„ÇíË°®Ë®ò„Åô„Çã„ÇÇ„ÅÆ):**
        - Ensure the report follows the rules for hiragana notation, replacing content that does not conform to commonly used kanji.
    - **Kana Notation for Non-Standard Kanji (‰∏ÄÈÉ®„Åã„Å™Êõ∏„ÅçÁ≠â„ÅßË°®Ë®ò„Åô„Çã„ÇÇ„ÅÆ):**
        - Ensure non-standard kanji are replaced with kana as the standard writing format.
    - **Correct Usage of Okurigana (‰∏ÄËà¨ÁöÑ„Å™ÈÄÅ„Çä‰ªÆÂêç„Å™„Å©):**
        - Ensure the correct usage of okurigana is applied.
    - **English Abbreviations, Loanwords, and Technical Terms (Ëã±Áï•Ë™û„ÄÅÂ§ñÊù•Ë™û„ÄÅÂ∞ÇÈñÄÁî®Ë™û„Å™„Å©):**
        - Check if English abbreviations, loanwords, and technical terms are expressed correctly.
    - **Identify and mark any Â∏∏Áî®Â§ñÊº¢Â≠ó (Hy≈çgai kanji):**
        - Identify and mark any Â∏∏Áî®Â§ñÊº¢Â≠ó (Hy≈çgai kanji) in the following text.
        - Â∏∏Áî®Â§ñÊº¢Â≠ó refers to Chinese characters that are not included in the [Â∏∏Áî®Êº¢Â≠óË°® (J≈çy≈ç kanji list)](https://ja.wikipedia.org/wiki/Â∏∏Áî®Êº¢Â≠ó), the official list of commonly used kanji in Japan.
        - For any Â∏∏Áî®Â§ñÊº¢Â≠ó identified, mark the character with (Â∏∏Áî®Â§ñÊº¢Â≠ó) next to it.

        1. ÂÖ•Âäõ„Åï„Çå„ÅüÂÖ®ÊñáÔºàReport Content to ProofreadÔºâ„Çí **‰∏ÄÊñáÂ≠ó„Åö„Å§** Ëµ∞Êüª„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÂçòË™ûÂçò‰Ωç„Åß„ÅØ„Å™„ÅèÊñáÂ≠óÂçò‰Ωç„ÅÆÁÖßÂêà„Åß„ÅôÔºâ„ÄÇ
        2. ÂêÑÊñáÂ≠ó„Çí„ÄÅÊåáÂÆö„Åï„Çå„Åü hyogaiKanjiList „ÅÆÊñáÂ≠ó„Å® **ÂÆåÂÖ®‰∏ÄËá¥** „ÅßÊØîËºÉ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        3. ‰∏ÄËá¥„Åô„ÇãÊñáÂ≠ó„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅ„Åù„ÅÆÊñáÂ≠ó„Çí„ÄåÂ∏∏Áî®Â§ñÊº¢Â≠ó„Äç„Å®„Åó„Å¶Ê§úÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        4. ‰∏ÄËá¥„Åó„Å™„ÅÑÊñáÂ≠ó„ÅØ„ÄÅÂ∏∏Áî®Êº¢Â≠ó„Å®„Åó„Å¶ÁÑ°Ë¶ñ„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàË™§Ê§úÂá∫„ÇíÈÅø„Åë„Çã„Åü„ÇÅÔºâ„ÄÇ
        5. Ê§úÂá∫„Åï„Çå„ÅüÂ∏∏Áî®Â§ñÊº¢Â≠ó„ÅØ„ÄÅ‰ª•‰∏ã„ÅÆ„Éï„Ç©„Éº„Éû„ÉÉ„Éà„ÅßÊ≥®Èáà„Çí„Å§„Åë„Å¶Ë°®Á§∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

        ---

        „ÄêÊ≥®Èáà„Éï„Ç©„Éº„Éû„ÉÉ„Éà„Äë

        Ê¨°„ÅÆ„Çà„ÅÜ„Å´„ÄÅÂÖÉ„ÅÆÊº¢Â≠ó„Å´ <s> „Çø„Ç∞„Å®ËÉåÊôØËâ≤„Çí‰ªò„Åë„ÄÅË™≠„Åø„Åæ„Åü„ÅØ‰ª£ÊõøË™û„ÇíËµ§Â≠ó„ÅßÁ§∫„Åó„ÄÅ„Åù„ÅÆÂæå„Å´ÁêÜÁî±„ÇíÊ∑ª„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

        ‰æã:
        <span style="color:red;">„Åú„ÅÑ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â∏∏Áî®Â§ñÊº¢Â≠ó„ÅÆ‰ΩøÁî® <s style="background:yellow;color:red">ËÑÜ</s> ‚Üí „Åú„ÅÑ</span>)

        ---

        „ÄêReport Content to Proofread„Äë:
        {corrected_text}

        **1: Typographical Errors (ËÑ±Â≠ó„ÉªË™§Â≠ó) Detection**
            -Detect any missing characters (ËÑ±Â≠ó) or misused characters (Ë™§Â≠ó) that cause unnatural expressions or misinterpretation.

            **Proofreading Requirements**:
            - Detect and correct all genuine missing characters (ËÑ±Â≠ó) or misused characters (Ë™§Â≠ó) that cause grammatical errors or change the intended meaning.
            - Always detect and correct any incorrect conjugations, misused readings, or wrong kanji/verb usage, even if they superficially look natural.
            - Do not point out stylistic variations, natural auxiliary expressions, or acceptable conjugations unless they are grammatically incorrect.
            - Confirm that each kanji matches the intended meaning precisely.
            - Detect cases where non-verb terms are incorrectly used as if they were verbs.

            - ‚Äù„Å®‚Äù„ÇíËÑ±Â≠ó„Åó„Åæ„Åó„Åü:
                -Example:
                input: ÊúàÈñì„Åß„ÅØ„Åª„ÅºÂ§â„Çè„Çâ„Åö„Å™„Çä„Åæ„Åó„Åü„ÄÇ
                output:
                <span style="color:red;">ÊúàÈñì„Åß„ÅØ„Åª„ÅºÂ§â„Çè„Çâ„Åö„Å™„Çä„Åæ„Åó„Åü„ÄÇ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ë™§Â≠ó <s style="background:yellow;color:red">ÊúàÈñì„Åß„ÅØ„Åª„ÅºÂ§â„Çè„Çâ„Åö„Å™„Çä„Åæ„Åó„Åü„ÄÇ</s> ‚Üí ÊúàÈñì„Åß„ÅØ„Åª„ÅºÂ§â„Çè„Çâ„Åö„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ</span>)
            - The kanji 'Ââ§' was incorrectly used instead of 'Ê∏à', resulting in a wrong word formation.
                -Example:
                input: ÁµåÂâ§ÊàêÈï∑
                output:
                <span style="color:red;">ÁµåÊ∏àÊàêÈï∑</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ë™§Â≠ó <s style="background:yellow;color:red">ÁµåÂâ§ÊàêÈï∑</s> ‚Üí ÁµåÊ∏àÊàêÈï∑</span>)
            - The verb "ÈÅä„Å∂" was incorrectly conjugated into a non-existent form "„ÅÇ„Åù„Åº„Çå„Çã" instead of the correct passive form "„ÅÇ„Åù„Å∞„Çå„Çã".
                -Example:
                input: „ÅÇ„Åù„Åº„Çå„Åæ„Åô„Åã„ÄÇ
                output:
                <span style="color:red;">„ÅÇ„Åù„Å∞„Çå„Åæ„Åô„Åã„ÄÇ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÂãïË©ûÊ¥ªÁî®„ÅÆË™§„Çä <s style="background:yellow;color:red">„ÅÇ„Åù„Åº„Çå„Åæ„Åô„Åã„ÄÇ</s> ‚Üí „ÅÇ„Åù„Å∞„Çå„Åæ„Åô„Åã„ÄÇ</span>)
            - "„Å®"„ÇíÁúÅÁï•„Åó„Åü„Çâ„ÄÅ„Äå„Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„Éà„Äç„ÅØÂêçË©û„Åß„ÅÇ„Çä„ÄÅÂãïË©û„ÅÆ„Çà„ÅÜ„Å´„Äå„Äú„Åó„Åü„Äç„Å®Ê¥ªÁî®„Åô„Çã„ÅÆ„ÅØÊñáÊ≥ïÁöÑ„Å´Ë™§„Çä„Åß„Åô„ÄÇ
                -Example:
                input: „Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„ÉàÔºàÂèÇËÄÉÊåáÊï∞„Å®ÊØî„Åπ‰Ωé„ÇÅ„ÅÆÊäïË≥áÊØîÁéáÔºâ„Åó„Åü
                output:
                <span style="color:red;">„Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„ÉàÔºàÂèÇËÄÉÊåáÊï∞„Å®ÊØî„Åπ‰Ωé„ÇÅ„ÅÆÊäïË≥áÊØîÁéáÔºâ„Å®„Åó„Åü</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÂãïË©ûÊ¥ªÁî®„ÅÆË™§„Çä <s style="background:yellow;color:red">„Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„ÉàÔºàÂèÇËÄÉÊåáÊï∞„Å®ÊØî„Åπ‰Ωé„ÇÅ„ÅÆÊäïË≥áÊØîÁéáÔºâ„Åó„Åü</s> ‚Üí „Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„ÉàÔºàÂèÇËÄÉÊåáÊï∞„Å®ÊØî„Åπ‰Ωé„ÇÅ„ÅÆÊäïË≥áÊØîÁéáÔºâ„Å®„Åó„Åü</span>)


            **correct Example*:
            - "Âèñ„ÇäÁµÑ„Åø„Åó"„ÅØËá™ÁÑ∂„Å™ÈÄ£Áî®ÂΩ¢Ë°®Áèæ„ÅÆ„Åü„ÇÅ„ÄÅ‰øÆÊ≠£‰∏çË¶Å'
                -Example:
                input: „Ç∞„É≠„Éº„Éê„É´„Åß‰∫ãÊ•≠„ÇíÂ±ïÈñã„Åô„Çã
                output:
                <span style="color:red;">„Ç∞„É≠„Éº„Éê„É´„Å´‰∫ãÊ•≠„ÇíÂ±ïÈñã„Åô„Çã</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ê†ºÂä©Ë©û„ÅÆË™§Áî® <s style="background:yellow;color:red">„Ç∞„É≠„Éº„Éê„É´„Åß‰∫ãÊ•≠„ÇíÂ±ïÈñã„Åô„Çã</s> ‚Üí „Ç∞„É≠„Éº„Éê„É´„Å´‰∫ãÊ•≠„ÇíÂ±ïÈñã„Åô„Çã</span>)

        **2: Punctuation (Âè•Ë™≠ÁÇπ) Usage Check**
            -Detect missing, excessive, or incorrect use of punctuation marks („ÄÅ„ÄÇ).

            **Proofreading Requirements**:
            -Ensure sentences correctly end with„Äå„ÄÇ„Äçwhere appropriate.
            -Avoid redundant commas„Äå„ÄÅ„Äçin unnatural positions.
            -Maintain standard business writing style.

            -Example:
            input: ÂèéÁõäË¶ãÈÄö„Åó„ÅåÊúüÂæÖ„Åß„Åç„Çã‰ºÅÊ•≠„Çí‰∏≠ÂøÉ„Å´ÊäïË≥á„ÇíË°å„Å™„ÅÜÊñπÈáù„Åß„Åô
            output:
            <span style="color:red;">ÂèéÁõäË¶ãÈÄö„Åó„ÅåÊúüÂæÖ„Åß„Åç„Çã‰ºÅÊ•≠„Çí‰∏≠ÂøÉ„Å´ÊäïË≥á„ÇíË°å„Å™„ÅÜÊñπÈáù„Åß„Åô</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÊñáÊú´Âè•ÁÇπ„ÅÆÊ¨†Â¶Ç <s style="background:yellow;color:red"ÂèéÁõäË¶ãÈÄö„Åó„ÅåÊúüÂæÖ„Åß„Åç„Çã‰ºÅÊ•≠„Çí‰∏≠ÂøÉ„Å´ÊäïË≥á„ÇíË°å„Å™„ÅÜÊñπÈáù„Åß„Åô</s> ‚Üí ÂèéÁõäË¶ãÈÄö„Åó„ÅåÊúüÂæÖ„Åß„Åç„Çã‰ºÅÊ•≠„Çí‰∏≠ÂøÉ„Å´ÊäïË≥á„ÇíË°å„Å™„ÅÜÊñπÈáù„Åß„Åô„ÄÇ</span>)

        **3: Unnatural Spaces (‰∏çËá™ÁÑ∂„Å™Á©∫ÁôΩ) Detection**
            -Detect unnecessary half-width or full-width spaces within sentences.

            **Proofreading Requirements**:
            -Remove any redundant spaces between words or inside terms.
            -Confirm that spacing follows standard Japanese document conventions.

            -Example:
            input: ÈÄÅ ÈÖçÈõªË®≠ÂÇô
            output:
            <span style="color:red;">ÈÄÅÈÖçÈõªË®≠ÂÇô</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ‰∏çË¶Å„Çπ„Éö„Éº„ÇπÂâäÈô§ <s style="background:yellow;color:red">ÈÄÅ ÈÖçÈõªË®≠ÂÇô</s> ‚Üí ÈÄÅÈÖçÈõªË®≠ÂÇô</span>)

            -Example:
            input: „Éû„Ç§„ÇØ„É≠„Ç≥„É≥„Éà„É≠„Éº„É©„Éº„ÇÑ Èñ¢ÈÄ£„ÅÆË§áÂêà‰ø°Âè∑Ë£ΩÂìÅ
            output:
            <span style="color:red;">„Éû„Ç§„ÇØ„É≠„Ç≥„É≥„Éà„É≠„Éº„É©„Éº„ÇÑÈñ¢ÈÄ£„ÅÆË§áÂêà‰ø°Âè∑Ë£ΩÂìÅ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ‰∏çË¶Å„Çπ„Éö„Éº„ÇπÂâäÈô§ <s style="background:yellow;color:red">„Éû„Ç§„ÇØ„É≠„Ç≥„É≥„Éà„É≠„Éº„É©„Éº„ÇÑ Èñ¢ÈÄ£„ÅÆË§áÂêà‰ø°Âè∑Ë£ΩÂìÅ</s> ‚Üí „Éû„Ç§„ÇØ„É≠„Ç≥„É≥„Éà„É≠„Éº„É©„Éº„ÇÑÈñ¢ÈÄ£„ÅÆË§áÂêà‰ø°Âè∑Ë£ΩÂìÅ</span>)


        **4: Omission or Misuse of Particles (Âä©Ë©û„ÅÆÁúÅÁï•„ÉªË™§Áî®) Detection**
            - Detect omissions and misuses of grammatical particles (Âä©Ë©û), especially„Äå„ÅÆ„Äç„Äå„Çí„Äç„Äå„Å´„Äç, that lead to structurally incorrect or unnatural expressions.

            **Proofreading Requirements**:

            - Carefully examine whether all necessary particles‚Äîparticularly„Äå„ÅÆ„Äç„Äå„Çí„Äç„Äå„Å´„Äç‚Äîare correctly used in every sentence.
            - Do not tolerate the omission of any structurally required particle, even if the sentence appears understandable or natural overall.
            - Focus on grammatical correctness, not perceived readability.
            - In long texts, perform sentence-by-sentence proofreading to ensure no required particle is missing at any position.
            - If a particle should be present according to standard Japanese grammar but is omitted, it must be explicitly identified and corrected.

            -Example:
            input: Ê¨ßÂ∑û„Å™„Å©Â∏ÇÂ†¥Ë™øÊüªÈñãÂßã„Åó„Å¶
            output:
            <span style="color:red;">Ê¨ßÂ∑û„Å™„Å©„ÅÆÂ∏ÇÂ†¥Ë™øÊüª„ÇíÈñãÂßã„Åó„Å¶</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÈÄ£‰Ωì‰øÆÈ£æ„ÅÆÂä©Ë©ûÁúÅÁï• <s style="background:yellow;color:red">Ê¨ßÂ∑û„Å™„Å©Â∏ÇÂ†¥Ë™øÊüªÈñãÂßã„Åó„Å¶</s> ‚Üí Ê¨ßÂ∑û„Å™„Å©„ÅÆÂ∏ÇÂ†¥Ë™øÊüª„ÇíÈñãÂßã„Åó„Å¶</span>)

            -Example:
            input: ECBÔºàÊ¨ßÂ∑û‰∏≠Â§ÆÈäÄË°åÔºâ„Å™„Å©Êµ∑Â§ñ‰∏ªË¶Å‰∏≠ÈäÄ„Å´„Çà„Çã
            output:
            <span style="color:red;">ECBÔºàÊ¨ßÂ∑û‰∏≠Â§ÆÈäÄË°åÔºâ„Å™„Å©„ÅÆÊµ∑Â§ñ‰∏ªË¶Å‰∏≠ÈäÄ„Å´„Çà„Çã</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÊâÄÊúâÊ†ºÂä©Ë©û„Äå„ÅÆ„Äç„ÅÆÁúÅÁï• <s style="background:yellow;color:red">ECBÔºàÊ¨ßÂ∑û‰∏≠Â§ÆÈäÄË°åÔºâ„Å™„Å©Êµ∑Â§ñ‰∏ªË¶Å‰∏≠ÈäÄ„Å´„Çà„Çã</s> ‚Üí ECBÔºàÊ¨ßÂ∑û‰∏≠Â§ÆÈäÄË°åÔºâ„Å™„Å©„ÅÆÊµ∑Â§ñ‰∏ªË¶Å‰∏≠ÈäÄ„Å´„Çà„Çã</span>)

            -Example:
            input: 5000ÂÑÑÂÜÜ
            output:
            <span style="color:red;">5,000ÂÑÑÂÜÜ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: ÈáëÈ°ç„Ç´„É≥„ÉûÂå∫Âàá„Çä <s style="background:yellow;color:red">5000ÂÑÑÂÜÜ</s> ‚Üí 5,000ÂÑÑÂÜÜ</span>)

        **5: Monetary Unit & Number Format (ÈáëÈ°çË°®Ë®ò„ÉªÊï∞ÂÄ§„Éï„Ç©„Éº„Éû„ÉÉ„Éà) Check**

            -Detect mistakes in number formatting, especially monetary values.
            -Proofreading Requirements:
            -Apply comma separator every three digits for numbers over 1,000.
            -Ensure currency units (ÂÜÜ„ÄÅÂÖÜÂÜÜ„ÄÅÂÑÑÂÜÜ) are correctly used.
            -Standardize half-width characters where needed.

            -Example:
            input: ÂØæÂøú„Å´„ÅØÊñ∞„Åü„Å™ÊäÄË°ìÈñãÁô∫„ÇÑÂà∂Â∫¶ÊîπÈù©„ÅÆÂøÖË¶ÅÊÄß„ÅåÊåáÊëò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ
            output:
            <span style="color:red;">ÂØæÂøú„ÅØÊñ∞„Åü„Å™ÊäÄË°ìÈñãÁô∫„ÇÑÂà∂Â∫¶ÊîπÈù©„ÅÆÂøÖË¶ÅÊÄß„ÅåÊåáÊëò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Ê†ºÂä©Ë©û„Äå„Å´„ÅØ„Äç„ÅÆË™§Áî® <s style="background:yellow;color:red">ÂØæÂøú„Å´„ÅØÊñ∞„Åü„Å™ÊäÄË°ìÈñãÁô∫„ÇÑÂà∂Â∫¶ÊîπÈù©„ÅÆÂøÖË¶ÅÊÄß„ÅåÊåáÊëò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ</s> ‚Üí ÂØæÂøú„ÅØÊñ∞„Åü„Å™ÊäÄË°ìÈñãÁô∫„ÇÑÂà∂Â∫¶ÊîπÈù©„ÅÆÂøÖË¶ÅÊÄß„ÅåÊåáÊëò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇ</span>)


            **Special Instructions**:
            - Always annotate all detected Hy≈çgai Kanji.
            - Never replace or modify the character unless explicitly instructed.

        **6: Detection of Misused Enumerative Particle„Äå„ÇÑ„Äç**
            **Proofreading Targets**:
            - Detect inappropriate use of the enumerative particle„Äå„ÇÑ„Äçwhen it connects elements with different grammatical structures.
            - The particle„Äå„ÇÑ„Äçmust only be used to list **nouns or noun phrases** that are grammatically equivalent.
            - If the item following„Äå„ÇÑ„Äçis a **verb phrase**, **adverbial clause**, or a structurally different element, then„Äå„ÇÑ„Äçis incorrect.
            - In such cases, replace„Äå„ÇÑ„Äçwith a comma„Äå„ÄÅ„Äçto properly separate clauses or adjust the sentence structure.


        **Special Rules:**
        1. **Do not modify proper nouns (e.g., names of people, places, or organizations) unless they are clearly misspelled.**
            -Exsample:
            „Éô„ÉÉ„Çª„É≥„ÉàÊ∞è: Since this is correct and not a misspelling, it will not be modified.
        2. **Remove unnecessary or redundant text instead of replacing it with other characters.**
            -Exsample:
            „É¶„Éº„É≠ÂúèÂüüÂÜÖ„ÅÆÊôØÊ∞óÂßî: Only the redundant character Âßî will be removed, and no additional characters like „ÅÆ will be added. The corrected text will be: „É¶„Éº„É≠ÂúèÂüüÂÜÖ„ÅÆÊôØÊ∞ó.
        3. **Preserve spaces between words in the original text unless they are at the beginning or end of the text.**
            -Example:
            input: Êúà„ÅÆ ÂâçÂçä„ÅØÁ±≥ÂõΩ„ÅÆ ÂÇµÂà∏Âà©Âõû„Çä„ÅÆ‰∏äÊòá „Å´„Å§„Çå„Å¶
            Output: Êúà„ÅÆ ÂâçÂçä„ÅØÁ±≥ÂõΩ„ÅÆ ÂÇµÂà∏Âà©Âõû„Çä„ÅÆ‰∏äÊòá „Å´„Å§„Çå„Å¶ (spaces between words are preserved).
        - **Task**: Header Date Format Validation & Correction  
        - **Target Area**: Date notation in parentheses following "‰ªäÂæåÈÅãÁî®ÊñπÈáù (Future Policy Decision Basis)"  
        ---
        ### Validation Requirements  
        1. **Full Format Compliance Check**:  
        - Must follow "YYYYÂπ¥MMÊúàDDÊó•ÁèæÂú®" (Year-Month-Day as of)  
        - **Year**: 4-digit number (e.g., 2024)  
        - **Month**: 2-digit (01-12, e.g., April ‚Üí 04)  
        - **Day**: 2-digit (01-31, e.g., 5th ‚Üí 05)  
        - **Suffix**: Must end with "ÁèæÂú®" (as of)  

        2. **Common Error Pattern Detection**:  
        ‚ùå "1Êúà0Êó•" ‚Üí Missing month leading zero + invalid day 0  
        ‚ùå "2024Âπ¥4Êúà1Êó•" ‚Üí Missing month leading zero (should be 04)  
        ‚ùå "2024Âπ¥12Êúà" ‚Üí Missing day value  
        ‚ùå "2024-04-05ÁèæÂú®" ‚Üí Incorrect separator usage (hyphen/slash)  
        ---
        ### Correction Protocol  
        1. **Leading Zero Enforcement**  
        - Add leading zeros to single-digit months/days (4Êúà ‚Üí 04Êúà, 5Êó• ‚Üí 05Êó•)  

        2. **Day 0 Handling**  
        - Replace day 0 with YYYYMMDD Date Format  
        - Example: 2024Âπ¥4Êúà0Êó• ‚Üí 2024Âπ¥04Êúà00Êó•

        3. **Separator Standardization**  
        - Convert hyphens/slashes to CJK characters:  
            `2024/04/05` ‚Üí `2024Âπ¥04Êúà05Êó•`  


        4. **Consistency with Report Data Section („É¨„Éù„Éº„Éà„ÅÆ„Éá„Éº„ÇøÈÉ®„Å®„ÅÆÊï¥ÂêàÊÄßÁ¢∫Ë™ç):**
        - Ensure the textual description in the report is completely consistent with the data section, without any logical or content-related discrepancies.

        5. **Eliminate language fluency(ÂçòË™ûÈñì„ÅÆ‰∏çË¶Å„Å™„Çπ„Éö„Éº„ÇπÂâäÈô§):**
        - Ensure that there are no extra spaces.
            -Example:
            input:ÊôØÊ∞óÊµÆÊèö„ÅåÊÑè Ë≠ò„Åï„Çå„Åü„Åì„Å®„Åß
            output:ÊôØÊ∞óÊµÆÊèö„ÅåÊÑèË≠ò„Åï„Çå„Åü„Åì„Å®„Åß
        
        6.  **Layout and Formatting Rules („É¨„Ç§„Ç¢„Ç¶„Éà„Å´Èñ¢„Åô„ÇãÁµ±‰∏Ä):**
            - **ÊñáÈ†≠„ÅÆ„Äå‚óã„ÄçÂç∞„Å®‰∏ÄÊñáÂ≠óÁõÆ„ÅÆÈñìÈöî„ÇíÁµ±‰∏Ä:**
                - When a sentence begins with the "‚óã" symbol, ensure the spacing between the symbol and the first character is consistent across the document.
            - **ÊñáÁ´†„ÅÆÈñìÈöî„ÅÆÁµ±‰∏Ä:**
                - If a sentence begins with "‚óã", ensure that the spacing within the frame remains consistent.
            - **‰∏ä‰Ωç10ÈäòÊüÑ „Ç≥„É°„É≥„ÉàÊ¨Ñ„Å´„Å§„ÅÑ„Å¶„ÄÅÊû†ÂÜÖ„Å´ÈÅ©Âàá„Å´Âèé„Åæ„Å£„Å¶„ÅÑ„Çã„Åã„ÉÅ„Çß„ÉÉ„ÇØ:**
                - If the stock commentary contains a large amount of text, confirm whether it fits within the designated frame. 
                - If the ranking changes in the following month, adjust the frame accordingly.
                - **Check point**
                    1. **ÊñáÂ≠óÊï∞Âà∂ÈôêÂÜÖ„Å´Âèé„Åæ„Å£„Å¶„ÅÑ„Çã„ÅãÔºü**
                    - 1Êû†„ÅÇ„Åü„Çä„ÅÆÊúÄÂ§ßÊñáÂ≠óÊï∞„ÇíË∂Ö„Åà„Å¶„ÅÑ„Å™„ÅÑ„ÅãÔºü
                    - ÈÅ©Âàá„Å™Ë°åÊï∞„ÅßÂèé„Åæ„Å£„Å¶„ÅÑ„Çã„ÅãÔºü

                    2. **Ê¨°Êúà„ÅÆÈ†Ü‰ΩçÂ§âÂãï„Å´‰º¥„ÅÜÊû†Ë™øÊï¥„ÅÆÂøÖË¶ÅÊÄß**
                    - È†Ü‰Ωç„ÅåÂ§âÊõ¥„Åï„Çå„Çã„Å®Êû†Ë™øÊï¥„ÅåÂøÖË¶Å„Å™„Åü„ÇÅ„ÄÅË™øÊï¥„ÅåÂøÖË¶Å„Å™ÁÆáÊâÄ„ÇíÁâπÂÆö

                    3. **Êû†ÂÜÖ„Å´Âèé„Åæ„Çâ„Å™„ÅÑÂ†¥Âêà„ÅÆ‰øÆÊ≠£ÊèêÊ°à**
                    - ÂøÖË¶Å„Å´Âøú„Åò„Å¶„ÄÅÁü≠Á∏ÆË°®Áèæ„ÇÑ‰∏çË¶Å„Å™ÊÉÖÂ†±„ÅÆÂâäÈô§„ÇíÊèêÊ°à
                    - ÈáçË¶Å„Å™ÊÉÖÂ†±„ÇíÊêç„Å™„Çè„Åö„Å´ÈÅ©Âàá„Å´„É™„É©„Ç§„Éà

                    output Format:
                    - **„Ç≥„É°„É≥„Éà„ÅÆÊû†Ë∂ÖÈÅé„ÉÅ„Çß„ÉÉ„ÇØ**
                    - (Êû†Ë∂ÖÈÅé„Åó„Å¶„ÅÑ„Çã„Åã: „ÅØ„ÅÑ / „ÅÑ„ÅÑ„Åà)
                    - (Ë∂ÖÈÅé„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ„Ç™„Éº„Éê„Éº„Åó„ÅüÊñáÂ≠óÊï∞)

                    - **È†Ü‰ΩçÂ§âÂãï„Å´„Çà„ÇãÊû†Ë™øÊï¥„ÅÆÂøÖË¶ÅÊÄß**
                    - (Ë™øÊï¥„ÅåÂøÖË¶Å„Å™„Ç≥„É°„É≥„Éà„É™„Çπ„Éà)

                    - **‰øÆÊ≠£ÊèêÊ°à**
                    - (Êû†ÂÜÖ„Å´Âèé„ÇÅ„Çã„Åü„ÇÅ„ÅÆ‰øÆÊ≠£Âæå„ÅÆ„Ç≥„É°„É≥„Éà)

            **Standardized Notation (Ë°®Ë®ò„ÅÆÁµ±‰∏Ä):**
            - **Âü∫Ê∫ñ‰æ°È°ç„ÅÆÈ®∞ËêΩÁéá:**
                - When there are three decimal places, round off using the round-half-up method to the second decimal place. If there are only two decimal places, keep the value unchanged.

            - **ÔºÖÔºà„Éë„Éº„Çª„É≥„ÉàÔºâ„ÄÅ„Ç´„Çø„Ç´„Éä:**
                - **ÂçäËßí„Ç´„Çø„Ç´„Éä ‚Üí ÂÖ®Ëßí„Ç´„Çø„Ç´„Éä**Ôºà‰æã:„ÄåÔΩ∂ÔæÄÔΩ∂ÔæÖ„Äç‚Üí„Äå„Ç´„Çø„Ç´„Éä„ÄçÔºâ
                - **ÂçäËßíË®òÂè∑ ‚Üí ÂÖ®ËßíË®òÂè∑**Ôºà‰æã:„Äå%„Äç‚Üí„ÄåÔºÖ„Äç„ÄÅ„Äå@„Äç‚Üí„ÄåÔº†„ÄçÔºâ
                    Example:
                        input: % ÔΩ∂ÔæÄÔΩ∂ÔæÖ 
                        output: ÔºÖ „Ç´„Çø„Ç´„Éä

            - **Êï∞Â≠ó„ÄÅ„Ç¢„É´„Éï„Ç°„Éô„ÉÉ„Éà„ÄÅ„ÄåÔºã„Äç„Éª„ÄåÔºç„Äç:**
                - **ÂÖ®ËßíÊï∞Â≠ó„Éª„Ç¢„É´„Éï„Ç°„Éô„ÉÉ„Éà ‚Üí ÂçäËßíÊï∞Â≠ó„Éª„Ç¢„É´„Éï„Ç°„Éô„ÉÉ„Éà**Ôºà‰æã:„ÄåÔºëÔºíÔºì„Äç‚Üí„Äå123„Äç„ÄÅ„ÄåÔº°Ôº¢Ôº£„Äç‚Üí„ÄåABC„ÄçÔºâ
                - **ÂÖ®Ëßí„ÄåÔºã„Äç„ÄåÔºç„Äç ‚Üí ÂçäËßí„Äå+„Äç„Äå-„Äç**Ôºà‰æã:„ÄåÔºãÔºç„Äç‚Üí„Äå+-„Äç
                    Example:
                        input: ÔºëÔºíÔºì Ôº°Ôº¢Ôº£ ÔΩ±ÔΩ≤ÔΩ≥ ÔºãÔºç
                        output: 123 ABC „Ç¢„Ç§„Ç¶ +-

            - **„Çπ„Éö„Éº„Çπ„ÅØÂ§âÊõ¥„Å™„Åó**  

            - **„Äå‚Äª„Äç„ÅÆ‰ΩøÁî®:**
                - „Äå‚Äª„Äç„ÅØÂèØËÉΩ„Åß„ÅÇ„Çå„Å∞ **‰∏ä‰ªò„ÅçÊñáÂ≠óÔºàsuperscriptÔºâ‚Äª** „Å´Â§âÊèõ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
                - Âá∫ÂäõÂΩ¢Âºè„ÅÆ‰æã:
                - „ÄåÈáçË¶Å‰∫ãÈ†Ö‚Äª„Äç ‚Üí „ÄåÈáçË¶Å‰∫ãÈ†Ö<sup>‚Äª</sup>„Äç

            - **Ôºà„Ç´„ÉÉ„Ç≥Êõ∏„ÅçÔºâ:**
                - Parenthetical notes should only be included in their first occurrence in a comment.
                ‰ª•‰∏ã„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Å´„Åä„ÅÑ„Å¶„ÄÅ„Ç´„ÉÉ„Ç≥Êõ∏„ÅçÔºàbracket "Ôºà Ôºâ"Ôºâ„ÅåÈÅ©Âàá„Å´‰ΩøÁî®„Åï„Çå„Å¶„ÅÑ„Çã„Åã„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

                **Check point**
                    1. **„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÅØ„ÄÅ„Ç≥„É°„É≥„Éà„ÅÆÂàùÂá∫„ÅÆ„Åø„Å´Ë®òËºâ„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÔºü**
                    - Âêå„Åò„Ç´„ÉÉ„Ç≥Êõ∏„Åç„Åå2Âõû‰ª•‰∏äÁôªÂ†¥„Åó„Å¶„ÅÑ„Å™„ÅÑ„ÅãÔºü
                    - ÂàùÂá∫„Éö„Éº„Ç∏‰ª•Èôç„ÅÆ„Ç≥„É°„É≥„Éà„Å´„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÅåÈáçË§á„Åó„Å¶Ë®òËºâ„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑ„ÅãÔºü

                    2. **„Éá„Ç£„Çπ„ÇØ„É≠„ÅÆ„Éö„Éº„Ç∏Áï™Âè∑È†Ü„Å´Âæì„Å£„Å¶„É´„Éº„É´„ÇíÈÅ©Áî®**
                    - „Ç∑„Éº„Éà„ÅÆÈ†ÜÁï™„Åß„ÅØ„Å™„Åè„ÄÅÂÆüÈöõ„ÅÆ„Éö„Éº„Ç∏Áï™Âè∑„ÇíÂü∫Ê∫ñ„Å´„Åô„Çã„ÄÇ

                    3. **‰æãÂ§ñÂá¶ÁêÜ**
                    - „Äå‰∏ÄÈÉ®‰æãÂ§ñ„Éï„Ç°„É≥„Éâ„ÅÇ„Çä„Äç„Å®„ÅÇ„Çã„Åü„ÇÅ„ÄÅ‰æãÂ§ñÁöÑ„Å´„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÅåË§áÊï∞ÂõûÁôªÂ†¥„Åô„Çã„Ç±„Éº„Çπ„ÇíËÄÉÊÖÆ„Åô„Çã„ÄÇ
                    - ‰æãÂ§ñ„Å®„Åó„Å¶Ë™ç„ÇÅ„Çâ„Çå„Çã„Ç±„Éº„Çπ„ÇíÂà§Êñ≠„Åó„ÄÅÈÅ©Âàá„Å´ÊåáÊëò„ÄÇ

                    output Format:
                    - **„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÅÆÂàùÂá∫„É™„Çπ„Éà**Ôºà„Å©„ÅÆ„Éö„Éº„Ç∏„Å´ÊúÄÂàù„Å´ÁôªÂ†¥„Åó„Åü„ÅãÔºâ
                    - **ÈáçË§á„ÉÅ„Çß„ÉÉ„ÇØÁµêÊûú**Ôºà„Å©„ÅÆ„Éö„Éº„Ç∏„Åß‰∫åÈáçË®òËºâ„Åï„Çå„Å¶„ÅÑ„Çã„ÅãÔºâ
                    - **‰øÆÊ≠£ÊèêÊ°à**Ôºà„Å©„ÅÆ„Éö„Éº„Ç∏„ÅÆ„Ç´„ÉÉ„Ç≥Êõ∏„Åç„ÇíÂâäÈô§„Åô„Åπ„Åç„ÅãÔºâ
                    - **‰æãÂ§ñ„Éï„Ç°„É≥„Éâ„ÅåÈÅ©Áî®„Åï„Çå„ÇãÂ†¥Âêà„ÄÅË£úË∂≥ÊÉÖÂ†±**

            - **‰ºöË®àÊúüÈñì„ÅÆË°®Ë®ò:**
                - The use of "ÔΩû" is prohibited; always use "-".
                - Example: 6ÔΩû8ÊúàÊúüÔºà√óÔºâ ‚Üí 6-8ÊúàÊúüÔºà‚óãÔºâ
            - **Âπ¥Â∫¶Ë°®Ë®ò:**
                - Use four-digit notation for years.
                - Make modifications directly in this article and explain the reasons for the modifications.
                - Example: 22Âπ¥Ôºà√óÔºâ ‚Üí 2022Âπ¥Ôºà‚óãÔºâ
            - **„É¨„É≥„Ç∏„ÅÆË°®Ë®ò:**
                - Always append "%" when indicating a range.
                - Make modifications directly in this article and explain the reasons for the modifications.
                - Example: -1ÔΩû0.5%Ôºà√óÔºâ ‚Üí -1%ÔΩû0.5%Ôºà‚óãÔºâ
            - **ÊäïË≥áÁí∞Â¢É„ÅÆË®òËø∞:**
                **„ÄåÂÖàÊúà„ÅÆÊäïË≥áÁí∞Â¢É„Äç**„ÅÆÈÉ®ÂàÜ„Åß„ÄåÂÖàÊúàÊú´„Äç„ÅÆË®òËø∞„ÅåÂê´„Åæ„Çå„ÇãÂ†¥Âêà„ÄÅ„ÄåÂâçÊúàÊú´„Äç„Å´Â§âÊõ¥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
                Example:
                ‰øÆÊ≠£Ââç: ÂÖàÊúàÊú´„ÅÆÂ∏ÇÂ†¥ÂãïÂêë„ÇíÂàÜÊûê„Åô„Çã„Å®‚Ä¶
                ‰øÆÊ≠£Âæå: ÂâçÊúàÊú´„ÅÆÂ∏ÇÂ†¥ÂãïÂêë„ÇíÂàÜÊûê„Åô„Çã„Å®‚Ä¶

            - **ÈÄöË≤®Ë°®Ë®ò„ÅÆÁµ±‰∏Ä:**
                - Standardize currency notation across the document.
                - Êó•Êú¨ÂÜÜ„ÅØ„ÄåJPY„Äç„Åæ„Åü„ÅØ„Äå¬•„Äç„ÅßÁµ±‰∏Ä ÂÜÜ.
                - Exsample: 
                input: ¬•100 or JPY 100
                output: 100 ÂÜÜ

            **Preferred and Recommended Terminology (ÁΩÆ„ÅçÊèõ„Åà„ÅåÂøÖË¶Å„Å™Áî®Ë™û/Ë°®Áèæ):**
            - **Á¨¨1ÂõõÂçäÊúü:**
                - Ensure the period is clearly stated.
                - Example: 18Âπ¥Á¨¨4ÂõõÂçäÊúüÔºà√óÔºâ ‚Üí 2018Âπ¥10-12ÊúàÊúüÔºà‚óãÔºâ
            - **Á¥Ñ‚óãÔºÖÁ®ãÂ∫¶:**
                - Do not use "Á¥Ñ" (approximately) and "Á®ãÂ∫¶" (extent) together. Choose either one.
                - Example: Á¥Ñ‚óãÔºÖÁ®ãÂ∫¶Ôºà√óÔºâ ‚Üí Á¥Ñ‚óãÔºÖ or ‚óãÔºÖÁ®ãÂ∫¶Ôºà‚óãÔºâ
            - **Â§ßÊâã‰ºÅÊ•≠Ë°®Ë®ò„ÅÆÊòéÁ¢∫Âåñ**  
            - **„Äå‚óã‚óãÂ§ßÊâã„Äç** „ÅåÂê´„Åæ„Çå„ÇãÂ†¥Âêà„ÄÅÊñá‰∏≠„ÅÆ **‰ºöÁ§æÂêç„ÇíÊäΩÂá∫** „Åó„ÄÅ  
                **„ÄåÂ§ßÊâã‚óã‚óã‰ºöÁ§æ/‰ºÅÊ•≠„Äç** „ÅÆÂΩ¢Âºè„Å´‰øÆÊ≠£„Åô„Çã„ÄÇ  
            - **ÂÖ•Âäõ‰æã:**  
                - „ÄåÂ§ßÊâã„É°„Éº„Ç´„Éº/‰ºöÁ§æ/‰ºÅÊ•≠„Äç  
                - **Âá∫Âäõ:** „ÄåÂ§ßÊâã‰∏çÂãïÁî£‰ºöÁ§æ„ÄÅÂ§ßÊâãÂçäÂ∞é‰Ωì„É°„Éº„Ç´„Éº„Äç  
            - **The actual company name must be found and converted in the article


        **Special Rules:**
        1. **Do not modify proper nouns (e.g., names of people, places, or organizations) unless they are clearly misspelled.**
            -Exsample:
            „Éô„ÉÉ„Çª„É≥„ÉàÊ∞è: Since this is correct and not a misspelling, it will not be modified.
        2. **Remove unnecessary or redundant text instead of replacing it with other characters.**
            -Exsample:
            „É¶„Éº„É≠ÂúèÂüüÂÜÖ„ÅÆÊôØÊ∞óÂßî: Only the redundant character Âßî will be removed, and no additional characters like „ÅÆ will be added. The corrected text will be: „É¶„Éº„É≠ÂúèÂüüÂÜÖ„ÅÆÊôØÊ∞ó.
        3. **Preserve spaces between words in the original text unless they are at the beginning or end of the text.**
            -Example:
            input: Êúà„ÅÆ ÂâçÂçä„ÅØÁ±≥ÂõΩ„ÅÆ ÂÇµÂà∏Âà©Âõû„Çä„ÅÆ‰∏äÊòá „Å´„Å§„Çå„Å¶
            Output: Êúà„ÅÆ ÂâçÂçä„ÅØÁ±≥ÂõΩ„ÅÆ ÂÇµÂà∏Âà©Âõû„Çä„ÅÆ‰∏äÊòá „Å´„Å§„Çå„Å¶ (spaces between words are preserved).
        ---

        ### **Correction Rules:**
        1. **Only output the corrected_map dictionary. No explanations or extra text.**
        2. **Only incorrect words and their corrected versions should be included.**
        3. **Do not include full sentence corrections.**
        4. **Ensure the corrected_map output is in valid Python dictionary format.**
        5. **Return only the following structure:**
        
        **Output Format:**
        {{
            "incorrect1": "corrected1",
            "incorrect2": "corrected2"
        }}

        """

    
    # ChatCompletion Call
    response = openai.ChatCompletion.create(
        deployment_id=deployment_id,  # Deploy Name
        messages=[
            {"role": "system", "content": "You are a professional Japanese text proofreading assistant."},
            {"role": "user", "content": prompt_result}
        ],
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  #seed
    )

    try:
        answer = response['choices'][0]['message']['content']
        corrected_map = parse_gpt_response(answer)
        
        full_corrected = prompt
        for k, v in corrected_map.items():
            full_corrected = full_corrected.replace(k, v)

        dynamic_corrections = detect_corrections(prompt, full_corrected)
        corrected_map.update(dynamic_corrections)

        return {k: v for k, v in corrected_map.items() if k and v and k != v}

    except Exception as e:
        print(f"req error: {e}")
        return {}

def correct_text_box_in_excel(input_bytes,corrected_map):
    # 1)  in-memory zip
    in_memory_zip = zipfile.ZipFile(io.BytesIO(input_bytes), 'r')
    
    # BytesIO
    output_buffer = io.BytesIO()
    new_zip = zipfile.ZipFile(output_buffer, 'w', zipfile.ZIP_DEFLATED)

    for item in in_memory_zip.infolist():
        file_data = in_memory_zip.read(item.filename)

        # 3) drawingN.xml
        if item.filename.startswith("xl/drawings/drawing") and item.filename.endswith(".xml"):
            try:
                tree = ET.fromstring(file_data)
                ns = {'a': 'http://schemas.openxmlformats.org/drawingml/2006/main'}

                # 4) find <a:t> tag 
                for t_element in tree.findall(".//a:t", ns):
                    original_text = t_element.text
                #----------------------------------------------------------------
                    # if original_text:  # None 
                    #     original_text_gpt = gpt_correct_text(original_text)

                    #     if original_text_gpt and original_text_gpt.strip() in corrected_map:
                    #         t_element.text = corrected_map[original_text_gpt.strip()]
                #----------------------------------------------------------------
                    # resultMap = gpt_correct_text(original_text)

                    if original_text in corrected_map:
                        t_element.text = corrected_map[original_text]
                #----------------------------------------------------------------

                file_data = ET.tostring(tree, encoding='utf-8', standalone=False)
                
            except Exception as e:
                print(f"Warning: Parsing {item.filename} failed - {e}")

        new_zip.writestr(item, file_data)

    in_memory_zip.close()
    new_zip.close()
    output_buffer.seek(0)
    return output_buffer.getvalue()

# Excel read -for debug
@app.route("/api/excel_upload", methods=["POST"])
def excel_upload():
    file = request.files["file"]  # XLSX
    original_bytes = file.read()

    corrected_map = {
        "Âú∞ÊîøÂ≠¶„É™„Çπ„ÇØ": "Âú∞ÊîøÂ≠¶ÁöÑ„É™„Çπ„ÇØ"
    }

    # 2) ÏàòÏ†ï
    modified_bytes = correct_text_box_in_excel(original_bytes, corrected_map)

    # 3) return xlsx
    return send_file(
        io.BytesIO(modified_bytes),
        mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        as_attachment=True,
        download_name="annotated.xlsx"
    )

# T-STAR„Éò„É´„Éó API --for debug
@app.route('/api/prompt_upload', methods=['POST'])
def prompt_upload():
    try:
        data = request.json
        prompt = data.get("input", "")
        original_text = data.get("original_text", "")

        if not prompt:
            return jsonify({"success": False, "error": "No input provided"}), 400
        
        prompt_result = f"""
        Please analyze the provided {original_text} and generate results based on the specified {prompt}.

        **Requirements**:
        1. **Extract relevant information**:
        - Extract only the information that directly answers the {prompt}.
        2. **Process the content**:
        - Process the extracted information to provide a clear and concise response.
        3. **Output in Japanese**:
        - Provide the results in Japanese, strictly based on the {prompt}.
        - Do not include any unrelated information or additional explanations.

        **Output**:
        - The output must be accurate, concise, and fully aligned with the {prompt}.
        - Only provide the response in Japanese.

        **Example**:
        - If the {prompt} is "Â£≤‰∏äÊàêÈï∑Áéá„ÇíÊïô„Åà„Å¶„Åè„Å†„Åï„ÅÑ", the output should be:
        "2023Âπ¥„ÅÆÂ£≤‰∏äÊàêÈï∑Áéá„ÅØ15ÔºÖ„Åß„Åô„ÄÇ"

        Ensure the output is accurate, concise, and aligned with the given {prompt} requirements.
        """

        # ChatCompletion Call
        response = openai.ChatCompletion.create(
            deployment_id=deployment_id,  # Deploy Name
            messages=[
                {"role": "system", "content": "You are a professional Japanese text proofreading assistant."},
                {"role": "user", "content": prompt_result}
            ],
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE,
            seed=SEED  # seed
        )
        answer = response['choices'][0]['message']['content'].strip()
        re_answer = remove_code_blocks(answer)

        # return JSON
        return jsonify({
            "success": True,
            "original_text": prompt,
            "corrected_text": re_answer,
            # "corrections": corrections
        })

    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

#------Auto app update API

@app.route('/api/auto_save_cosmos', methods=['POST'])
def auto_save_cosmos():
    try:
        data = request.json
        response_data = data['result']
        link_url = data['link']
        container_name = data['containerName']
        file_name_decoding = data['fileName']

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        container = get_db_connection(container_name)

        item = {
            'id': file_name,
            'fileName': file_name,
            'result': response_data,
            'link': link_url,
            'updateTime': datetime.utcnow().isoformat(),
        }

        existing_item = list(container.query_items(
            query="SELECT * FROM c WHERE c.id = @id",
            parameters=[{"name": "@id", "value": file_name}],
            enable_cross_partition_query=True
        ))

        if not existing_item:
            container.create_item(body=item)
            logging.info(f"‚úÖ Cosmos DB Update Success: {file_name}")
        else:
            existing_id = existing_item[0]['id']
            item['id'] = existing_id
            container.replace_item(item=existing_item[0], body=item)
            logging.info(f"üîÑ Cosmos DB update success: {file_name}")

        return jsonify({"success": True, "message": "Data Update Success"}), 200

    except CosmosHttpResponseError as e:
        logging.error(f"‚ùå Cosmos DB Save error: {e}")
        return jsonify({"success": False, "message": str(e)}), 500
    except Exception as e:
        logging.error(f"‚ùå API Save error: {e}")
        return jsonify({"success": False, "message": str(e)}), 500
    
#----auto app save to blob
@app.route('/api/auto_save_blob', methods=['POST'])
def auto_save_blob():
    try:
        if 'file' not in request.files:
            return jsonify({"success": False, "message": "no find file."}), 400

        file = request.files['file']
        blob_name = file.filename
        
        container_client = get_storage_container()

        blob_client = container_client.get_blob_client(blob_name)

        blob_client.upload_blob(file, overwrite=True)

        file_url = blob_client.url
        logging.info(f"‚úÖ Azure Blob Storage Update Success: {blob_name}")

        return jsonify({"success": True, "url": file_url}), 200

    except Exception as e:
        logging.error(f"‚ùå Azure Blob Storage update error: {e}")
        return jsonify({"success": False, "message": str(e)}), 500

#----auto app save log 
@app.route('/api/auto_save_log_cosmos', methods=['POST','PUT'])
def auto_save_log_cosmos():
    """log Cosmos DB Save to API"""
    try:
        container = get_db_connection(APPLOG_CONTAINER_NAME)

        log_data = request.json
        log_by_date = log_data.get("logs", {})

        # ‚úÖ Cosmos DB Save
        for log_id, logs in log_by_date.items():
            existing_logs = list(container.query_items(
                query="SELECT * FROM c WHERE c.id = @log_id",
                parameters=[{"name": "@log_id", "value": log_id}],
                enable_cross_partition_query=True
            ))

            if existing_logs:
                existing_log = existing_logs[0]
                existing_log["logEntries"].extend(logs)
                existing_log["timestamp"] = datetime.utcnow().isoformat(), 
                #update
                container.replace_item(item=existing_log["id"], body=existing_log)
                logging.info(f"üîÑ SUCCESS: Update Log Success: {log_id}")
            else:
                log_data = {
                    "id": log_id,  # YYYYMMDD format ID
                    "logEntries": logs,
                    "timestamp": datetime.utcnow().isoformat(),
                }
                #create
                container.create_item(body=log_data)
                logging.info(f"‚úÖ SUCCESS: Save to Log Success: {log_id}")

        return jsonify({"code": 200, "message": "Logs saved successfully."}), 200

    except Exception as e:
        logging.error(f"‚ùå ERROR: Save Log Error: {e}")
        return jsonify({"code": 500, "message": "Error saving logs."}), 500


# integeration ruru

@app.route('/api/integeration_ruru_cosmos', methods=['POST'])
def integeration_ruru_cosmos():
    try:
        data = request.json

        base_month = data['Base_Month']
        fundType = data['fundType']
        fcode = data['Fcode']
        org_sheet_name = data['Org_SheetName']
        org_title = data['Org_Title']
        org_text = data['Org_Text']
        org_type = data['Org_Type']
        target_sheet_name = data['Target_SheetName']
        target_text = data['Target_Text']
        target_type = data['Target_Type']
        target_condition = data['Target_Condition']
        result = data['result']
        Target_Consult = data['Target_Consult']
        flag = data['flag']
        id = data['id']
        No = data['No']

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        query = f"SELECT * FROM c WHERE c.Fcode = '{data['Fcode']}' AND c.Base_Month = '{data['Base_Month']}' AND c.fundType = '{data['fundType']}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        common_item = {
            "id": id,
            "No": No,
            "fundType": fundType,
            "Base_Month": base_month,
            "Fcode": fcode,
            "Org_SheetName": org_sheet_name,
            "Org_Title": org_title,
            "Org_Text": org_text,
            "Org_Type": org_type,
            "Target_SheetName": target_sheet_name,
            "Target_Text": target_text,
            "flag": flag,
            "Target_Type": target_type,
            "Target_Condition": target_condition,
            "updateTime": datetime.utcnow().isoformat(),  
        }

        if flag == 'close':
            common_item["result"] = result

        elif flag == 'open':
            common_item["Target_Consult"] = Target_Consult

        item = common_item

        if items:
            # container.upsert_item(item)
            items[0].update(item)
            container.upsert_item(items[0])
            logging.info("‚úÖ Data updated in Cosmos DB successfully.")
            return jsonify({"success": True, "message": "Data updated successfully."}), 200
        else:
            container.upsert_item(item)
            logging.info("‚úÖ Data inserted into Cosmos DB successfully.")
            return jsonify({"success": True, "message": "Data inserted successfully."}), 200

        # if items:
        #     item["id"] = items[0]["id"]
        #     container.replace_item(item=items[0], body=item)
        #     logging.info("‚úÖ Data updated in Cosmos DB successfully.")
        #     return jsonify({"success": True, "message": "Data updated successfully."}), 200
        # else:
        #     container.create_item(body=item)
        #     logging.info("‚úÖ Data inserted into Cosmos DB successfully.")
        #     return jsonify({"success": True, "message": "Data inserted successfully."}), 200

    except Exception as e:
        logging.error(f"‚ùå Cosmos DB save error: {e}")
        return jsonify({"success": False, "message": str(e)}), 500

@app.route('/api/integeration_ruru_cosmos', methods=['GET'])
def get_integeration_ruru_cosmos():
    # Cosmos DB ËøûÊé•
    container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

    flag = request.args.get("flag")
    base_month = request.args.get("Base_Month")

    query = "SELECT * FROM c"
    parameters = []

    if flag and base_month:
        query += " WHERE c.flag = @flag AND c.Base_Month = @base_month"
        parameters = [
            {"name": "@flag", "value": flag},
            {"name": "@base_month", "value": base_month}
        ]
    elif flag:
        query += " WHERE c.flag = @flag"
        parameters = [{"name": "@flag", "value": flag}]
    elif base_month:
        query += " WHERE c.Base_Month = @base_month"
        parameters = [{"name": "@base_month", "value": base_month}]

    users = list(container.query_items(
        query=query,
        parameters=parameters,
        enable_cross_partition_query=True
    ))

    response = {
        "code": 200,
        "data": users
    }
    return jsonify(response), 200

# common ruru add logic
def common_ruru_text(text):
    corrections = []
    seen = set()

    # ‚ë† „Éï„Ç°„É≥„ÉâÔºã„Éô„É≥„ÉÅ„Éû„Éº„ÇØ‰∏°Êñπ ‚Üí Ë∂ÖÈÅéÂèéÁõä 
    pattern_excess = (
        r"Âü∫Ê∫ñ‰æ°È°ç„ÅÆÈ®∞ËêΩÁéá„ÅØ([+-]?\d+(\.\d+)?)ÔºÖ„ÄÅ"
        r"„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆÈ®∞ËêΩÁéá„ÅØ([+-]?\d+(\.\d+)?)ÔºÖ"
    )
    match = re.search(pattern_excess, text)

    if match:
        fund_return = float(match.group(1))
        benchmark_return = float(match.group(3))

        # round 2
        calculated_excess = round(fund_return - benchmark_return, 2)

        result = {
            "È®∞ËêΩÁéá": fund_return,
            "„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆÈ®∞ËêΩÁéá": benchmark_return,
            "Ë∂ÖÈÅéÂèéÁõäÔºà„Éù„Ç§„É≥„ÉàÂ∑ÆÔºâ": calculated_excess,
            "reason": "Âü∫Ê∫ñ‰æ°È°ç„Å®„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆÂ∑Æ„ÇíË®àÁÆó„Åó„Åæ„Åó„Åü"
        }

        key = str(result)  # dict set duipli
        if key not in seen:
            seen.add(key)
            corrections.append(result)


    else:
        # pass
        # ‚ë° ÂÄãÂà•„Éë„Çø„Éº„É≥„ÉÅ„Çß„ÉÉ„ÇØ
        patterns = {
            # "fund_only": r"ÊúàÈñì„ÅÆÂü∫Ê∫ñ‰æ°È°ç„ÅÆÈ®∞ËêΩÁéá„ÅØ[+-]?\d+(\.\d+)?ÔºÖ",
            "benchmark_only": r"„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅÆÈ®∞ËêΩÁéá„ÅØ[+-]?\d+(\.\d+)?ÔºÖ",
            "course_multi": r"([Ôº°-Ôº∫A-Z„ÅÅ-„Çì„Ç°-„É≥‰∏Ä-Èæ•]+„Ç≥„Éº„Çπ)„Åå[+-]?\d+(\.\d+)?ÔºÖ",
            "hedge": r"(ÁÇ∫Êõø„Éò„ÉÉ„Ç∏„ÅÇ„Çä|ÁÇ∫Êõø„Éò„ÉÉ„Ç∏„Å™„Åó)„ÅØ[+-]?\d+(\.\d+)?ÔºÖ",
            "currency_type": r"(ÂÜÜÊäïË≥áÂûã|Á±≥„Éâ„É´ÊäïË≥áÂûã)„ÅÆÊúàÈñìÈ®∞ËêΩÁéá„ÅØ[+-]?\d+(\.\d+)?ÔºÖ",
            # "global_type": r"„Äê[^„Äë]+„Äë[+-]?\d+(\.\d+)?ÔºÖ",
            # "point_value": r"[+-]?\d+(\.\d+)?„Éù„Ç§„É≥„Éà",
            "select_course": r"ÈÄöË≤®„Çª„É¨„ÇØ„Éà„Ç≥„Éº„Çπ.*?(‰∏äÊòá|‰∏ãËêΩ)",
            "fund_updown": r"Âü∫Ê∫ñ‰æ°È°çÔºàÂàÜÈÖçÈáëÂÜçÊäïË≥áÔºâ„ÅØ.*?(‰∏äÊòá|‰∏ãËêΩ)"
        }

        # „Éï„Ç°„É≥„ÉâÂûã: ÂΩì„Éï„Ç°„É≥„Éâ + „Éô„É≥„ÉÅ„Éû„Éº„ÇØ
        pattern_fund = r"ÂΩì„Éï„Ç°„É≥„Éâ„ÅÆÊúàÈñìÈ®∞ËêΩÁéá.*?„Éô„É≥„ÉÅ„Éû„Éº„ÇØ[^„ÄÇ]*?„Éù„Ç§„É≥„Éà[^„ÄÇ]"
        # Â∏ÇÊ≥ÅÂûã: Ê†™ÂºèÂ∏ÇÂ†¥ + TOPIX
        pattern_market = r"TOPIXÔºàÊù±Ë®ºÊ†™‰æ°ÊåáÊï∞Ôºâ[^„ÄÇ]*"

        # --- „Éï„Ç°„É≥„ÉâÂûã ---
        fund_sentences = re.findall(pattern_fund, text)
        for sentence in fund_sentences:
            for m in re.finditer(r"[^„ÄÅ„ÄÇ]+?(ÔºÖ|„Éù„Ç§„É≥„Éà)", sentence):
                extracted = m.group(0).strip()
                if extracted not in seen:
                    seen.add(extracted)
                    corrections.append({"extract": extracted})

        # --- Â∏ÇÊ≥ÅÂûã  ---
        market_sentences = re.findall(pattern_market, text)
        for sentence in market_sentences:
            for m in re.finditer(r"[^„ÄÅ„ÄÇ]+?(ÔºÖ|„Éù„Ç§„É≥„Éà)", sentence):
                extracted = m.group(0).strip()
                if extracted not in seen:
                    seen.add(extracted)
                    corrections.append({"extract": extracted})


        # „Åù„ÅÆ‰ªñ„ÅÆ„Éë„Çø„Éº„É≥‰∏ÄÊã¨ÊäΩÂá∫
        for name, pat in patterns.items():
            for m in re.finditer(pat, text):
                extracted_other = m.group(0)
                if extracted_other not in seen:
                    seen.add(extracted_other)
                    corrections.append({"extract": extracted_other})

    return corrections

# --- common ruru api
@app.route('/api/common_ruru', methods=['POST'])
def common_ruru():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("‚úÖ Token Update SUCCESS")
        
        data = request.json
        input_list = data.get("input", "")
        pdf_base64 = data.get("pdf_bytes", "")
        pageNumber = data.get('pageNumber',0)

        if not input_list:
            return jsonify({"success": False, "error": "No input provided"}), 400
        
        corrections = []
        if isinstance(input_list, list):
            for idx, t in enumerate(input_list, start=1):
                part_result = common_ruru_text(t) 
                for pr in part_result:
                    corrections.append({
                        "page": pageNumber,
                        "original_text": pr.get("extract", t),
                        "comment": f"{pr.get("extract", t)} ‚Üí ",
                        "reason_type": pr.get("reason", "Êï¥ÂêàÊÄß"),
                        "check_point": pr.get("extract", t),
                        "locations": [], 
                        "intgr": True
                    })
        else:
            part_result = common_ruru_text(input_list)
            for pr in part_result:
                corrections.append({
                    "page": pageNumber,
                    "original_text": input_list,
                    "comment": f"{input_list} ‚Üí {pr.get('extract', pr.get('Ë∂ÖÈÅéÂèéÁõäÔºà„Éù„Ç§„É≥„ÉàÂ∑ÆÔºâ', ''))}",
                    "reason_type": pr.get("reason", "Êï¥ÂêàÊÄß"),
                    "check_point": pr.get("extract", input_list),
                    "locations": [],
                    "intgr": True
                })
        
        try:
            pdf_bytes = base64.b64decode(pdf_base64)
            find_locations_in_pdf(pdf_bytes, corrections)

            return jsonify({
                "success": True,
                "corrections": corrections,
            })

        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500
        
        
    except Exception as e:
        # exception return JSON 
        return jsonify({"success": False, "error": str(e)}), 500

# --- ruru test api

@app.route('/api/ruru_search_db', methods=['POST'])
def ruru_search_db():
    try:
        data = request.json

        fcode = data.get('fcode')
        base_month = data.get('Base_Month')
        fund_type = data.get('fundType', 'private')

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        query = f"SELECT * FROM c WHERE c.Fcode = '{fcode}' AND c.Base_Month = '{base_month}' AND c.fundType = '{fund_type}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        if items:
            # results = [{"id": item["id"], "result": item["result"],"Org_Text":item["Org_Text"],"Org_Type":item["Org_Type"],"Target_Condition":item["Target_Condition"]} for item in items]
            results = [item if item.get("flag") else {"id": item["id"], "result": item["result"],"Org_Text":item["Org_Text"],"Org_Type":item["Org_Type"],"Target_Condition":item["Target_Condition"]} for item in items]
            return jsonify({"success": True, "data": results}), 200
        else:
            return jsonify({"success": False, "message": "No matching data found in DB."}), 200

    except Exception as e:
        logging.error(f"‚ùå Error occurred while searching DB: {e}")
        return jsonify({"success": False, "message": str(e)}), 500

@app.route('/api/refer_operate', methods=['GET'])
def get_rule():
    try:
        data = request.args
        flag = data.get('flag', "")
        fund_type = data.get('fundType', 'private')

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        query = f"SELECT * FROM c WHERE c.flag = '{flag}' AND c.fundType = '{fund_type}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        if items:
            items_map = list(map(lambda y: dict(filter(lambda x: not x[0].startswith("_"), y.items())), items))
            return jsonify({"success": True, "data": items_map}), 200
        else:
            return jsonify({"success": False, "message": "No matching data found in DB."}), 404

    except Exception as e:
        logging.error(f"‚ùå Error occurred while searching DB: {e}")
        return jsonify({"success": False, "message": str(e)}), 500
    

@app.route('/api/open_cosmos_data', methods=['POST'])
def get_open_data():
    data = request.json
    f_code = data.get("f_code", "")
    flag = data.get("flag", "open")
    base_month = data.get("base_month", "M2411")
    query = f"SELECT * FROM c WHERE c.flag = '{flag}' AND c.Base_Month = '{base_month}' and c.Fcode = '{f_code}'"
    items = list(integeration_container.query_items(query=query, enable_cross_partition_query=True))

    if items:
        return jsonify({"success": True, "data": items}), 200
    else:
        return jsonify({"success": False, "message": "No matching data found in DB."}), 200

@app.route('/api/save_cosmos_data', methods=['POST'])
def save_open_data():
    data = request.json
    item = data.get("item")

    if item:
        integeration_container.upsert_item(item)
        return jsonify({"success": True}), 200
    else:
        return jsonify({"success": False}), 200


@app.route('/api/refer_operate', methods=['POST'])
def insert_rule():
    try:
        data = request.json

        base_month = data.get('Base_Month', '')
        fund_type = data.get('fundType', '')
        fcode = data.get('Fcode', '')
        org_sheet_name = data.get('Org_SheetName', '')
        org_title = data.get('Org_Title', '')
        org_text = data.get('Org_Text', '')
        org_type = data.get('Org_Type', '')
        target_sheet_name = data.get('Target_SheetName', '')
        target_title = data.get('Target_Title', '')
        target_text = data.get('Target_Text', '')
        target_type = data.get('Target_Type', '')
        target_condition = data.get('Target_Condition', '')
        target_consult = data.get('Target_Consult', '')
        id = str(uuid.uuid4())

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        item = {
            "id": id,
            "No": id,
            "fundType": fund_type,
            "Base_Month": base_month,
            "Fcode": fcode,
            "Org_SheetName": org_sheet_name,
            "Org_Title": org_title,
            "Org_Text": org_text,
            "Org_Type": org_type,
            "Target_SheetName": target_sheet_name,
            "Target_Text": target_text,
            "Target_Title": target_title,
            "Target_Type": target_type,
            "Target_Condition": target_condition,
            "Target_Consult": target_consult,
            "flag": "open",
            "updateTime": datetime.now().isoformat()
        }
        container.upsert_item(item)
        return jsonify({"success": True}), 200
    except Exception as e:
        return jsonify({"success": False, "message": str(e)}), 500

async def get_original(input_data, org_text, file_name="", target_text=""):
    dt = [
        "ÊñáÁ´†„Åã„ÇâÂéüÊñá„Å´È°û‰ºº„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
        "Âá∫Âäõ„ÅØ‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„Åß„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô:",
        "- [{'target': '[ÊäΩÂá∫„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà:]'}]",
        "- È°û‰ºº„Åó„Åü„ÇÇ„ÅÆ„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅÁ©∫„ÅÆÊñáÂ≠óÂàó„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        "ÊäΩÂá∫„É´„Éº„É´Ôºö",
        "- ÂéüÊñá„Å®„ÄåË™ûÈ†Ü„ÉªÊñáÊßãÈÄ†„ÉªÊñáÊ≥ï„Éë„Çø„Éº„É≥„Äç„ÅåÈ´ò„Åè‰∏ÄËá¥„Åó„Å¶„ÅÑ„ÇãÊñá„ÅØ„ÄÅ„Åü„Å®„ÅàË™ûÂè•ÔºàÂêçË©û„ÇÑ‰∏ªË™û„Å™„Å©Ôºâ„Åå‰∏ÄÈÉ®ÈÅï„Å£„Å¶„ÅÑ„Å¶„ÇÇ„ÄÅ**ÂøÖ„ÅöÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ**„ÄÇ",
        "- È°û‰ººÂ∫¶„Åå50%‰ª•‰∏ä„ÅÆÊñá„Çí„Åô„Åπ„Å¶ÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        "- **ÂéüÊñá„Å®ÊßãÈÄ†„Åå‰ºº„Å¶„ÅÑ„ÇãÊñá„ÇÇË¶ãËêΩ„Å®„Åï„Åö„Å´ÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ**",
        "- **ÊäΩÂá∫„Åô„ÇãÊñá„ÅåÂéüÊñá„ÅÆË®Ä„ÅÑÊèõ„Åà„ÉªÊñáÂûã„Éë„Çø„Éº„É≥„ÅÆÂÖ±ÈÄöÊÄß„Åå„ÅÇ„ÇãÂ†¥Âêà„ÄÅ„Ç≠„Éº„ÉØ„Éº„Éâ„ÅÆÈÅï„ÅÑ„Åå„ÅÇ„Å£„Å¶„ÇÇÂØæË±°„Å´Âê´„ÇÅ„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ**",
        "- ÊúÄ„ÇÇÈ°û‰ºº„Åó„Åü‰∏ÄÊñá„Å†„Åë„ÇíËøî„Åï„Åö„ÄÅÊù°‰ª∂„ÇíÊ∫Ä„Åü„Åô„Åô„Åπ„Å¶„ÅÆÊñá„ÇíÂøÖ„ÅöÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",

        f"ÂéüÊñá:{org_text}\nÊñáÁ´†:{input_data}"
    ]
    input_data = "\n".join(dt)

    question = [
        {"role": "system", "content": "„ÅÇ„Å™„Åü„ÅØ„ÉÜ„Ç≠„Çπ„ÉàÊäΩÂá∫„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô"},
        {"role": "user", "content": input_data},
        {"role": "user", "content": input_data}
    ]
    response = await openai.ChatCompletion.acreate(
        deployment_id=deployment_id,  # Deploy Name
        messages=question,
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  # seed
    )
    answer = response['choices'][0]['message']['content'].strip().replace("`", "").replace("json", "", 1)

    src_score = 0.5
    src_content = ""
    if answer:
        parsed_data = ast.literal_eval(answer)
        for once in parsed_data:
            similar_content = once.get("target")
            if file_name.startswith("180015"):
                if org_text[:4] in similar_content[:6]:
                    src_content = similar_content
                    break
            elif re.search("180332|180358|180359|180360|180344|180345", file_name):
                if org_text[:5] in similar_content[:10]:
                    if target_text in ["„Çª„ÇØ„Çø„ÉºÂà•ÈÖçÂàÜ", "„Çª„ÇØ„Çø„ÉºÂà•ÂØÑ‰∏éÂ∫¶"]:
                        re_content = re.search("(„Çª„ÇØ„Çø„ÉºÂà•.*)ÂÄãÂà•„ÅÆÂØÑ‰∏éÂ∫¶", similar_content, re.DOTALL)
                        if re_content:
                            src_content = re_content.groups(1)[0]
                            break
                    elif target_text in ["ÂØÑ‰∏éÂ∫¶", "ÂØÑ‰∏éÂ∫¶Ôºû„Äê‰∏ä‰Ωç5ÈäòÊüÑ„Äë"]:
                        re_content = re.search("ÂÄãÂà•„ÅÆÂØÑ‰∏éÂ∫¶.*", similar_content, re.DOTALL)
                        if re_content:
                            src_content = re_content.group()
                            break
            elif re.search("180001|180002|180003|180004|180015|180021|180022|180023", file_name):
                if org_text[1: 5] in similar_content[: 10]:
                    src_content = similar_content
                    break
            elif re.search("140672", file_name):
                if org_text[1: 6] in similar_content[: 10]:
                    src_content = similar_content
                    break
            if similar_content:
                score = SequenceMatcher(None, org_text, similar_content).ratio()
                if score > src_score:
                    src_score = score
                    src_content = similar_content
    return src_content, answer


LOCAL_LINK = "local_link"
@app.route('/api/getaths', methods=['GET'])
def get_local_link():
    try:
        container = get_db_connection(LOCAL_LINK)
        log_data = list(container.query_items(
            query=f"SELECT * FROM c",
            enable_cross_partition_query=True
        ))
        log_map = list(map(lambda y: dict(filter(lambda x: x[1] and not x[0].startswith("_"), y.items())), log_data))
        return jsonify({"success": True, "data": log_map}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 404


@app.route('/api/saveaths', methods=['POST'])
def save_local_link():
    try:
        data = request.json
        commonComment = data.get("commonComment")
        individualCheckPath = data.get("individualCheckPath")
        individualComment = data.get("individualComment")
        individualExcelPath = data.get("individualExcelPath")
        individualPdfPath = data.get("individualPdfPath")
        meigaramaster = data.get("meigaramaster")
        reportData = data.get("reportData")
        simu = data.get("simu")
        resultngPath = data.get("resultngPath")
        resultokPath = data.get("resultokPath")
        fund_type = data.get("fund_type")
        container = get_db_connection(LOCAL_LINK)
        link_data = list(container.query_items(
           query=f"SELECT * FROM c WHERE c.fund_type='{fund_type}'",
            enable_cross_partition_query=True
        ))
        update_data = dict(
                fund_type=fund_type,
                commonComment=commonComment,
                individualCheckPath=individualCheckPath,
                individualComment=individualComment,
                individualExcelPath=individualExcelPath,
                individualPdfPath=individualPdfPath,
                meigaramaster=meigaramaster,
                reportData=reportData,
                simu=simu,
                resultngPath=resultngPath,
                resultokPath=resultokPath
        )

        if not link_data:
            update_data.update(id=str(uuid.uuid4()))
            container.upsert_item(update_data)
        else:
            effective_data = dict(filter(lambda x: x[1] is not None, update_data.items()))
            link_data[0].update(effective_data)
            container.upsert_item(link_data[0])
        return jsonify({"success": True}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 404


@app.route('/api/log_operate')
def get_log():
    try:
        # 1)  page=1, size=15
        page = int(request.args.get('page', 1))
        size = int(request.args.get('size', 15))
        file_name = request.args.get('fileName', "")
        log_controller = get_db_connection(LOG_RECORD_CONTAINER_NAME)
        offset = (page - 1) * size
        if file_name:
            file_query = f"SELECT * FROM c WHERE CONTAINS(c.fileName, '{file_name}') OFFSET {offset} LIMIT {size}"
            total_file = list(log_controller.query_items(
                query=file_query,
                enable_cross_partition_query=True
            ))
            name_count = f"SELECT VALUE COUNT(1) FROM c WHERE CONTAINS(c.fileName, '{file_name}')"
            count_result = list(log_controller.query_items(
                query=name_count,
                enable_cross_partition_query=True
            ))[0]
            return jsonify({
                "success": True,
                "data": total_file,
                "total": count_result

            }), 200
        count_query = "SELECT VALUE COUNT(1) FROM c"
        total_count = list(log_controller.query_items(
            query=count_query,
            enable_cross_partition_query=True
        ))[0]

        query = f"""
                SELECT * FROM c
                ORDER BY c.created_at DESC
                OFFSET {offset} LIMIT {size}
                """
        log_data = list(log_controller.query_items(
            query=query,
            enable_cross_partition_query=True
        ))

        log_map = list(map(lambda y: dict(filter(lambda x: x[1] and not x[0].startswith("_"), y.items())), log_data))

        return jsonify({
            "success": True,
            "data": log_map,
            "total": total_count
        }), 200
    
        # return jsonify({"success": True, "data": log_map}), 200

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/check_file', methods=['POST'])
def check_file_statue():
    try:
        data = request.json
        file_name = data.get("file_name")
        fund_type = data.get("fund_type")
        comment_type = data.get("comment_type")
        upload_type = data.get("upload_type", "")
        container = get_db_connection(FILE_MONITOR_ITEM)
        file_data = list(container.query_items(
            query=f"SELECT * FROM u WHERE u.file_name = '{file_name}'",
            enable_cross_partition_query=True
        ))
        if file_data:
            file_info = file_data[0]
            if file_info.get("flag") == "success":
                pdf_name = re.sub(r"\.(xlsx|xlsm|xls|docx|doc)", ".pdf", file_name)
                result = {
                    "corrections": file_info.get("corrections", [])
                }
                is_url = file_info.get("link", "")
                link_url = re.sub(r"\.(xlsx|xlsm|xls|docx|doc)", ".pdf", is_url)
                save_to_cosmos(pdf_name, result, link_url, fund_type, comment_type=comment_type, upload_type=upload_type)
                return jsonify({"success": True}), 200
        return jsonify({"success": False}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 303


@app.route('/api/file_status_update', methods=['POST'])
def file_update():
    try:
        data = request.json
        id = data.get("id", "")
        flag = data.get("flag", "")
        file_name = data.get("file_name", "")
        link_url = data.get("link", "")
        error_space = data.get("error_space", "")
        if id and flag and file_name:
            container = get_db_connection(FILE_MONITOR_ITEM)
            corrections = list(map(lambda x: dict(
                check_point=x.get("original_text"),
                original_text=x.get("original_text"),
                comment=x.get("original_text"),
                intgr=False,
                page=0,
                reason_type=x.get("reason_type"),
                locations=[{"x0": 0, "x1": 0, "y0": 0, "y1": 0}]
                ), error_space))
            container.upsert_item({"id": id, "flag": flag, "file_name": file_name, "link": link_url, "corrections": corrections})
            return jsonify({"success": True}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 303


@app.route('/api/file_status_search', methods=['GET'])
def file_search():
    try:
        container = get_db_connection(FILE_MONITOR_ITEM)
        file_data = list(container.query_items(
            query=f"SELECT * FROM u WHERE u.flag = 'wait'",
            enable_cross_partition_query=True
        ))
        if file_data:
            results = []
            for file_info in file_data:
                results.append(file_info)
            return jsonify({"success": True, "data": results}), 200
        else:
            return jsonify({"success": False, "data": []}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 303


@app.route('/api/ruru_ask_gpt_enhance', methods=['POST'])
def integrate_enhance():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("‚úÖ Token Update Done")

        data = request.json
        _content = data.get("input", "")
        condition = data.get("Target_Condition", "")
        category = data.get("Org_Type", "")
        consult = data.get("Target_Consult", "")
        base_month = data.get("Base_month", "")
        pageNumber = data.get('pageNumber',0)
        file_name = data.get("file_name", "")
        target_text = data.get("Target_Text", "")

        org_text = data.get("Org_Text", "")
        __answer = ""

        if org_text == "„É™„Çπ„ÇØÊäëÂà∂Êà¶Áï•„ÅÆÁä∂Ê≥Å":
            if "„É™„Çπ„ÇØÊäëÂà∂Êà¶Áï•„ÅÆÁä∂Ê≥Å" in _content:
                return jsonify({
                    "success": True,
                    "corrections": [{
                        "page": pageNumber,
                        "original_text": "„É™„Çπ„ÇØÊäëÂà∂Êà¶Áï•„ÅÆÁä∂Ê≥Å",
                        "check_point": "„É™„Çπ„ÇØÊäëÂà∂Êà¶Áï•„ÅÆÁä∂Ê≥Å",
                        "comment": f"„É™„Çπ„ÇØÊäëÂà∂Êà¶Áï•„ÅÆÁä∂Ê≥Å ‚Üí ",
                        "reason_type":"Êï¥ÂêàÊÄß", 
                        "locations": [{"x0": 0, "x1": 0, "y0": 0, "y1": 0}],
                        "intgr": True, 
                    }]
                })
            else:
                return jsonify({
                    "success": True,
                    "corrections": [{
                        "page": pageNumber,
                        "original_text": "„É™„Çπ„ÇØÊäëÂà∂Êà¶Áï•„ÅÆÁä∂Ê≥Å",
                        "check_point": "„É™„Çπ„ÇØÊäëÂà∂Êà¶Áï•„ÅÆÁä∂Ê≥Å",
                        "comment": f"„É™„Çπ„ÇØÊäëÂà∂Êà¶Áï•„ÅÆÁä∂Ê≥Å ‚Üí ",
                        "reason_type": "„É™„Çπ„ÇØÊäëÂà∂Êà¶Áï•„ÅÆÁä∂Ê≥Å„ÅåÂ≠òÂú®„Åó„Å¶„ÅÑ„Åæ„Åõ„Çì„ÄÇ",  
                        "locations": [{"x0": 0, "x1": 0, "y0": 0, "y1": 0}],
                        "intgr": True,  
                    }]
                })

        elif org_text == "ÈäòÊüÑÂêç1ÔΩû10":
            content = _content
        elif org_text == "„ÄêÈäòÊüÑÂêç„ÄëL‚ÄôOccitane en ProvenceÔºàÊ¨ßÂ∑ûÔºâ":
            content_re = re.search("„ÄêÈäòÊüÑÂêç„Äë.{,100}", _content)
            if content_re:
                content = content_re.group()
            else:
                content = ""


        else:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            content, __answer = loop.run_until_complete(get_original(_content, org_text, file_name, target_text))

            if not content:
                return jsonify({
                    "success": True,
                    "answer": __answer,
                    "corrections": []
                })

        pdf_base64 = data.get("pdf_bytes", "")

        file_name_decoding = data.get("file_name", "")

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        if condition:
            result_temp = []
            table_list = condition.split("\n")
            for data in table_list:
                if data:
                    if category in ["ÊØîÁéá", "ÈÖçÂàÜ"]:
                        re_num = re.search(r"([-\d. ]+)(%|ÔºÖ)", content)
                        if re_num:
                            num = re_num.groups()[0]
                            float_num = len(str(num).split(".")[1]) if "." in num else 0
                            old_data = pd.read_json(StringIO(data))
                            result_temp.append(old_data.applymap(
                                lambda x: (str(round(x * 100, float_num)) + "%" if float_num != 0 else str(
                                    int(round(x * 100, float_num))) + "%")
                                if not pd.isna(x) and isinstance(x, float) else x).to_json(force_ascii=False))
                        else:
                            result_temp.append(pd.read_json(StringIO(data)).to_json(force_ascii=False))
                    else:
                        result_temp.append(pd.read_json(StringIO(data)).to_json(force_ascii=False))
            if len(result_temp) > 1:
                result_data = "\n".join(result_temp)
            else:
                result_data = result_temp[0]
        else:
            result_data = ""

        input_list = [
            "‰ª•‰∏ã„ÅÆÂÜÖÂÆπ„Å´Âü∫„Å•„ÅÑ„Å¶„ÄÅÂéüÊñá„ÅÆË®òËø∞„ÅåÊ≠£„Åó„ÅÑ„Åã„Å©„ÅÜ„Åã„ÇíÂà§Êñ≠„Åó„Å¶„Åè„Å†„Åï„ÅÑ", "Ë¶Å‰ª∂:",
            "- „ÄéÂèÇËÄÉ„Éá„Éº„Çø„Äè„Å´Ë©≤ÂΩì„Åô„ÇãÊÉÖÂ†±„Åå„Å™„ÅÑÂ†¥Âêà„ÄÅ„Åù„ÅÆË®òËø∞„Å´„Å§„ÅÑ„Å¶„ÅØÂà§Êñ≠„ÇíË°å„Çè„Åö„ÄÅ„ÄåÂà§ÂÆöÂØæË±°Â§ñ„Äç„Å®ÊòéË®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
            "- ÊúÄÂæå„Å´ÂéüÊñá„ÅÆË®òËø∞„ÅåÊ≠£„Åó„ÅÑ„Åã„Å©„ÅÜ„Åã„ÇíÊòéÁ¢∫„Å´Âà§Êñ≠„Åó„ÄÅÊñáÊú´„Å´„ÄéOK„Äè„Åæ„Åü„ÅØ„ÄéNG„Äè„ÇíË®òËºâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
            f"- **ÁèæÂú®„ÅÆÂèÇËÄÉ„Éá„Éº„Çø„ÅØ20{base_month[1:3]}Âπ¥{base_month[3:]}Êúà„ÅÆÂèÇËÄÉ„Éá„Éº„Çø„Åß„Åô**",
            f"- Êñá‰∏≠„Å´„ÄéÂÖàÊúàÊú´„Äè„ÄéÂâçÊúàÊú´„Äè„Äé‚óãÊúàÊú´„Äè„Å™„Å©„ÅÆË°®Áèæ„Åå„ÅÇ„Å£„Å¶„ÇÇ„ÄÅÁèæÂú®„ÅÆÂèÇËÄÉ„Éá„Éº„ÇøÔºàÊúàÔºâ„ÇíÂü∫Ê∫ñ„Å®„Åó„Å¶Âà§Êñ≠„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
            f"ÂéüÊñá„ÅÆÂà§Êñ≠:'{content}'\nÂèÇËÄÉ„Éá„Éº„Çø:\n'{result_data}'",
        ]

        if consult:
            input_list.insert(3, consult)
        input_data = "\n".join(input_list)
        question = [
            {"role": "system", "content": "„ÅÇ„Å™„Åü„ÅØÊó•Êú¨Ë™ûÊñáÊõ∏„ÅÆÊ†°Ê≠£„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô"},
            {"role": "user", "content": input_data}
        ]

        response = openai.ChatCompletion.create(
            deployment_id=deployment_id,  # Deploy Name
            messages=question,
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE,
            seed=SEED  # seed
        )
        answer = response['choices'][0]['message']['content'].strip()
        if answer:
            dt = [
                "‰ª•‰∏ã„ÅÆÂàÜÊûêÁµêÊûú„Å´Âü∫„Å•„Åç„ÄÅÂéüÊñá‰∏≠„ÅÆË™§„Çä„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "Âá∫Âäõ„ÅØ‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„Åß„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô:",
                "- [{'original': '[ÂéüÊñá‰∏≠„ÅÆË™§„Å£„Å¶„ÅÑ„ÇãÈÉ®ÂàÜ:]', 'reason': '[ÁêÜÁî±:]'}]",
                "- ÂéüÊñá„ÅÆÊú´Â∞æ„Å´„ÄåOK„Äç„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅÁ©∫ÊñáÂ≠óÂàó„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                f"ÂéüÊñá:'{content}'\nÂàÜÊûêÁµêÊûú:'{answer}'"
            ]
            summarize = "\n".join(dt)
            _question = [
                {"role": "system", "content": "„ÅÇ„Å™„Åü„ÅØÊó•Êú¨Ë™ûÊñáÊõ∏„ÅÆÊ†°Ê≠£„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô"},
                {"role": "user", "content": summarize}
            ]
            _response = openai.ChatCompletion.create(
                deployment_id=deployment_id,  # Deploy Name
                messages=_question,
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                seed=SEED  # seed
            )
            _answer = _response['choices'][0]['message']['content'].strip().replace("`", "").replace("json", "", 1)
            parsed_data = ast.literal_eval(_answer)
            corrections = []
            if parsed_data:
                for once in parsed_data:
                    error_data = once.get("original", "")
                    reason = once.get("reason", "")
                    corrections.append({
                        "page": pageNumber,
                        "original_text": get_src(error_data, _content).replace("„ÄÇ‚óã","").replace("„ÄÇ‚óØ","").strip().rsplit('\n', 1)[0],
                        "check_point": content,
                        "comment": f"{error_data} ‚Üí {reason}", #
                        "reason_type":reason, 
                        "locations": [],
                        "intgr": True, 
                    })
            else:
                corrections.append({
                    "page": pageNumber,
                    "original_text": get_src(content, _content).replace("„ÄÇ‚óã","").replace("„ÄÇ‚óØ","").strip().rsplit('\n', 1)[0],
                    "check_point": content,
                    "comment": f"{content} ‚Üí ",
                    "reason_type": "Êï¥ÂêàÊÄß",  
                    "locations": [],
                    "intgr": True,
                })

            try:
                pdf_bytes = base64.b64decode(pdf_base64)
                find_locations_in_pdf(pdf_bytes, corrections)
                

            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500

            return jsonify({
                "success": True,
                "answer": __answer,
                "first_answer": answer,
                "input_data": input_data,
                "corrections": corrections
            })
        else:
            return jsonify({
                "success": True,
                "corrections": [{
                    "page": pageNumber,
                    "original_text": content,
                    "check_point": content,
                    "comment": f"{content} ‚Üí ",
                    "reason_type":"Êï¥ÂêàÊÄß", 
                    "locations": [{"x0": 0, "x1": 0, "y0": 0, "y1": 0}],
                    "intgr": True, 
                }]  
            })

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 200

def extract_or_return(sentence):
    pattern = (
    r"(?P<fund_return>(?:„Éï„Ç°„É≥„Éâ|Âü∫Ê∫ñ‰æ°È°ç(?:ÔºàÂàÜÈÖçÈáëÂÜçÊäïË≥áÔºâ)?|Âü∫Ê∫ñ‰æ°È°ç„ÅÆÂ§âÂãïÁéá|Âü∫Ê∫ñ‰æ°È°çÈ®∞ËêΩÁéá|È®∞ËêΩÁéá)[„ÅÆ]?(?:Â§âÂãïÁéá|È®∞ËêΩÁéá)?[-‚àí]?\d+\.?\d*ÔºÖ?)?.*?"
    r"(?P<benchmark_return>(?:BM|„Éô„É≥„ÉÅ„Éû„Éº„ÇØ|ÂèÇËÄÉÊåáÊï∞)[„ÅÆ]?(?:È®∞ËêΩÁéá|Â§âÂãïÁéá)?[-‚àí]?\d+\.?\d*ÔºÖ?)?.*?"
    r"(?P<diff_points>\d+\.?\d*„Éù„Ç§„É≥„Éà)?.*?"
    r"(?P<direction>(‰∏äÂõû[„Çä]*|‰∏ãÂõû[„Çä]*))?"
    )

    match = re.search(pattern, sentence)

    extracted = [v for v in match.groupdict().values() if v]

    return extracted if extracted else [sentence]

@app.route('/api/ruru_ask_gpt', methods=['POST'])
def ruru_ask_gpt():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("‚úÖ Token Update SUCCESS")
        
        data = request.json
        _input = data.get("input", "")
        result = data.get("result", "")
        orgtext = data.get("Org_Text", "")
        OrgType = data.get("Org_Type", "")
        TargetCondition = data.get("Target_Condition", "")
        pageNumber = data.get('pageNumber',0)
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        input, __answer = loop.run_until_complete(get_original(_input, orgtext))
        corrections = []
        pdf_base64 = data.get("pdf_bytes", "")
        if not input:
            dt = [
            "ÊñáÁ´†„Åã„ÇâÂéüÊñá„Å´È°û‰ºº„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
            "Âá∫Âäõ„ÅØ‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„Åß„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô:",
            "- {'target': '[ÊäΩÂá∫„Åï„Çå„Åü„ÉÜ„Ç≠„Çπ„Éà:]'}",
            "- È°û‰ºº„Åó„Åü„ÇÇ„ÅÆ„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅÁ©∫„ÅÆÊñáÂ≠óÂàó„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
            "- È°û‰ºº„Åó„Åü„ÇÇ„ÅÆ„ÅåÂ≠òÂú®„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅÊúÄ„ÇÇÈ°û‰ººÂ∫¶„ÅÆÈ´ò„ÅÑ„ÇÇ„ÅÆ„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ",

            f"ÂéüÊñá:{orgtext}\nÊñáÁ´†:{_input}"
            ]
            input_data = "\n".join(dt)

            question = [
                {"role": "system", "content": "„ÅÇ„Å™„Åü„ÅØ„ÉÜ„Ç≠„Çπ„ÉàÊäΩÂá∫„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô"},
                {"role": "user", "content": input_data}
            ]
            response = openai.ChatCompletion.create(
                deployment_id=deployment_id,  # Deploy Name
                messages=question,
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                seed=SEED  # seed
            )
            _answer = response['choices'][0]['message']['content'].strip().strip().replace("`", "").replace("json", "", 1)
            _parsed_data = ast.literal_eval(_answer)
            _similar = _parsed_data.get("target")

            pattern = r'([ABCDEF]„Ç≥„Éº„Çπ.?[+-]?\d+(?:\.\d+)?ÔºÖ|[ABCDEF]„Ç≥„Éº„Çπ.?Âü∫Ê∫ñ‰æ°È°ç„ÅØ(?:‰∏ãËêΩ|‰∏äÊòá)(?:„Åæ„Åó„Åü)?)'

            matches_list = re.findall(pattern, _similar)
            for re_result in matches_list:
                                
                corrections.append({
                        "page": pageNumber,
                        "original_text": re_result,
                        "check_point": re_result,
                        "comment": f"{re_result} ‚Üí ", # +0.2% ‚Üí 0.85% f"{reason} ‚Üí {corrected}"
                        "reason_type": "Êï¥ÂêàÊÄß",  
                        "locations": [],
                        "intgr": True,
                    })
                
            try:
                pdf_bytes = base64.b64decode(pdf_base64)
                
                find_locations_in_pdf(pdf_bytes, corrections)
                
            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500

        else:
            if not input:
                return jsonify({"success": False, "error": "No input provided"}), 400
            
            # add the write logic
            dt = [
                "ÊñáÁ´†„Åã„ÇâÂéüÊñá„Å´È°û‰ºº„Åó„Åü„ÉÜ„Ç≠„Çπ„Éà„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "Âá∫Âäõ„ÅØ‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„Åß„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô:",
                "- [{'original': '[ÂéüÊñá‰∏≠„ÅÆË™§„Å£„Å¶„ÅÑ„ÇãÈÉ®ÂàÜ:]', 'reason': '[ÁêÜÁî±:]'}]",
                "- È°û‰ºº„Åó„Åü„ÇÇ„ÅÆ„Åå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅÁ©∫„ÅÆÊñáÂ≠óÂàó„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
                "- È°û‰ºº„Åó„Åü„ÇÇ„ÅÆ„ÅåÂ≠òÂú®„Åô„ÇãÂ†¥Âêà„ÅØ„ÄÅÊúÄ„ÇÇÈ°û‰ººÂ∫¶„ÅÆÈ´ò„ÅÑ„ÇÇ„ÅÆ„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ",

                "„ÅÇ„Å™„Åü„ÅØÊó•Êú¨„ÅÆÈáëËûç„É¨„Éù„Éº„Éà„ÇíÂ∞ÇÈñÄ„Å®„Åô„Çã„Éó„É≠„ÅÆÊ†°Ê≠£ËÄÖ„Åß„Åô„ÄÇ",
                "‰ª•‰∏ã„ÅÆË¶ÅÁ¥ÑÊñá(Input)„Çí„ÄÅÁµêÊûú(Result)„Å®ÊØîËºÉ„Åó„ÄÅÊï∞ÂÄ§„ÇÑÊÑèÂë≥„Å´Èñ¢„Åó„Å¶Ê≠£„Åó„ÅÑ„Åã„Çí„ÉÅ„Çß„ÉÉ„ÇØ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
                "Áâπ„Å´Ê¨°„ÅÆ„Çà„ÅÜ„Å™Ë™§„Çä„Åå„Å™„ÅÑ„Åã„ÇíÁ¢∫Ë™ç„Åó„Å¶„Åè„Å†„Åï„ÅÑ:",
                "- È®∞ËêΩÁéáÔºà%Ôºâ„ÅÆ‰∏ç‰∏ÄËá¥",
                "- ÂèÇËÄÉÊåáÊï∞Ôºà„Éô„É≥„ÉÅ„Éû„Éº„ÇØÔºâ„ÅÆÈ®∞ËêΩÁéá„ÅÆ‰∏ç‰∏ÄËá¥",
                "- „Éù„Ç§„É≥„Éà",
                "- ‰∏äÂõû„Å£„ÅüÔºè‰∏ãÂõû„Å£„Åü„ÅÆÊñπÂêëÊÄß„ÅÆË™§„Çä",
                "- Êúà„ÇÑÊúüÈñì„ÅÆ‰∏ç‰∏ÄËá¥",

                f"ÂéüÊñá(Input): {input}",
                f"ÊßãÁµêÊûú(Result): {result}",
                f"ÂéüÊñáÁ®ÆÂà•(original): {OrgType}"
            ]

            input_data = "\n".join(dt)

            question = [
                {"role": "system", "content": "„ÅÇ„Å™„Åü„ÅØ„ÉÜ„Ç≠„Çπ„ÉàÊäΩÂá∫„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô"},
                {"role": "user", "content": input_data}
            ]
            response = openai.ChatCompletion.create(
                deployment_id=deployment_id,  # Deploy Name
                messages=question,
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                seed=SEED  # seed
            )
            _answer = response['choices'][0]['message']['content'].strip().strip().replace("`", "").replace("json", "", 1)
            _parsed_data = ast.literal_eval(_answer)
            corrections = []
            if _parsed_data:
                for once in _parsed_data:
                    error_data = once.get("original", "")
                    reason = once.get("reason", "")
                    corrections.append({
                        "page": pageNumber,
                        "original_text": clean_percent_prefix(error_data),
                        "check_point": input,
                        "comment": f"{error_data} ‚Üí {reason}", 
                        "reason_type":reason, 
                        "locations": [],
                        "intgr": True, 
                    })
            else:
                segments = []
                segments= extract_parts_with_direction(input)
                corrections = []
                for part in segments:
                    if part:
                        corrections.append({
                            "page": pageNumber,
                            "original_text": part.strip(),
                            "check_point": input,
                            "comment": f"{part.strip()} ‚Üí ",
                            "reason_type": "Êï¥ÂêàÊÄß",  
                            "locations": [],
                            "intgr": True,
                        })
                
            if pdf_base64:
                try:
                    pdf_bytes = base64.b64decode(pdf_base64)
                    
                    find_locations_in_pdf(pdf_bytes, corrections)
                    
                except ValueError as e:
                    return jsonify({"success": False, "error": str(e)}), 400
                except Exception as e:
                    return jsonify({"success": False, "error": str(e)}), 500
        
        if not corrections:
        #     match = re.search(r"Ë∂ÖÈÅéÂèéÁõä[^-+0-9]*([+-]?\d+(?:\.\d+)?)", input)
        #     if match:
        #         value = match.group(1)
        #     else:
        #         value = input
            corrections.append({
                        "page": pageNumber,
                        "original_text": clean_percent_prefix(input),  # ÂÄíÊï∞4‰∏™Â≠óÁ¨¶ [:15]
                        "check_point": input,
                        "comment": f"{input} ‚Üí ", # +0.2% ‚Üí 0.85% f"{reason} ‚Üí {corrected}"
                        "reason_type": "Êï¥ÂêàÊÄß",
                        "locations": [],
                        "intgr": True,
                    })
                
            try:
                pdf_bytes = base64.b64decode(pdf_base64)
                
                find_locations_in_pdf(pdf_bytes, corrections)
                
            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500
            
        # return JSON
        return jsonify({
            "success": True,
            "corrections": corrections,  
            "input": input, 
            "answer": _parsed_data, 
        })
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    
# 611 opt - debug new prompt
def extract_text_from_base64_pdf(pdf_base64: bytes) -> list:
    # Base64 -> PDF bytes
    # pdf_bytes = base64.b64decode(pdf_base64)

    pdf_document = fitz.open(stream=pdf_base64, filetype="pdf")

    text_all = []
    keyword_pages = []
    page_list = []
    for page_num in range(pdf_document.page_count):
        page = pdf_document.load_page(page_num)

        full_text = page.get_text()

        keyword_pos = -1
        for keyword in ["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "ÁµÑÂÖ•ÈäòÊüÑ","ÁµÑÂÖ•‰∏ä‰Ωç10ÈäòÊüÑ„ÅÆËß£Ë™¨"]:
            keyword_pos = full_text.find(keyword)
            if keyword_pos != -1:
                keyword_pages.append(page_num)
                break

        if page_num in keyword_pages:
            # text_all.append(full_text)
            page_text = full_text
            

        else:
            blocks = page.get_text("blocks")  # (x0, y0, x1, y1, "text", block_no, block_type)
            blocks.sort(key=lambda b: b[1])
            page_text = "".join(block[4] for block in blocks)
            # text_all.append(page_text)

        # page_list.append(("".join(text_all), page_num))

        page_list.append((page_text, page_num))
            
    return page_list


# add pre-half logic
half_to_full_map = {
    '%': 'ÔºÖ',
    '@': 'Ôº†',
    '&': 'ÔºÜ',
    '!': 'ÔºÅ',
    '?': 'Ôºü',
    '#': 'ÔºÉ',
    '$': 'ÔºÑ',
    '(': 'Ôºà',
    ')': 'Ôºâ',
    '+': 'Ôºã'
}
def convert_halfwidth_to_fullwidth_safely(text):
    # (‰øÆÊ≠£ÁêÜÁî±)
    protected_blocks = {}
    
    def protect_span(match):
        key = f"__PROTECT_{len(protected_blocks)}__"
        protected_blocks[key] = match.group(0)
        return key

    text = re.sub(r'<span[^>]*?>‰øÆÊ≠£ÁêÜÁî±:.*?</span>\)', protect_span, text)

    def replace_half(match):
        char = match.group(0)
        full = half_to_full_map[char]
        return (
            f'<span style="color:red;">{full}</span> '
            f'(<span>‰øÆÊ≠£ÁêÜÁî±: ÂçäËßíË®òÂè∑„ÇíÂÖ®Ëßí„Å´Áµ±‰∏Ä '
            f'<s style="background:yellow;color:red">{char}</s> ‚Üí {full}</span>)'
        )

    pattern = re.compile('|'.join(map(re.escape, half_to_full_map.keys())))
    text = pattern.sub(replace_half, text)

    for key, val in protected_blocks.items():
        text = text.replace(key, val)

    return text

def get_num(num):
    if num:
        num_str = str(num)
        num_len = len(num_str)
        num_list = []
        for i in range(num_len, 0, -3):
            if i - 3 < 0:
                num_r = 0
            else:
                num_r = i - 3
            num_list.insert(0, num_str[num_r: i])
        return ",".join(num_list)
    return ""


def get_src(no_space, src_content):
    content_flag = "".join([i + "‚òÜ" for i in no_space])
    content_re = regcheck.escape(content_flag).replace("‚òÜ", ".?")
    res = regcheck.search(content_re, src_content, flags=regcheck.DOTALL)
    if res:
        return res.group()
    else:
        return no_space



# async call ,need FE promises
def opt_common(input, prompt_result, pdf_base64, pageNumber, re_list, rule_list, rule1_list, rule3_list,symbol_list):  
    # ChatCompletion Call
    response = openai.ChatCompletion.create(
        deployment_id=deployment_id,  # Deploy Name
        messages=[
            {"role": "system", "content": "You are a Japanese text extraction tool capable of accurately extracting the required text."},
            {"role": "user", "content": prompt_result}
        ],
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  # seed
    )
    answer = response['choices'][0]['message']['content'].strip().replace("`", "").replace("json", "", 1).replace("\n", "")
    parsed_data = ast.literal_eval(answer)
    combine_corrections = []
    src_corrections = []
    if isinstance(parsed_data, list):
        for re_index, data in enumerate(parsed_data):
            _re_rule = ".{,2}"
            data["original"] = get_src(data["original"], input)
            _original_re = regcheck.search(f"{_re_rule}{regcheck.escape(data["original"])}{_re_rule}", input)
            if _original_re:
                _original_text = _original_re.group()
            else:
                _original_text = data["original"]
            combine_corrections.append({
                "page": pageNumber,
                "original_text": _original_text,
                "comment": f'{_original_text} ‚Üí {data["correct"]}',
                "reason_type": data["reason"],
                "check_point": _original_text,
                "locations": [],
                "intgr": False,  
            })
            src_corrections.append(f'{data["original"]} ‚Üí {data["correct"]}')

    if rule_list:
        for rule_result in rule_list:
            combine_corrections.append({
                "page": pageNumber,
                "original_text": str(rule_result),  
                "comment": f"{str(rule_result)} ‚Üí ÂΩìÊúà„ÅÆÊäïË≥áÈÖçÂàÜ",
                "reason_type": "Ë™§Â≠óËÑ±Â≠ó",
                "check_point": str(rule_result),
                "locations": [],
                "intgr": False,  
            })

    if re_list:
        for re_result in re_list:
            correct = get_num(re_result)
            combine_corrections.append({
                "page": pageNumber,
                "original_text": str(re_result),  
                "comment": correct,
                "reason_type": "Êï∞ÂÄ§ÂçÉ‰ΩçÈÄóÂè∑ÂàÜÈöî‰øÆÊ≠£",
                "check_point": str(re_result),
                "locations": [],
                "intgr": False,  
            })

    if rule1_list:
        for rule1_result in rule1_list:
            combine_corrections.append({
                "page": pageNumber,
                "original_text": rule1_result,  
                "comment": f"{rule1_result} ‚Üí  ",
                "reason_type": "ÂâäÈô§",
                "check_point": rule1_result,
                "locations": [],
                "intgr": False,  
            })

    if rule3_list:
        for rule3_result in rule3_list:
            combine_corrections.append({
                "page": pageNumber,
                "original_text": rule3_result,  
                "comment": f"{rule3_result} ‚Üí {rule3_result[1:]}",
                "reason_type": "ÂâäÈô§",
                "check_point": rule3_result,
                "locations": [],
                "intgr": False,  
            })

    # if word_list:
    #     for word_result in word_list:
    #         combine_corrections.append({
    #             "page": pageNumber,
    #             "original_text": word_result,  
    #             "comment": f"{word_result} ‚Üí ÂÄ§‰∏ä„Åå„Çä„Åó",
    #             "reason_type": "ÂãïË©ûÂõ∫ÂÆöÁî®Ê≥ï",
    #             "check_point": word_result,
    #             "locations": [],
    #             "intgr": False,  
    #         })
    
    # „Åï„Çå„ÄÅ‰∏ãËêΩ„Åó
    if symbol_list:
        for symbol_result in symbol_list:
            combine_corrections.append({
                "page": pageNumber,
                "original_text": symbol_result,  
                "comment": f"{symbol_result} ‚Üí „Åï„Çå‰∏ãËêΩ„Åó",
                "reason_type": "Ë™≠ÁÇπ„ÇíÂâäÈô§„Åô„Çã",
                "check_point": symbol_result,
                "locations": [],
                "intgr": False,  
            })

    if pdf_base64:
        try:
            pdf_bytes = base64.b64decode(pdf_base64)
            
            find_locations_in_pdf(pdf_bytes, combine_corrections)
            for idx, _comment in enumerate(src_corrections):
                combine_corrections[idx]["comment"] = _comment

        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500

    # return JSON
    return jsonify({
        "success": True,
        "corrections": combine_corrections,  
        "parsed_data": parsed_data
    })

async def opt_common_wording(file_name,fund_type,input,prompt_result,excel_base64,pdf_base64,resutlmap,upload_type,comment_type,icon,pageNumber):
    # ChatCompletion Call
    response = await openai.ChatCompletion.acreate(
        deployment_id=deployment_id,  # Deploy Name
        messages=[
            {"role": "system", "content": "„ÅÇ„Å™„Åü„ÅØÊõñÊòß„Å™Ë°®Áèæ„ÇíÂÆöÂûãË™û„Å´Â§âÊèõ„Åô„Çã„ÄÅÂé≥Ê†º„Å™ÈáëËûçÊ†°Ê≠£AI„Åß„Åô„ÄÇÂá∫ÂäõÂΩ¢Âºè„Éª‰øÆÊ≠£„É´„Éº„É´„ÅØ„Åô„Åπ„Å¶Âé≥ÂÆà„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"},
            {"role": "user", "content": prompt_result}
        ],
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  # seed
    )
    answer = response['choices'][0]['message']['content'].strip()
    re_answer = remove_code_blocks(answer)

    # add the write logic
    corrections = find_corrections(re_answer,input,pageNumber)

    corrections_wording = find_corrections_wording(input,pageNumber)

    combine_corrections = corrections + corrections_wording

    if excel_base64:
        try:
            excel_bytes_decoding = base64.b64decode(excel_base64)
            modified_bytes = correct_text_box_in_excel(excel_bytes_decoding,resutlmap)

            # 3) return xlsx
            return send_file(
                io.BytesIO(modified_bytes),
                mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                as_attachment=True,
                download_name="annotated.xlsx"
            )
        except Exception as e:
            return jsonify({
                "success": False,
                "error": str(e)
            })


    if pdf_base64:
        try:
            pdf_bytes = base64.b64decode(pdf_base64)
            
            find_locations_in_pdf(pdf_bytes, combine_corrections)
            
        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500

    # return JSON
    return jsonify({
        "success": True,
        "corrections": combine_corrections,  
        "debug_re_answer":re_answer, #610 debug
    })

@app.route('/api/prompt_test', methods=['GET'])
def get_prompt_data():
    prompt_result1 = get_prompt("\"" + "111111111111111111111111111" + "\"")
    prompt_result2 = loop_in_ruru("\"" + "1111111111111111111111111111" + "\"")
    return jsonify(dict(xu=list(prompt_result1), tang=list(prompt_result2)))


@app.route('/api/opt_typo', methods=['POST'])
def opt_typo():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("‚úÖ Token Update SUCCESS")
        
        data = request.json
        input = data.get("input", "")

        pdf_base64 = data.get("pdf_bytes", "")
        excel_base64 = data.get("excel_bytes", "")
        resutlmap = data.get("original_text", "")

        fund_type = data.get("fund_type", "public")  #  'public'
        file_name_decoding = data.get("file_name", "")
        upload_type = data.get("upload_type", "")
        comment_type = data.get("comment_type", "")
        icon = data.get("icon", "")
        pageNumber = data.get('pageNumber',0)

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        if not input:
            return jsonify({"success": False, "error": "No input provided"}), 400
        
        if len(input) < 5:
            return jsonify({"success": True, "corrections": [],})

        prompt_result = get_prompt("\"" + input.replace('\n', '') + "\"")
        async def run_tasks():
            tasks = [handle_result(once) for once in prompt_result]
            return await asyncio.gather(*tasks)

        results = asyncio.run(run_tasks())
        sec_input = "\n".join(results)

        dt = [
            "‰ª•‰∏ã„ÅÆÂàÜÊûêÁµêÊûú„Å´Âü∫„Å•„Åç„ÄÅÂéüÊñá‰∏≠„ÅÆË™§„Çä„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
            "- Âá∫ÂäõÁµêÊûú„ÅØÊØéÂõûÂêå„Åò„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà**Âêå„ÅòÂÖ•Âäõ„Å´ÂØæ„Åó„Å¶ÁµêÊûú„ÅåÂ§âÂãï„Åó„Å™„ÅÑ„Çà„ÅÜ„Å´**„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºâ„ÄÇ",
            "- original„Å´„ÅØÂøÖ„ÅöÂÖ®Êñá„ÇÑÈï∑„ÅÑÊñá„Åß„ÅØ„Å™„Åè„ÄÅ**reason_type„ÅßÊåáÊëò„Åï„Çå„Å¶„ÅÑ„ÇãÊúÄÂ∞èÈôê„ÅÆË™§„Çä„Éù„Ç§„É≥„ÉàÔºàÂçòË™û„ÇÑÂä©Ë©û„Å™„Å©Ôºâ**„ÅÆ„Åø„ÇíË®òËºâ„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
            "- 1ÂçòË™û„Åæ„Åü„ÅØ„Åî„ÅèÁü≠„ÅÑ„Éï„É¨„Éº„Ç∫Âçò‰Ωç„Åßoriginal„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
            "- original„ÅØreason_type„ÅÆË™¨Êòé„Å´Ë©≤ÂΩì„Åô„ÇãÈÉ®ÂàÜ„ÅÆ„Åø„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà‰æãÔºö„Äé„Å™„Å©„Äè„ÅÆÂæå„Å´Âä©Ë©û„Äé„ÅÆ„Äè„ÅåÂøÖË¶Å‚Üíoriginal„ÅØÂøÖ„Åö„Äé„Å™„Å©„ÄèÔºâ„ÄÇ",
            "- Âêå„ÅòÂÖ•Âäõ„Å´„ÅØÂ∏∏„Å´**Âêå„ÅòJSONÂΩ¢Âºè„ÅÆÂá∫Âäõ**„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÊé®Ë´ñ„ÅÆÊè∫„Çå„ÇíÈÅø„Åë„Å¶„Åè„Å†„Åï„ÅÑÔºâ„ÄÇ",
            
            "Âá∫Âäõ„ÅØ‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„Åß„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô:",
            "- [{'original': '[ÂéüÊñá‰∏≠„ÅÆË™§„Å£„Å¶„ÅÑ„ÇãÊúÄÂ∞èÂçò‰Ωç„ÅÆÈÉ®ÂàÜ]', 'correct': '[Ê≠£„Åó„ÅÑ„ÉÜ„Ç≠„Çπ„Éà]', 'reason': '[ÁêÜÁî±:]'}]",
            "- ÂàÜÊûêÁµêÊûú„Å´‰øÆÊ≠£ÈÉ®ÂàÜ„Åå„ÅÇ„ÇãÂ†¥Âêà„ÅØ„ÄÅÂøÖ„ÅöÁ©∫„ÅÆ„É™„Çπ„Éà„ÇíËøî„Åï„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ",
            "„Äê‰æã„Äë",
            "reason_type: 'Âπ¥Ë°®Ë®ò„ÅØ4Ê°ÅÔºàË•øÊö¶Ôºâ„Å´Áµ±‰∏Ä'",
            "ÂéüÊñá: \"22Âπ¥„ÅÆÁµåÊ∏àÊàêÈï∑Áéá„ÅØ-1ÔΩû0.5„ÅÆÁØÑÂõ≤„ÅßÊé®Áßª„Åó„Åæ„Åó„Åü„ÄÇ\"",
            "Âá∫Âäõ‰æã:",
            "[",
            "  {",
            "    \"original\": \"22Âπ¥\",",
            "    \"correct\": \"2022Âπ¥\",",
            "    \"reason\": \"Âπ¥Ë°®Ë®ò„ÅØ4Ê°ÅÔºàË•øÊö¶Ôºâ„Å´Áµ±‰∏Ä\"",
            "  }",
            "]",
            "reason_type: '‰æãÁ§∫„Äé„Å™„Å©„Äè„ÅÆÂæå„Å´„ÅØÂä©Ë©û„Äé„ÅÆ„Äè„ÅåÂøÖË¶Å„ÄÇ„Äé„Å™„Å©Êµ∑Â§ñ‰∏ªË¶Å‰∏≠ÈäÄ„Äè„ÅØÊñáÊ≥ïÁöÑ„Å´‰∏çËá™ÁÑ∂„Å™„Åü„ÇÅ„ÄÇ'",
            "ÂéüÊñá: \"„Å™„Å©Êµ∑Â§ñ‰∏ªË¶Å‰∏≠ÈäÄ„Å´„Çà„Çã\"",
            "Âá∫Âäõ‰æã:",
            "[",
            "  {",
            "    \"original\": \"„Å™„Å©\",",
            "    \"correct\": \"„Å™„Å©„ÅÆ\",",
            "    \"reason\": \"‰æãÁ§∫„Äé„Å™„Å©„Äè„ÅÆÂæå„Å´„ÅØÂä©Ë©û„Äé„ÅÆ„Äè„ÅåÂøÖË¶Å\"",
            "  }",
            "]",
            f"ÂéüÊñá:'{input}'\nÂàÜÊûêÁµêÊûú:'{sec_input}'"

        ]
        sec_prompt = "\n".join(dt)
        re_list = regcheck.findall(r"(\d{4,})[‰∫∫Á®Æ‰∏áÂÜÜÂÖÜÂÑÑ]", input)
        # word_list = regcheck.findall(r".{,2}ÂÄ§‰∏ä„Åå„Çä(?!„Åó).{,2}", input)
        rule_list = regcheck.findall(r"ÂΩìÊúàÊäïË≥áÈÖçÂàÜ", input)
        rule1_list = regcheck.findall(r"„Äê(ÂÖàÊúà„ÅÆÊäïË≥áÁí∞Â¢É|ÂÖàÊúà„ÅÆÈÅãÁî®ÁµåÈÅé|‰ªäÂæå„ÅÆÈÅãÁî®ÊñπÈáù)„Äë", input)
        rule3_list = regcheck.findall(r"-[\d.ÔºÖ]{4,6}‰∏ãËêΩ", input)
        symbol_list = regcheck.findall(r"„Åï„Çå„ÄÅ‰∏ãËêΩ„Åó", input)

        _content = opt_common(input, sec_prompt, pdf_base64,pageNumber,re_list,rule_list,rule1_list,rule3_list,symbol_list)
        return _content

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

async def handle_result(prompt_result):
    response = await openai.ChatCompletion.acreate(
        deployment_id=deployment_id,  # Deploy Name
        messages=[
            {"role": "system",
            "content": "You are a professional Japanese business document proofreader specialized in financial and public disclosure materials."},
            {"role": "user", "content": prompt_result}
        ],
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  # seed
    )
    answer = response['choices'][0]['message']['content'].strip()
    return answer

def get_prompt(corrected):
    example_0 = "'original': 'ÊúàÈñì„Åß„ÅØ„Åª„ÅºÂ§â„Çè„Çâ„Åö„Å™„Çä„Åæ„Åó„Åü„ÄÇ', 'correct': 'ÊúàÈñì„Åß„ÅØ„Åª„ÅºÂ§â„Çè„Çâ„Åö„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ', 'reason': 'Ë™§Â≠ó'"
    example_1 = "'original': 'ÁµåÂâ§ÊàêÈï∑', 'correct': 'ÁµåÊ∏àÊàêÈï∑', 'reason': 'Ë™§Â≠ó'"
    example_10 = "'original': 'Â≠ê‰æõ„Åü„Å°„ÅØÂÖ¨Âúí„ÅßËá™Áî±„Å´„ÅÇ„Åù„Åº„Çå„Åæ„Åô„Åã„ÄÇ', 'correct': 'Â≠ê‰æõ„Åü„Å°„ÅØÂÖ¨Âúí„ÅßËá™Áî±„Å´„ÅÇ„Åù„Å∞„Çå„Åæ„Åô„Åã„ÄÇ', 'reason': 'ÂãïË©ûÊ¥ªÁî®„ÅÆË™§„ÇäÔºà„ÄåÈÅä„Å∞„Çå„Çã„Äç‚Üí„ÄåÈÅä„Åº„Çå„Çã„ÄçÔºâ'"
    example_11 = "'original': 'Êàë„ÄÖ„ÅØÊñ∞„Åó„ÅÑ„Éó„É≠„Ç∏„Çß„ÇØ„Éà„Å´Âèñ„ÇäÁµÑ„Åø„Åó„ÄÅÊàêÊûú„Çí‰∏ä„Åí„Åæ„Åó„Åü„ÄÇ'"
    example_110 = "'original': '„Çª„ÇØ„Çø„ÉºÈÖçÂàÜ„Å´„Åä„ÅÑ„Å¶ÁâπÂåñÂûãÔºàÁâ©ÊµÅÊñΩË®≠Ôºâ„Çí„Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„ÉàÔºàÂèÇËÄÉÊåáÊï∞„Å®ÊØî„Åπ‰Ωé„ÇÅ„ÅÆÊäïË≥áÊØîÁéáÔºâ„Åó„Åü„Åì„Å®„Å™„Å©„Åå„Éó„É©„Çπ„Å´ÂØÑ‰∏é„Åó„Åæ„Åó„Åü„ÄÇ', 'correct': '„Çª„ÇØ„Çø„ÉºÈÖçÂàÜ„Å´„Åä„ÅÑ„Å¶ÁâπÂåñÂûãÔºàÁâ©ÊµÅÊñΩË®≠Ôºâ„Çí„Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„ÉàÔºàÂèÇËÄÉÊåáÊï∞„Å®ÊØî„Åπ‰Ωé„ÇÅ„ÅÆÊäïË≥áÊØîÁéáÔºâ„Å®„Åó„Åü„Åì„Å®„Å™„Å©„Åå„Éó„É©„Çπ„Å´ÂØÑ‰∏é„Åó„Åæ„Åó„Åü„ÄÇ', 'reason': 'ÂãïË©ûÊ¥ªÁî®„ÅÆË™§„ÇäÔºà„ÄåÈÅä„Å∞„Çå„Çã„Äç‚Üí„ÄåÈÅä„Åº„Çå„Çã„ÄçÔºâ'"
    example_111 = "'original': 'ÈõªÂ≠êÈÉ®ÂìÅ„ÇÑÈÄö‰ø°Ê©üÂô®„Å™„Å©„ÅÆË£ΩÈÄ†„ÉªË≤©Â£≤„ÇíË°å„Å™„ÅÜ„Ç∞„É≠„Éº„Éê„É´„Åß‰∫ãÊ•≠„ÇíÂ±ïÈñã„Åô„ÇãÈõªÂ≠ê„É¢„Ç∏„É•„Éº„É´„ÉªÈÉ®ÂìÅ„É°„Éº„Ç´„Éº„ÄÇ', 'correct': 'ÈõªÂ≠êÈÉ®ÂìÅ„ÇÑÈÄö‰ø°Ê©üÂô®„Å™„Å©„ÅÆË£ΩÈÄ†„ÉªË≤©Â£≤„ÇíË°å„Å™„ÅÜ„Ç∞„É≠„Éº„Éê„É´„Å´‰∫ãÊ•≠„ÇíÂ±ïÈñã„Åô„ÇãÈõªÂ≠ê„É¢„Ç∏„É•„Éº„É´„ÉªÈÉ®ÂìÅ„É°„Éº„Ç´„Éº„ÄÇ', 'reason': '„Ç∞„É≠„Éº„Éê„É´„ÅØ„Äå„Å´„Äç„Çí‰ΩøÁî®„Åô„Çã'"
    # example_2 = "'original': '‰ªäÂæå„ÅØ„Éà„É©„É≥„ÉóÊ¨°ÊúüÁ±≥Â§ßÁµ±È†ò„ÅåÊé≤„Åí„ÇãÊ∏õÁ®é„ÇÑË¶èÂà∂Á∑©Âíå„ÅÆÊîøÁ≠ñ„ÅåÁ±≥ÊôØÊ∞ó„ÇíÊäº„Åó‰∏ä„Åí„Çã„Åì„Å®„Åå„ÄÅÂ∏ÇÂ†¥„ÅÆ‰∏ãÊîØ„Åà„Å´„Å™„Çã„Å®ËÄÉ„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇÂºï„ÅçÁ∂ö„Åç„ÄÅFRB„Å´„Çà„ÇãÈáëËûçÊîøÁ≠ñ„ÇÑÊñ∞ÊîøÊ®©„ÅÆÊîøÁ≠ñ„Å´„Çà„ÇäÂΩ±Èüø„ÇíÂèó„Åë„Çã„Çª„ÇØ„Çø„Éº„Å™„Å©„ÇíÊ≥®Ë¶ñ„Åó„Å™„Åå„Çâ„ÄÅÈäòÊüÑ„ÇíÈÅ∏ÂÆö„Åó„Å¶ÈÅãÁî®„ÇíË°å„Å™„ÅÑ„Åæ„Åô', 'correct': '‰ªäÂæå„ÅØ„Éà„É©„É≥„ÉóÊ¨°ÊúüÁ±≥Â§ßÁµ±È†ò„ÅåÊé≤„Åí„ÇãÊ∏õÁ®é„ÇÑË¶èÂà∂Á∑©Âíå„ÅÆÊîøÁ≠ñ„ÅåÁ±≥ÊôØÊ∞ó„ÇíÊäº„Åó‰∏ä„Åí„Çã„Åì„Å®„Åå„ÄÅÂ∏ÇÂ†¥„ÅÆ‰∏ãÊîØ„Åà„Å´„Å™„Çã„Å®ËÄÉ„Åà„Å¶„ÅÑ„Åæ„Åô„ÄÇÂºï„ÅçÁ∂ö„Åç„ÄÅFRB„Å´„Çà„ÇãÈáëËûçÊîøÁ≠ñ„ÇÑÊñ∞ÊîøÊ®©„ÅÆÊîøÁ≠ñ„Å´„Çà„ÇäÂΩ±Èüø„ÇíÂèó„Åë„Çã„Çª„ÇØ„Çø„Éº„Å™„Å©„ÇíÊ≥®Ë¶ñ„Åó„Å™„Åå„Çâ„ÄÅÈäòÊüÑ„ÇíÈÅ∏ÂÆö„Åó„Å¶ÈÅãÁî®„ÇíË°å„Å™„ÅÑ„Åæ„Åô„ÄÇ', 'reason': 'ÊñáÊ≥ïË™§Áî®'"
    # example_4 = "'original': 'ÂçäÂ∞é‰Ωì„É°„Éº„Ç´„Éº„ÄÇ„Éû„Ç§„ÇØ„É≠„Ç≥„É≥„Éà„É≠„Éº„É©„Éº„ÇÑ Èñ¢ÈÄ£„ÅÆË§áÂêà‰ø°Âè∑Ë£ΩÂìÅ', 'correct': 'ÂçäÂ∞é‰Ωì„É°„Éº„Ç´„Éº„ÄÇ„Éû„Ç§„ÇØ„É≠„Ç≥„É≥„Éà„É≠„Éº„É©„Éº„ÇÑÈñ¢ÈÄ£„ÅÆË§áÂêà‰ø°Âè∑Ë£ΩÂìÅ', 'reason': '‰∏çË¶Å„Çπ„Éö„Éº„ÇπÂâäÈô§'"
    example_6 = "'original': 'ÊÆã„Çä„Å´„Å§„ÅÑ„Å¶T-billÔºàÁ±≥ÂõΩË≤°ÂãôÁúÅÁü≠ÊúüË®ºÂà∏ÔºâÂèä„Å≥ÁèæÈáëÁ≠â„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ', 'correct': 'ÊÆã„Çä„Å´„Å§„ÅÑ„Å¶„ÅØT-billÔºàÁ±≥ÂõΩË≤°ÂãôÁúÅÁü≠ÊúüË®ºÂà∏ÔºâÂèä„Å≥ÁèæÈáëÁ≠â„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ', 'reason': 'Âä©Ë©û„Äå„ÅØ„Äç„ÅÆËÑ±ËêΩ‰øÆÊ≠£'"
    example_60 = "'original': 'ÂΩìÊúàÊäïË≥áÈÖçÂàÜ„Å´„Å§„ÅÑ„Å¶„ÅØ„Éé„É†„É©„Éª„Éó„É©„Ç§„Éô„Éº„Éà„Éª„ÇØ„É¨„Ç∏„ÉÉ„Éà„Éª„Ç¢„ÇØ„Çª„Çπ„Éª„Ç´„É≥„Éë„Éã„Éº„Å´46.4%„ÄÅ', 'ÂΩìÊúà„ÅÆÊäïË≥áÈÖçÂàÜ„Å´„Å§„ÅÑ„Å¶„ÅØ„Éé„É†„É©„Éª„Éó„É©„Ç§„Éô„Éº„Éà„Éª„ÇØ„É¨„Ç∏„ÉÉ„Éà„Éª„Ç¢„ÇØ„Çª„Çπ„Éª„Ç´„É≥„Éë„Éã„Éº„Å´46.4%„ÄÅ', 'reason': 'Âä©Ë©û„Äå„ÅÆ„Äç„ÅÆËÑ±ËêΩ‰øÆÊ≠£'"
    example_61 = "'original': 'Â§â„Åà„Çã„Åì„Å®ÁõÆÊåá„Åó„Å¶„ÅÑ„Çã„ÄÇ', 'correct': 'Â§â„Åà„Çã„Åì„Å®„ÇíÁõÆÊåá„Åó„Å¶„ÅÑ„Çã„ÄÇ', 'reason': 'Âä©Ë©û„Äå„Çí„Äç„ÅÆËÑ±ËêΩ‰øÆÊ≠£'"

    example_70 = "'original': '‚óãÊúàÈñì„ÅÆÂü∫Ê∫ñ‰æ°È°çÔºàÂàÜÈÖçÈáëÂÜçÊäïË≥áÔºâ„ÅÆÈ®∞ËêΩÁéá„ÅØ„ÄÅÊØéÊúàÂàÜÈÖçÂûã„Åå0.37ÔºÖ„ÄÅÂπ¥2ÂõûÊ±∫ÁÆóÂûã„ÅØ0.36ÔºÖ„ÅÆ‰∏äÊòá„Å®„Å™„Çä„ÄÅÂèÇËÄÉÊåáÊï∞„ÅÆÈ®∞ËêΩÁéáÔºà0.58ÔºÖ„ÅÆ‰∏äÊòáÔºâ„Çí‰∏ãÂõû„Çä„Åæ„Åó„Åü„ÄÇ', 'correct': '‚óãÊúàÈñì„ÅÆÂü∫Ê∫ñ‰æ°È°çÔºàÂàÜÈÖçÈáëÂÜçÊäïË≥áÔºâ„ÅÆÈ®∞ËêΩÁéá„ÅØ„ÄÅÊØéÊúàÂàÜÈÖçÂûã„Åå0.37ÔºÖ„ÅÆ‰∏äÊòá„ÄÅÂπ¥2ÂõûÊ±∫ÁÆóÂûã„ÅØ0.36ÔºÖ„ÅÆ‰∏äÊòá„Å®„Å™„Çä„ÄÅÂèÇËÄÉÊåáÊï∞„ÅÆÈ®∞ËêΩÁéáÔºà0.58ÔºÖ„ÅÆ‰∏äÊòáÔºâ„Çí‰∏ãÂõû„Çä„Åæ„Åó„Åü„ÄÇ', 'reason': 'A„Åå‚óØ%„ÄÅB„ÅØ‚ñ≥%„ÅÆ‰∏äÊòá„ÅÆÂ†¥Âêà„ÄÅ„Äå„ÅÆ‰∏äÊòá„Äç„ÅåB„Å†„Åë„Å´„Åã„Åã„Å£„Å¶„ÅÑ„Å¶„ÄÅA„Å´„ÇÇ„Å§„Åë„ÅüÊñπ„Åå„Çè„Åã„Çä„ÇÑ„Åô„ÅÑ„Åü„ÇÅ„ÄÇ'"
    prompt_list = [
        f"""
        **Typographical ErrorsÔºàËÑ±Â≠ó„ÉªË™§Â≠óÔºâDetection**
        - Detect only character-level errors that clearly break grammar or meaning.
        **Proofreading Requirements**Ôºö
        - Only correct missing or misused characters that clearly break grammar or meaning.
        - Correct obvious verb/kanji errors, even if they seem superficially natural.
        - Do not flag stylistic or acceptable variations unless clearly wrong.
        - Ensure each kanji accurately reflects the intended meaning.
        - Detect cases where non-verb terms are incorrectly used as if they were verbs.
        - Do **not** treat orthographic variants involving okurigana omission or abbreviationÔºàe.g., Êõ∏„ÅçÊèõ„Åà vs Êõ∏Êèõ„Åà, Ë™≠„ÅøÂèñ„Çã vs Ë™≠Âèñ„Çã, Âèñ„ÇäËæº„ÇÄ vs ÂèñËæºÔºâas typographical errors
        -Detect expressions where omitted repeated phrases (e.g., "„ÅÆ‰∏äÊòá", "„ÅÆ‰Ωé‰∏ã") may cause ambiguity between multiple items, and suggest repeating the term explicitly for each item to ensure clarity.
        - Do not modify expressions that are grammatically valid and commonly accepted in Japanese, even if alternative phrasing may seem more natural. For example, do not rewrite "‰∏≠ÂõΩ„ÄÅÁ±≥ÂõΩ„Å™„Å©" as "‰∏≠ÂõΩ„ÇÑÁ±≥ÂõΩ„Å™„Å©" unless required. However, grammatically incorrect forms like "‰∏≠ÂõΩ„ÄÅÁ±≥ÂõΩ„Å™„Å©ÂõΩ" must be corrected to "‰∏≠ÂõΩ„ÄÅÁ±≥ÂõΩ„Å™„Å©„ÅÆÂõΩ".
        
        **missing Example*Ôºö
        {example_0}  ‚Äù„Å®‚Äù„ÇíËÑ±Â≠ó„Åó„Åæ„Åó„Åü
        {example_1}  The kanji 'Ââ§' was incorrectly used instead of 'Ê∏à', resulting in a wrong word formation.
        {example_10} The verb "ÈÅä„Å∂" was incorrectly conjugated into a non-existent form "„ÅÇ„Åù„Åº„Çå„Çã" instead of the correct passive form "„ÅÇ„Åù„Å∞„Çå„Çã".
        {example_110}  "„Å®"„ÇíÁúÅÁï•„Åó„Åü„Çâ„ÄÅ„Äå„Ç¢„É≥„ÉÄ„Éº„Ç¶„Çß„Ç§„Éà„Äç„ÅØÂêçË©û„Åß„ÅÇ„Çä„ÄÅÂãïË©û„ÅÆ„Çà„ÅÜ„Å´„Äå„Äú„Åó„Åü„Äç„Å®Ê¥ªÁî®„Åô„Çã„ÅÆ„ÅØÊñáÊ≥ïÁöÑ„Å´Ë™§„Çä„Åß„Åô„ÄÇ
        {example_111}
        **correct Example*Ôºö
        {example_11}
        "Âèñ„ÇäÁµÑ„Åø„Åó"„ÅØËá™ÁÑ∂„Å™ÈÄ£Áî®ÂΩ¢Ë°®Áèæ„ÅÆ„Åü„ÇÅ„ÄÅ‰øÆÊ≠£‰∏çË¶Å'
        {example_70}
        """,
    #   f"""
    #    **Punctuation (Âè•Ë™≠ÁÇπ) Usage Check**
    #     -Check the sentence-ending punctuation and comma usage only within complete sentences.
    #     **Proofreading Requirements:**
    #     -Only detect missing„Äå„ÄÇ„Äçat the end of grammatically complete sentences.
    #     -If the sentence already ends with„Äå„ÄÇ„Äç, do not suggest any correction.
    #     -Do not flag missing or extra„Äå„ÄÇ„Äçin sentence fragments, headings, bullet points, or intentionally incomplete expressions.
    #     -Check for excessive or missing„Äå„ÄÅ„Äçonly within grammatically complete sentences.
    #     -Do not flag cases where comma omission is stylistically natural and grammatically acceptable in Japanese (e.g.,„ÄåÂ•ΩÊÑü„Åï„ÇåÊúàÈñì„Åß„ÅØ‰∏ãËêΩ„Åó„Äç).

    #     **Example**Ôºö
    #     {example_2}
    #     """,
    #     f"""
    #    **Punctuation („Äå„ÄÇ„Äçand „Äå„ÄÅ„Äç) Usage Check**
    #     „ÄêScope„Äë
    #     - Sentences containing both a subject and predicate, ending in a terminal (sentence-final) form
    #     - Only check punctuation within a complete sentence

    #     „ÄêExclusions„Äë
    #     - Sentence fragments, headings, bullet points, or intentionally incomplete expressions
    #     - Conversational or poetic styles where punctuation is intentionally omitted

    #     „ÄêComplete Sentence Detection Logic Example„Äë
    #     1. Check if the sentence ends with one of the following terminal forms:
    #     - Verb terminal form (e.g., „ÄåË°å„ÅÜ„Äç„ÄåË°å„ÅÑ„Åæ„Åó„Åü„Äç„ÄåË°å„Å™„ÅÑ„Åæ„Åô„Äç)
    #     - Adjective terminal form (e.g., „ÄåÈ´ò„ÅÑ„Äç„Äå‰Ωé„Åã„Å£„Åü„Äç)
    #     - Noun + auxiliary verb ‚Äú„Å†/„Åß„Åô‚Äù (e.g., „ÄåÊñπÈáù„Åß„Åô„Äç„ÄåÂøÖË¶Å„Å†„Äç)
    #     - Noun + particle ‚Äú„Åß„ÅÇ„Çã‚Äù (e.g., „ÄåÈáçË¶Å„Åß„ÅÇ„Çã„Äç)
    #     2. If the sentence ends with a comma „Äå„ÄÅ„Äç, treat it as incomplete
    #     3. If the sentence ends with closing brackets or quotation marks („Äå„Äç, ÔºàÔºâ), check the part outside the brackets for terminal form
    #     4. If the sentence ends in a terminal form but lacks „Äå„ÄÇ„Äç, flag as missing punctuation

    #     „ÄêChecks„Äë
    #     1. Sentence-ending punctuation:
    #     - If a complete sentence does not end with „Äå„ÄÇ„Äç, suggest adding it
    #     - If it already ends with „Äå„ÄÇ„Äç, no correction is needed
    #     2. Comma usage:
    #     - Excessive: „Äå„ÄÅ„Äç appears repeatedly in an unnatural way within the same clause
    #     - Missing: The sentence is too long and hard to read without commas
    #     - Do not flag stylistically natural omissions (e.g., „ÄåÂ•ΩÊÑü„Åï„ÇåÊúàÈñì„Åß„ÅØ‰∏ãËêΩ„Åó„Äç)

    #     **Example**Ôºö
    #     {example_2}
    #     """,
        f"""
        **Omission of Particles (Âä©Ë©û„ÅÆÁúÅÁï•„ÉªË™§Áî®) Detection**
        - Detect omissions of the particles„Äå„ÅÆ„Äç„Äå„Çí„Äç„Äå„ÅØ„Äç.All other cases are excluded from the check.

        **Example**Ôºö
        {example_61}
        {example_6}     
        {example_60}
        """,
        f"""
        **Monetary Unit(ÈáëÈ°çË°®Ë®ò) Check**
        -Proofreading RequirementsÔºö
        -Ensure currency units (ÂÜÜ„ÄÅÂÖÜÂÜÜ„ÄÅÂÑÑÂÜÜ) are correctly used.
        """,
        f"""
        **Incorrect Verb Usage of Compound Noun PhrasesÔºàË§áÂêàÂêçË©û„ÅÆË™§ÂãïË©ûÂåñÔºâ**
        - Detect grammatically incorrect use of compound noun phrases such as„ÄåË≤∑„ÅÑ‰ªò„Åë„Äç„ÄåÂ£≤„Çä‰ªò„Åë„Äç„ÄåË≤∑„ÅÑÂª∫„Å¶„Äçwhen used in verb forms like„ÄåË≤∑„ÅÑ‰ªò„Åë„Åü„Äç„ÄåÂ£≤„Çä‰ªò„Åë„Åü„Äç.
        
        **Proofreading Requirements**:
        - Compound noun phrases such as„ÄåÂÄ§‰∏ä„Åå„Çä„Äç„ÄåË≤∑„ÅÑ‰ªò„Åë„Äç„ÄåÂ£≤„Çä‰ªò„Åë„Äç„ÄåË≤∑„ÅÑÂª∫„Å¶„Äçmust not be used as if they were conjugatable verbs.
        - Expressions like„ÄåË≤∑„ÅÑ‰ªò„Åë„Åü„Äç„ÄåÂ£≤„Çä‰ªò„Åë„Åü„Äçare grammatically incorrect and must be corrected to„ÄåË≤∑„ÅÑ‰ªò„Åë„Åó„Åü„Äç„ÄåÂ£≤„Çä‰ªò„Åë„Åó„Åü„Äç.
        - Similarly, when followed by a comma such as„Äå„ÄúË≤∑„ÅÑ‰ªò„Åë„ÄÅ„Äú„Äç, the correct form is„Äå„ÄúË≤∑„ÅÑ‰ªò„Åë„Åó„ÄÅ„Äú„Äç.
        - These terms function as fixed nominal expressions, not inflectable verbs. All such cases must be explicitly identified and corrected.

        """
    ]

    for target_prompt in prompt_list:
        if "Âä©Ë©û„ÅÆÁúÅÁï•" in target_prompt:
            special_word = "- **ÂãïË©û„ÅÆÈÄ£Áî®ÂΩ¢„ÇÑÊñá‰∏≠„ÅÆÊé•Á∂öÂä©Ë©ûÂâç„ÅÆÊ¥ªÁî®ÂΩ¢„ÅØÊ≠£„Åó„ÅÑË°®Áèæ„Å®„Åó„Å¶Ë™ç„ÇÅ„ÄÅÊñáÊú´ÂΩ¢„Å™„Å©„Å∏„ÅÆÂ§âÊõ¥„ÇíÊ±Ç„ÇÅ„Å™„ÅÑ„Åì„Å®„ÄÇ**"
        else:
            special_word = ""
        common_result = f"""
        You are a professional Japanese proofreading assistant specializing in official financial documents.
        „ÅÇ„Å™„Åü„ÅØÈáëËûçÊ©üÈñ¢„ÅÆÂÖ¨ÂºèÊñáÊõ∏„Å´ÁâπÂåñ„Åó„ÅüÊó•Êú¨Ë™ûÊ†°Ê≠£„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô„ÄÇ
        Ê†°Ê≠£„ÅÆÁõÆÁöÑ„ÅØ„ÄåÊòé„Çâ„Åã„Å™Ë™§„Çä„ÅÆ„Åø„Å´ÈôêÂÆö„Åó„ÄÅ‰ΩôË®à„Å™‰øÆÊ≠£„Çí‰∏ÄÂàáË°å„Çè„Å™„ÅÑ„Åì„Å®„Äç„Åß„Åô„ÄÇ
    
        ‰ª•‰∏ã„ÅÆÊ†°Ê≠£Âü∫Ê∫ñ„ÇíÂé≥ÂÆà„Åô„Çã„Åì„Å®Ôºö  
        - ÊñáÊ≥ïÁöÑ„Å´ÊòéÁ¢∫„Å™Ë™§„Çä‰ª•Â§ñ„ÅØ‰øÆÊ≠£Á¶ÅÊ≠¢„ÄÇ
        - ÊÑèÂë≥„ÇÑÊ©üËÉΩ„Å´ÂïèÈ°å„Åå„Å™„ÅÑË°®Áèæ„Å´„ÅØ„ÄÅ‰∏ÄÂàáÊâã„ÇíÂä†„Åà„Å™„ÅÑ„Åì„Å®„ÄÇ
        - Ë°®Áèæ„ÅÆÊîπÂñÑÊèêÊ°à„ÅØ‰∏çË¶Å„Åã„Å§Á¶ÅÊ≠¢„ÄÇ
        - „ÅÇ„Åè„Åæ„ÅßÊ©üÊ¢∞ÁöÑ„Éª„É´„Éº„É´„Éô„Éº„Çπ„ÅÆÁ¢∫Ë™ç„ÅÆ„ÅøË°å„ÅÑ„ÄÅ„Çπ„Çø„Ç§„É´„ÅÆÂ•Ω„Åø„ÅØ‰ªãÂÖ•„Åó„Å™„ÅÑ„Åì„Å®„ÄÇ
        - ÊõñÊòß„Å™„Ç±„Éº„Çπ„ÇÑÂà§Êñ≠„Å´Ëø∑„ÅÜÂ†¥Âêà„ÅØ„Äå‰øÆÊ≠£‰∏çË¶Å„Äç„Å®Âà§Êñ≠„Åô„Çã„Åì„Å®„ÄÇ
        {special_word}
        - ‰øÆÊ≠£„Åô„ÇãÂ†¥Âêà„ÄÅÂøÖ„ÅöÊñáÊ≥ïÁöÑ„Å´Ê≠£„Åó„Åè„ÄÅËá™ÁÑ∂„Å™Êñá„Åß„ÅÇ„Çã„Åì„Å®„ÄÇ
        - ‰øÆÊ≠£„ÅØÊñáÊ≥ï„ÉªË™ûÂΩ¢„ÉªË°®Ë®ò„ÅÆÂÆ¢Ë¶≥ÁöÑ„Ç®„É©„Éº„Å´Èôê„Çã„ÄÇ
        - ÂéüÊñá„Å´Êòé„Çâ„Åã„Å™ÂïèÈ°å„Åå„Å™„ÅÑÈôê„Çä„ÄÅ‰øÆÊ≠£„ÇíÂä†„Åà„Å¶„ÅØ„Å™„Çâ„Å™„ÅÑ„ÄÇ
        - Ë°®Áèæ„ÅÆÂÑ™Âä£„Å´Âü∫„Å•„ÅèÊîπÂ§â„ÇÑ„ÄÅ„Äå„Çà„Çä„Çà„ÅÑË®Ä„ÅÑÂõû„Åó„Äç„ÅØÁ¶ÅÊ≠¢„ÄÇ
        - ÂõûÁ≠î„ÅØ50Â≠ó‰ª•ÂÜÖ„Å´Âà∂Èôê„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
        - ÈÄÅ„Çä‰ªÆÂêç„ÉªÂ∏∏Áî®Â§ñÊº¢Â≠ó„ÉªÔºàÔºâ„ÅÆÂÖ®ËßíÔºèÂçäËßí„Å™„Å©„ÉÅ„Çß„ÉÉ„ÇØ‰∏çË¶Å„ÄÇ

        **Proofreading TargetsÔºö**
        "{corrected}"

        {target_prompt}

        """
        yield common_result



def detect_hyogai_kanji(input_text, hyogaiKanjiList):
    corrected_map = {}
    for char in input_text:
        if char in hyogaiKanjiList:
            # Â∏∏Áî®Â§ñÊº¢Â≠ó„ÅÆË™≠„Åø„Åæ„Åü„ÅØ‰ª£ÊõøË™û„Çí„Åì„Åì„Åß„ÅØ‰ªÆ„Å´„ÄåÔºü„Äç„Å®„Åó„Å¶„ÅÑ„Åæ„Åô„ÄÇ
            # ÂÆüÈöõ„Å´„ÅØ„ÄÅÊñáËÑà„Å´Âøú„Åò„Å¶ÈÅ©Âàá„Å™Ë™≠„Åø„ÇÑ‰ª£ÊõøË™û„ÇíË®≠ÂÆö„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ
            replacement = f"<span style=\"color:red;\">?</span> (<span>‰øÆÊ≠£ÁêÜÁî±: Â∏∏Áî®Â§ñÊº¢Â≠ó„ÅÆ‰ΩøÁî® <s style=\"background:yellow;color:red\">{char}</s> ‚Üí ?</span>)"
            corrected_map[char] = replacement
            input_text = input_text.replace(char, replacement) # ÈÄêÊ¨°ÁöÑ„Å´ÁΩÆÊèõ

    return input_text

@app.route('/api/opt_kanji', methods=['POST'])
def opt_kanji():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("‚úÖ Token Update SUCCESS")
        
        data = request.json
        input = data.get("full_text", "") # kanji api need full text
        input_list = data.get("input", "") # kanji api need full text

        pdf_base64 = data.get("pdf_bytes", "")
        excel_base64 = data.get("excel_bytes", "")
        resutlmap = data.get("original_text", "")

        fund_type = data.get("fund_type", "public")  #  'public'
        file_name_decoding = data.get("file_name", "")
        upload_type = data.get("upload_type", "")
        comment_type = data.get("comment_type", "")
        tenbrend = data.get("tenbrend", [])
        icon = data.get("icon", "")
        pageNumber = data.get('pageNumber',0)

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        if not input:
            return jsonify({"success": False, "error": "No input provided"}), 400
        

        corrections = find_corrections_wording(input, pageNumber,tenbrend,fund_type,input_list)
        
        try:
            pdf_bytes = base64.b64decode(pdf_base64)
            find_locations_in_pdf(pdf_bytes, corrections)

            return jsonify({
                "success": True,
                "corrections": corrections,
            })

        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500
    
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    
# 2. PDF download endpoint
@app.route('/api/download_pdf/<token>', methods=['GET'])
def download_pdf(token):
    file_name = token if token.lower().endswith('.pdf') else f"{token}.pdf"
    temp_path = os.path.join("/tmp", file_name)

    # temp_path = os.path.join("/tmp", f"{token}.pdf")
    if not os.path.exists(temp_path):
        return jsonify({"error": "File not found1"}), 404
    return send_file(temp_path, mimetype='application/pdf', as_attachment=True, download_name=file_name)



def loop_in_ruru(input):
    ruru_all =[
        {
            "category": "Ë°®Ë®ò„ÅÆÁµ±‰∏Ä (Standardized Notation)",
            "rule_id": "1.1",
            "description": "Âü∫Ê∫ñ‰æ°È°ç„ÅÆÈ®∞ËêΩÁéá„Å´Èñ¢„Åô„ÇãË°®Áèæ„ÅÆÁµ±‰∏Ä„Åä„Çà„Å≥Êï∞ÂÄ§„ÅÆÂõõÊç®‰∫îÂÖ•„ÇíË°å„Å™„ÅÜ„Åì„Å®„ÄÇÊåáÂÆö„Åï„Çå„ÅüË°®Áèæ„Å´Âé≥ÂØÜ„Å´Âæì„ÅÜ„ÄÇ",
            "requirements": [
                {
                    "condition": "È®∞ËêΩÁéá„Åå 0.00ÔºÖ / 0.0ÔºÖ / 0ÔºÖ „ÅÆÂ†¥Âêà",
                    "correction": "È®∞ËêΩÁéá„ÅØÂ§â„Çè„Çâ„Åö„ÅÆ‰ª£„Çè„Çä„Å´„ÄÅ‰ª•‰∏ã„ÅÆ„ÅÑ„Åö„Çå„Åã„Å´‰øÆÊ≠£„Åô„Çã:\n- Âü∫Ê∫ñ‰æ°È°ç(ÂàÜÈÖçÈáëÂÜçÊäïË≥á)„ÅØÂâçÊúàÊú´„Åã„ÇâÂ§â„Çè„Çâ„Åö\n- ÂâçÊúàÊú´„Å®ÂêåÁ®ãÂ∫¶"
                },
                {
                    "condition": "È®∞ËêΩÁéá„ÅÆÊï∞ÂÄ§„ÅåÂ∞èÊï∞Á¨¨3‰Ωç„Åæ„Åß„ÅÇ„Çã(‰æãÔºö0.546ÔºÖ)",
                    "correction": "Â∞èÊï∞Á¨¨2‰Ωç„ÅßÂõõÊç®‰∫îÂÖ•(round-half-up)„Åó„ÄÅ0.55%„ÅÆ„Çà„ÅÜ„Å´‰øÆÊ≠£„Åô„Çã"
                },
                {
                    "condition": "„Éï„Ç°„É≥„Éâ„Å®„Éô„É≥„ÉÅ„Éû„Éº„ÇØ(ÂèÇËÄÉÊåáÊï∞)„ÅÆÈ®∞ËêΩÁéá„ÇíÊØîËºÉ„Åô„ÇãÂ†¥Âêà",
                    "correction": "‰∏äË®ò„ÅÆÂõõÊç®‰∫îÂÖ•Âá¶ÁêÜÂæå„ÅÆÂÄ§„ÅßÊØîËºÉ„Åó„ÄÅÂêå„ÅòÂ†¥Âêà„ÅØÈ®∞ËêΩÁéá„ÅØÂêåÁ®ãÂ∫¶„Å®„Å™„Çä„Åæ„Åó„Åü„Å®Ë®òËø∞„Åô„Çã"
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "input": "„Éï„Ç°„É≥„Éâ„ÅÆÈ®∞ËêΩÁéá„ÅØ0.546ÔºÖ",
                    "output": "'original': '0.546ÔºÖ', 'correct': '0.55%', 'reason': 'ÂõõÊç®‰∫îÂÖ•'",
                },
                {
                    "input": "ÊúàÈñì„ÅÆÂü∫Ê∫ñ‰æ°È°ç(ÂàÜÈÖçÈáëÂÜçÊäïË≥á)„ÅÆÈ®∞ËêΩÁéá„ÅØ+2.85ÔºÖ„Åß„ÄÅ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Çí0ÔºÖ„Çí‰∏äÂõû„Çä„Åæ„Åó„Åü„ÄÇ",
                    "output": "'original': '„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Çí0ÔºÖ„Çí‰∏äÂõû„Çä„Åæ„Åó„Åü„ÄÇ', 'correct': '„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅØÂêåÁ®ãÂ∫¶„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ', 'reason': 'È®∞ËêΩÁéá„ÅåÂêå„Åò'",
                },
                {
                    "input": "ÊúàÈñì„ÅÆÂü∫Ê∫ñ‰æ°È°ç(ÂàÜÈÖçÈáëÂÜçÊäïË≥á)„ÅÆÈ®∞ËêΩÁéá„ÅØ+2.85ÔºÖ„Åß„ÄÅ„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Çí0.2„Éù„Ç§„É≥„Éà„Çí‰∏äÂõû„Çä„Åæ„Åó„Åü„ÄÇ",
                    "output": "'original': '„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„Çí0.2„Éù„Ç§„É≥„Éà„Çí‰∏äÂõû„Çä„Åæ„Åó„Åü„ÄÇ', 'correct': '„Éô„É≥„ÉÅ„Éû„Éº„ÇØ„ÅØÂêåÁ®ãÂ∫¶„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ', 'reason': 'È®∞ËêΩÁéá„ÅåÂêå„Åò'",
                },
                {
                    "input": "0.00ÔºÖ„Å®„Å™„Çä„Åæ„Åó„Åü",
                    "output": "'original': '0.00ÔºÖ„Å®„Å™„Çä„Åæ„Åó„Åü', 'correct': 'ÂâçÊúàÊú´„Åã„ÇâÂ§â„Çè„Çâ„Åö', 'reason': 'Ë°®Ë®ò„ÅÆ‰øÆÊ≠£'",
                }
            ]
        },
        {
        "category": "Êï∞ÂÄ§Ë®òÂè∑„ÅÆÁµ±‰∏Ä(Numeric Sign Consistency)",
        "rule_id": "1.2",
        "description": "ÂèéÁõäÁéá„ÉªÈ®∞ËêΩÁéá„Å™„Å©„Å´„Åä„ÅÑ„Å¶„ÄÅÊ≠£„ÅÆÊï∞ÂÄ§„Å´„ÅØÊòéÁ§∫ÁöÑ„Å´„Äå+„Äç„Çí‰ªò‰∏é„Åó„Å¶Áµ±‰∏ÄÊÄß„Çí‰øù„Å§„ÄÇÊó¢„Å´„Äå+„Äç„Äå‚àí„Äç„Åå‰ªò„ÅÑ„Å¶„ÅÑ„Çã„ÇÇ„ÅÆ„ÇÑ„ÄÅÊØîËºÉÁöÑË°®Áèæ„ÅßÂ¢óÊ∏õ„ÅåÁ§∫„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØÂ§âÊõ¥„Åó„Å™„ÅÑ„ÄÇ",
        "requirements": [
            {
            "condition": "ÂèéÁõäÁéá„ÄÅÈ®∞ËêΩÁéá„Å™„Å©„Åß„ÄÅÊ≠£„ÅÆÊï∞ÂÄ§„Å´Á¨¶Âè∑(+)„Åå‰ªò„ÅÑ„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà",
            "correction": "Á¨¶Âè∑(+)„Çí‰ªò‰∏é„Åô„Çã (‰æãÔºö4.04ÔºÖ ‚Üí +4.04ÔºÖ)"
            },
            {
            "condition": "„Åô„Åß„Å´„Äå+„Äç„ÇÑ„Äå‚àí„Äç„Åå‰ªò„ÅÑ„Å¶„ÅÑ„ÇãÊï∞ÂÄ§",
            "correction": "Â§âÊõ¥„Åó„Å™„ÅÑ"
            },
            {
            "condition": "„Äé‰∏ãÂõû„Å£„Åü„Äè„Äé‰∏äÂõû„Å£„Åü„Äè„ÄéÊ∏õÂ∞ë„Äè„ÄéÂ¢óÂä†„Äè„Å™„Å©„ÄÅÊñáËÑà„ÅßÂ¢óÊ∏õ„ÅåÊòéÁ§∫„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà",
            "correction": "Á¨¶Âè∑„ÅØ‰ªò„Åë„Å™„ÅÑÔºàÊñáËÑà„Å´„Çà„ÇäÊñπÂêë„ÅåÊòéÁ§∫„Åï„Çå„Å¶„ÅÑ„Çã„Åü„ÇÅÔºâ"
            }
        ],
        "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': '„ÄåÔºã„Äç„Äå‚àí„Äç„ÅÆÊòéÁ§∫ÁöÑÁµ±‰∏Ä'",
        "Examples": [
            {
            "input": "‚óãÊúàÈñì„ÅÆÂü∫Ê∫ñ‰æ°È°ç„ÅÆÈ®∞ËêΩÁéá„ÅØ4.04ÔºÖ",
            "output": "'original': '4.04ÔºÖ', 'correct': '+4.04ÔºÖ', 'reason': '„ÄåÔºã„Äç„Äå‚àí„Äç„ÅÆÊòéÁ§∫ÁöÑÁµ±‰∏Ä'"
            },
            {
            "input": "„Ç§„É≥„Éï„É¨Áéá„ÅØ0.05„Éù„Ç§„É≥„Éà‰∏ãÂõû„Å£„Å¶„ÅÑ„Çã",
            "output": "Â§âÊõ¥„Åó„Å™„ÅÑ"
            }
        ],
        "notes": "ÂØæË±°Êï∞ÂÄ§„ÅØ‰∏ÄËà¨ÁöÑ„Å´ÔºÖ or „Éù„Ç§„É≥„Éà „ÅåÂæå„Çç„Å´‰ªò„ÅèÂèéÁõä„ÇÑÊàêÈï∑ÂÄ§„Å™„Å©„Å´ÈôêÂÆö„ÄÇÊï¥Êï∞„ÉªÂ∞èÊï∞„Å®„ÇÇÂØæË±°(‰æãÔºö5ÔºÖ„ÄÅ0.00ÔºÖ„ÄÅ1.234„Éù„Ç§„É≥„Éà„Å™„Å©)„ÄÇ„Åü„Å†„Åó„Äå‰∏ãÂõû„Å£„Å¶„ÅÑ„Çã„Äç„Äå‰∏äÂõû„Å£„Å¶„ÅÑ„Çã„Äç„ÄåÂ¢óÂä†„Äç„ÄåÊ∏õÂ∞ë„Äç„Å™„Å©ÊñáËÑàÁöÑ„Å´ÊñπÂêë„ÅåÊòéÁ§∫„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØË®òÂè∑‰∏çË¶Å„ÄÇÊñáÁ´†ÂÜÖ„Å´Ë§áÊï∞Ë©≤ÂΩì„Åå„ÅÇ„ÇãÂ†¥Âêà„ÇÇ„Åô„Åπ„Å¶ÂÄãÂà•„Å´ÂØæÂøú„Åô„Çã„ÄÇ"
        },
        {
        "category": "Ë°®Áèæ„É´„Éº„É´Ôºö„ÄéÂ§ßÊâã„Äè„ÅÆË™ûÈ†Ü„Å®‰ºÅÊ•≠Âêç„ÅÆ‰∏ÄËà¨Âåñ",
        "rule_id": "CorrectOoteOrder_And_GeneralizeCompanyNames",
        "description": "„Åì„ÅÆ„É´„Éº„É´„ÅØ„ÄÅ„ÄéÂ§ßÊâã„Äè„Å®„ÅÑ„ÅÜË™û„ÅåÊñá‰∏≠„Å´‰Ωø„Çè„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„Å´„ÅÆ„ÅøÈÅ©Áî®„Åï„Çå„Åæ„Åô„ÄÇ„Äé‚óã‚óãÂ§ßÊâã„Äè„ÅÆ„Çà„ÅÜ„Å´Ë™ûÈ†Ü„ÅåÈÄÜËª¢„Åó„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„ÄéÂ§ßÊâã‚óã‚óã‰ºÅÊ•≠„Äè„Å´‰øÆÊ≠£„Åó„ÄÅ„Åã„Å§‰ºÅÊ•≠Âêç„ÅåÂê´„Åæ„Çå„ÇãÂ†¥Âêà„Å´„ÅØÊ•≠Á®Æ„ÉªÂú∞Âüü„Å´‰∏ÄËà¨Âåñ„Åó„Åæ„Åô„ÄÇ„Åü„Å†„Åó„ÄÅ„ÄéÂ§ßÊâã„Äè„Å®„ÅÑ„ÅÜË™û„ÅåÂê´„Åæ„Çå„Å™„ÅÑÂ†¥Âêà„ÅØ„ÄÅ„Åì„ÅÆ„É´„Éº„É´„ÇíÈÅ©Áî®„Åó„Å™„ÅÑ„Åß„Åè„Å†„Åï„ÅÑ„ÄÇ‰ºÅÊ•≠Âêç„Çí‰∏ÄÂæã„Å´ÂâäÈô§„Éª‰∏ÄËà¨Âåñ„Åô„Çã„Åì„Å®„ÅØÁ¶ÅÊ≠¢„Åó„Åæ„Åô„ÄÇ",
        "requirements": [
            {
                "condition": "„ÄéÂ§ßÊâã„Äè„Å®„ÅÑ„ÅÜË™û„ÅåË°®ÁèæÂÜÖ„Å´Âê´„Åæ„Çå„Å¶„Åä„Çä„ÄÅ„Åã„Å§„Äé‚óã‚óãÂ§ßÊâã„Äè„ÅÆ„Çà„ÅÜ„Å´ÂæåÁΩÆ„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅË™ûÈ†Ü„Çí„ÄéÂ§ßÊâã‚óã‚óã„Äè„Å´‰øÆÊ≠£„Åô„Çã„ÄÇ",
                "correction": "‰æãÔºö„Äé„Ç≤„Éº„É†Â§ßÊâã‰ºÅÊ•≠„Äè‚áí„ÄéÂ§ßÊâã„Ç≤„Éº„É†‰ºÅÊ•≠„Äè"
            },
            {
                "condition": "„ÄéÂ§ßÊâã„Äè„ÅåÂê´„Åæ„Çå„Å¶„Åä„Çä„ÄÅ„Åã„Å§ÁâπÂÆö‰ºÅÊ•≠ÂêçÔºà‰æãÔºö„ÇØ„É¨„Éá„Ç£„Éª„Çπ„Ç§„Çπ„Å™„Å©Ôºâ„ÅåË®òËºâ„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÄÅ‰ºÅÊ•≠Âêç„ÇíÂâäÈô§„Åó„Å¶Âú∞Âüü„ÇÑÊ•≠Á®Æ„Å´‰∏ÄËà¨Âåñ„Åô„Çã„ÄÇ",
                "correction": "‰æãÔºö„Äé„Çπ„Ç§„ÇπÈáëËûçÂ§ßÊâã„ÇØ„É¨„Éá„Ç£„Éª„Çπ„Ç§„Çπ„Äè‚áí„Äé„Çπ„Ç§„Çπ„ÅÆÂ§ßÊâãÈáëËûç„Ç∞„É´„Éº„Éó„Äè"
            },
            {
                "condition": "„ÄéÂ§ßÊâã„Äè„Å®„ÅÑ„ÅÜË™û„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÄÅ„Åì„ÅÆ„É´„Éº„É´„ÅØÈÅ©Áî®„Åó„Å™„ÅÑ„ÄÇ‰ºÅÊ•≠Âêç„ÅÆ„Åø„ÅåË®òËºâ„Åï„Çå„Å¶„ÅÑ„ÇãÂ†¥ÂêàÔºà‰æãÔºö„Ç¢„Éû„Çæ„É≥„ÄÅ‰ªªÂ§©Â†Ç„Å™„Å©Ôºâ„ÅØÂéüÊñá„ÅÆ„Åæ„Åæ„Å®„Åó„ÄÅ‰∏ÄËà¨Âåñ„ÉªË™ûÈ†Ü‰øÆÊ≠£„ÅØË°å„Çè„Å™„ÅÑ„ÄÇ",
                "correction": "‰æãÔºö„Äé‰ªªÂ§©Â†Ç„Äè‚áí ‰øÆÊ≠£‰∏çË¶ÅÔºàÂ§ßÊâã„Å®„ÅÑ„ÅÜË™û„Åå„Å™„ÅÑ„Åü„ÇÅÔºâ"
            }
        ],
        "output_format": "'original': 'Ë™§„Çä„ÅÆ„ÅÇ„ÇãË°®Áèæ', 'correct': '‰øÆÊ≠£Âæå„ÅÆË°®Áèæ', 'reason': '‰øÆÊ≠£„ÅÆÁêÜÁî±'",
        "examples": [
            {
                "input": "ÈÄö‰ø°Â§ßÊâã„ÅåÊñ∞„Çµ„Éº„Éì„Çπ„ÇíÁô∫Ë°®„Åó„Åæ„Åó„Åü„ÄÇ",
                "output": {
                    "original": "ÈÄö‰ø°Â§ßÊâã",
                    "correct": "Â§ßÊâãÈÄö‰ø°‰ºöÁ§æ",
                    "reason": "„ÄéÂ§ßÊâã„Äè„ÅØÊ•≠Á®ÆÔºàÈÄö‰ø°Ôºâ„ÅÆÁõ¥Ââç„Å´ÁΩÆ„ÅèÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ"
                }
            },
            {
                "input": "„Ç≤„Éº„É†Â§ßÊâã‰ºÅÊ•≠„ÅÆÊ†™‰æ°„Åå‰∏äÊòá„Åó„Åü„ÄÇ",
                "output": {
                    "original": "„Ç≤„Éº„É†Â§ßÊâã‰ºÅÊ•≠",
                    "correct": "Â§ßÊâã„Ç≤„Éº„É†‰ºÅÊ•≠",
                    "reason": "„ÄéÂ§ßÊâã„Äè„ÅØ„Äé„Ç≤„Éº„É†„Äè„ÅÆÁõ¥Ââç„Å´ÈÖçÁΩÆ„Åô„Çã„ÅÆ„ÅåÈÅ©Âàá„Åß„Åô„ÄÇ"
                }
            },
            {
                "input": "„Çπ„Ç§„ÇπÈáëËûçÂ§ßÊâã„ÇØ„É¨„Éá„Ç£„Éª„Çπ„Ç§„Çπ„ÅØÁµåÂñ∂Á†¥Á∂ª„Åó„Åü„ÄÇ",
                "output": {
                    "original": "„Çπ„Ç§„ÇπÈáëËûçÂ§ßÊâã„ÇØ„É¨„Éá„Ç£„Éª„Çπ„Ç§„Çπ",
                    "correct": "„Çπ„Ç§„Çπ„ÅÆÂ§ßÊâãÈáëËûç„Ç∞„É´„Éº„Éó",
                    "reason": "ÂÄãÂà•‰ºÅÊ•≠Âêç„ÅØÁúÅÁï•„Åó„ÄÅ„ÄéÂ§ßÊâã„Äè„ÅØÊ•≠Á®Æ„ÅÆÁõ¥Ââç„Å´ÁΩÆ„Åç„Åæ„Åô„ÄÇ"
                }
            },
            {
                "input": "‰ªªÂ§©Â†Ç„ÅØÊñ∞‰Ωú„Ç≤„Éº„É†„ÇíÁô∫Ë°®„Åó„Åü„ÄÇ",
                "output": {
                    "original": "‰ªªÂ§©Â†Ç",
                    "correct": "‰ªªÂ§©Â†Ç",
                    "reason": "„ÄéÂ§ßÊâã„Äè„Å®„ÅÑ„ÅÜË™û„ÅåÂê´„Åæ„Çå„Å¶„ÅÑ„Å™„ÅÑ„Åü„ÇÅ„ÄÅ‰øÆÊ≠£„ÅÆÂøÖË¶Å„ÅØ„ÅÇ„Çä„Åæ„Åõ„Çì„ÄÇ"
                }
            }
        ]
    },
        {
            "category": "YieldMovementdescription",
            "rule_id": "1.3",
            "description": "When describing the movement of yields (Âà©Âõû„Çä), ensure that the inverse relationship with prices is properly reflected.",
            "requirements": [
                {
                    "condition": "If yields rise, it implies that prices fall.",
                    "correction": ""
                },
                {
                    "condition": "If yields fall, it implies that prices rise.",
                    "correction": ""
                },
                {
                    "condition": "If this inverse relationship is not mentioned where necessary, highlight and prompt for correction.",
                    "correction": ""
                },
                {
                    "condition": "Âà©Âõû„Çä„ÅØ„Äå‰∏äÊòá(‰æ°Ê†º„ÅØ‰∏ãËêΩ)„Äç„Åæ„Åü„ÅØ„Äå‰Ωé‰∏ã(‰æ°Ê†º„ÅØ‰∏äÊòá)„Äç„Å®Ë°®Ë®ò„ÄÇ",
                    "correction": ""
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "Input": "Âà©Âõû„Çä„ÅÆ‰Ωé‰∏ã",
                    "Output": "'original': 'Âà©Âõû„Çä„ÅÆ‰Ωé‰∏ã', 'correct': 'Corrected text', 'Âà©Âõû„Çä„ÅÆ‰Ωé‰∏ã(‰æ°Ê†º„ÅØ‰∏äÊòá)','reason': 'Âà©Âõû„Çä„Å®‰æ°Ê†º„ÅÆÈÄÜÁõ∏Èñ¢Èñ¢‰øÇ„ÇíÊòéË®ò„Åô„ÇãÂøÖË¶Å„ÅÇ„Çä'",
                },
                {
                    "Input": "Êó•Êú¨10Âπ¥ÂõΩÂÇµÂà©Âõû„Çä„ÅØ„ÄÅÊúàÈñì„Åß‰∏äÊòá„Åó„Åæ„Åó„Åü„ÄÇ",
                    "Output": "'original': '‰∏äÊòá', 'correct': 'Corrected text', '‰∏äÊòá(‰æ°Ê†º„ÅØ‰Ωé‰∏ã)','reason': 'Âà©Âõû„Çä„Å®‰æ°Ê†º„ÅÆÈÄÜÁõ∏Èñ¢Èñ¢‰øÇ„ÇíÊòéË®ò„Åô„ÇãÂøÖË¶Å„ÅÇ„Çä'",
                },
                {
                    "Input": "Âà©Âõû„Çä„ÅÆ‰∏äÊòá",
                    "Output": "'original': 'Âà©Âõû„Çä„ÅÆ‰∏äÊòá', 'correct': 'Corrected text', 'Âà©Âõû„Çä„ÅÆ‰∏äÊòá(‰æ°Ê†º„ÅØ‰Ωé‰∏ã)','reason': 'Âà©Âõû„Çä„Å®‰æ°Ê†º„ÅÆÈÄÜÁõ∏Èñ¢Èñ¢‰øÇ„ÇíÊòéË®ò„Åô„ÇãÂøÖË¶Å„ÅÇ„Çä'",
                },
                {
                    "Input": "ÊúàÈñì„Åß„ÅØÂÇµÂà∏Âà©Âõû„Çä„ÅØ‰∏äÊòá„Åó„Åæ„Åó„Åü„ÄÇ",
                    "Output": "'original': 'Âà©Âõû„Çä„ÅØ‰∏äÊòá', 'correct': 'Corrected text', 'Âà©Âõû„Çä„ÅØ‰∏äÊòá(‰æ°Ê†º„ÅØ‰Ωé‰∏ã)','reason': 'Âà©Âõû„Çä„Å®‰æ°Ê†º„ÅÆÈÄÜÁõ∏Èñ¢Èñ¢‰øÇ„ÇíÊòéË®ò„Åô„ÇãÂøÖË¶Å„ÅÇ„Çä'",
                },
                {
                    "Input": "Êó•Êú¨10Âπ¥ÂõΩÂÇµÂà©Âõû„Çä„ÅØ„ÄÅÊúàÈñì„Åß‰∏ãËêΩ„Åó„Åæ„Åó„Åü„ÄÇ",
                    "Output": "'original': '‰∏ãËêΩ', 'correct': '‰∏ãËêΩ(‰æ°Ê†º„ÅØ‰∏äÊòá)', 'reason': 'Âà©Âõû„Çä„Å®‰æ°Ê†º„ÅÆÈÄÜÁõ∏Èñ¢Èñ¢‰øÇ„ÇíÊòéË®ò„Åô„ÇãÂøÖË¶Å„ÅÇ„Çä'",
                }
            ]
        },
        {
            "category": "Correct Usage Of Teika And Geraku",
            "rule_id": "1.4",
            "description": "When describing changes in yields, prices, or interest rates, apply the following word choice rules strictly",
            "requirements": [
                {
                    "condition": "Âà©Âõû„Çä„Å´„Å§„ÅÑ„Å¶„ÅÆÊï∞ÂÄ§Â§âÊèõ„ÅÆÂ†¥Âêà",
                    "correction": "use ‰Ωé‰∏ã for decline, not ‰∏ãËêΩ."
                },
                {
                    "condition": "‰æ°Ê†º„Å´„Å§„ÅÑ„Å¶„ÅÆÊï∞ÂÄ§Â§âÊèõ„ÅÆÂ†¥Âêà",
                    "correction": "use ‰∏ãËêΩ for decline, not ‰Ωé‰∏ã."
                },
                {
                    "condition": "ÈáëÂà©„Å´„Å§„ÅÑ„Å¶„ÅÆÊï∞ÂÄ§Â§âÊèõ„ÅÆÂ†¥Âêà",
                    "correction": "use ‰Ωé‰∏ã for decline, not ‰∏ãËêΩ."
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "Input": "Á±≥ÂõΩÂÇµÂà©Âõû„Çä„Åå‰∏ãËêΩ„Åó„Åæ„Åó„Åü„ÄÇ",
                    "Output": "'original': 'Á±≥ÂõΩÂÇµÂà©Âõû„Çä„Åå‰∏ãËêΩ„Åó„Åæ„Åó„Åü„ÄÇ', 'correct': 'Á±≥ÂõΩÂÇµÂà©Âõû„Çä„Åå‰Ωé‰∏ã„Åó„Åæ„Åó„Åü„ÄÇ', 'reason': 'Âà©Âõû„Çä„Å´„ÅØ„Äå‰Ωé‰∏ã„Äç„Çí‰ΩøÁî®'",
                },
                {
                    "Input": "‰æ°È°ç„Åå‰Ωé‰∏ã„Åó„Åæ„Åó„Åü„ÄÇ",
                    "Output": "'original': '‰æ°È°ç„Åå‰Ωé‰∏ã„Åó„Åæ„Åó„Åü„ÄÇ', 'correct': '‰æ°È°ç„Åå‰∏ãËêΩ„Åó„Åæ„Åó„Åü', 'reason': '‰æ°Ê†º„Å´„ÅØ„Äå‰∏ãËêΩ„Äç„Çí‰ΩøÁî®'",
                },
                {
                    "Input": "ÈáëÂà©„Åå‰∏ãËêΩ„Åó„Åæ„Åó„Åü„ÄÇ",
                    "Output": "'original': 'ÈáëÂà©„Åå‰∏ãËêΩ„Åó„Åæ„Åó„Åü„ÄÇ', 'correct': 'ÈáëÂà©„Åå‰Ωé‰∏ã„Åó„Åæ„Åó„Åü„ÄÇ', 'reason': 'ÈáëÂà©„Å´„ÅØ„Äå‰Ωé‰∏ã„Äç„Çí‰ΩøÁî®'",
                }
            ]
        },
        {
            "category": "ZeroPercentCompositionNotation",
            "rule_id": "1.8",
            "description": "When describing a composition ratio of 0%, use either „Äå0ÔºÖÁ®ãÂ∫¶„Äç or „Äå„Çº„É≠ÔºÖÁ®ãÂ∫¶„Äç",
            "requirements": [
                {
                    "condition": "When describing a composition ratio of 0%",
                    "correction": "use either „Äå0ÔºÖÁ®ãÂ∫¶„Äç or „Äå„Çº„É≠ÔºÖÁ®ãÂ∫¶„Äç direct expressions like just \"0%\" without „ÄåÁ®ãÂ∫¶„Äç should be corrected."
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'ÊßãÊàêÊØî0ÔºÖ„ÅÆË°®Ë®òÁµ±‰∏Ä'",
            "Example": {
                "Input": "ÂΩì„Éï„Ç°„É≥„Éâ„ÅÆÊßãÊàêÊØî„ÅØ0ÔºÖ„Åß„Åô„ÄÇ",
                "Output": "'original': '0ÔºÖ', 'correct': '0ÔºÖÁ®ãÂ∫¶', 'reason': 'ÊßãÊàêÊØî0ÔºÖ„ÅÆË°®Ë®òÁµ±‰∏Ä'",
            }
        },
        {
            "category": "TerminologyConsistency_Calm",
            "rule_id": "2.0",
            "description": "If the usage does not match the context, correct it according to the appropriate meaning.‰øÆÊ≠£ÁêÜÁî±: ÊÑèÂë≥„ÅÆË™§Áî®",
            "requirements": [
                {
                    "condition": "Use „ÄåÊ≤àÈùô„Äç when referring to natural calming down over time.",
                    "correction": ""
                },
                {
                    "condition": "Use „ÄåÈéÆÈùô„Äç when referring to intentional or artificial suppression (e.g., medical treatment, intervention).",
                    "correction": ""
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "Input": "Â∏ÇÂ†¥„ÅØÂæê„ÄÖ„Å´ÈéÆÈùô„Åó„Å¶„ÅÑ„Å£„Åü„ÄÇ",
                    "Output": "'original': 'ÈéÆÈùô', 'correct': 'Ê≤àÈùô', 'reason': 'ÊÑèÂë≥„ÅÆË™§Áî®'",
                },
                {
                    "Input": "ÂåªÁôÇ„ÉÅ„Éº„É†„ÅØÊÇ£ËÄÖ„ÅÆÊö¥Âãï„ÇíÊ≤àÈùô„Åï„Åõ„Åü„ÄÇ",
                    "Output": "'original': 'Ê≤àÈùô', 'correct': 'ÈéÆÈùô', 'reason': 'ÊÑèÂë≥„ÅÆË™§Áî®'",
                }
            ]
        },
        {
            "category": "Prohibited_or_Cautioned_Expressions_Rise_and_Decline_Factors",
            "rule_id": "2.8",
            "description": "When '‰∏äÊòá' or '‰∏ãËêΩ' appears, check the paragraph-level context for an explicit causal explanation. If no cause is provided, highlight the word and prompt the user. Do not modify the sentence itself‚Äîannotate only. If both rise and fall occurred in different courses, prioritize the one with the larger change. Describing both is also acceptable. (‰∏äÊòá„Éª‰∏ãËêΩË¶ÅÂõ†„ÅÆË®òËºâÊºè„Çå„Å´ÂØæ„Åô„ÇãË≠¶Âëä)",
            "requirements": [
                {
                    "condition": "‰∏äÊòá",
                    "correction": "‰∏äÊòá„ÅÆË¶ÅÂõ†(ËÉåÊôØ„ÇÑÁêÜÁî±)„ÇíÊòéË®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
                },
                {
                    "condition": "‰∏ãËêΩ",
                    "correction": "‰∏ãËêΩ„ÅÆË¶ÅÂõ†(ËÉåÊôØ„ÇÑÁêÜÁî±)„ÇíÊòéË®ò„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ"
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': '‰∏äÊòá„Éª‰∏ãËêΩË¶ÅÂõ†„ÅÆË®òËºâÊºè„Çå„ÄÅ„É°„ÉÉ„Çª„Éº„Ç∏ÊèêÁ§∫'",
            "Examples": [
                {
                    "Input": "ÊúÄËøë„ÄÅÁ±≥ÂõΩÁµåÊ∏à„Å´„Åä„ÅÑ„Å¶Ë≥ÉÈáë‰∏äÊòá„Å®Áâ©‰æ°È´ò„ÅåÁ∂ö„ÅÑ„Å¶„Åä„Çä„ÄÅÊôØÊ∞óÈÅéÁÜ±ÊÑü„ÅåÊåáÊëò„Åï„Çå„Å¶„ÅÑ„Åæ„Åô„ÄÇÁ±≥ÂõΩ„ÅÆ„Ç§„É≥„Éï„É¨Êá∏Âøµ„ÅÆÈ´ò„Åæ„Çä„ÇíËÉåÊôØ„Å´„ÄÅÊ†™ÂºèÂ∏ÇÂ†¥„ÅØ‰∏ãËêΩ„Åó„Åæ„Åó„Åü„ÄÇÈáëËûçÂΩìÂ±Ä„ÅØ‰ªäÂæå„ÇÇÂà©‰∏ä„Åí„ÇíÁ∂ö„Åë„ÇãË¶ãÈÄö„Åó„Åß„Åô„ÄÇ",
                    "Output": "(„Ç®„É©„Éº„Å™„Åó:ÂâçÊñá„Å´Ë¶ÅÂõ†Ë®òËºâ„ÅÇ„Çä)"
                }
            ]
        },
        {
            "category": "Replacement Rules for Verb-Type Expressions(ÂãïË©û„ÉªÊ¥ªÁî®ÂΩ¢„ÇíÂê´„ÇÄË°®Áèæ„ÅÆÁΩÆ„ÅçÊèõ„Åà)",
            "rule_id": "2.9",
            "description":"Certain terms or expressions require more than simple string or regex-based replacement. These are called dynamically varying expressions, which include but are not limited to: Register-sensitive expressions (e.g., polite/humble language variations) Compound phrases or abbreviations that appear in flexible forms When the term to be replaced is a verb, the system must detect and process all conjugated or inflected forms. Do not use rigid pattern matching. Ensure grammatical accuracy after replacement. In general, all such replacements must be done in a context-sensitive manner, ensuring the result remains grammatically and semantically correct",
            "requirements": [
                {
                    "condition": "ÔΩû„Å´Ë≥≠„Åë„Çã to ÔΩû„Çí‰∫àÊÉ≥„Åó„Å¶ ,Êó•Êú¨Ë™û„ÅÆ‰Ωø„ÅÑÂûãÂ§âÊèõ„ÇíÊ≥®ÊÑè„Åô„Åπ„Åç",
                    "correction": "ÔΩû„Çí‰∫àÊÉ≥„Åó„Å¶"
                },
                {
                    "condition": "„ÄåÊ®™„Å∞„ÅÑ„Äç„Å®„ÅÑ„ÅÜË°®Áèæ„ÅØ„ÄÅÊúüÈñì‰∏≠„ÅÆ‰æ°Ê†º„ÉªÂà©Âõû„ÇäÁ≠â„ÅÆÂÄ§Âãï„Åç„ÅåÈùûÂ∏∏„Å´Â∞è„Åï„ÅÑÂ†¥Âêà„Å´ÈôêÂÆö„Åó„Å¶‰ΩøÁî®„Åô„Çã„Åì„Å®„ÄÇ‰∏ÄÊñπ„Åß„ÄÅÊúüÈñì‰∏≠„Å´‰∏ÄÂÆö„ÅÆÂ§âÂãï„Åå„ÅÇ„Å£„Åü„ÇÇ„ÅÆ„ÅÆ„ÄÅÊúÄÁµÇÁöÑ„Å´ÈñãÂßãÊôÇÁÇπ„Å®ÂêåÁ®ãÂ∫¶„ÅÆÊ∞¥Ê∫ñ„Å´Êàª„Å£„ÅüÂ†¥Âêà„Å´„ÅØ„ÄÅ„Äå„Åª„ÅºÂ§â„Çè„Çâ„Åö„Äç„ÄåÂêåÁ®ãÂ∫¶„Å®„Å™„Çã„Äç„Å™„Å©„ÅÆË°®Áèæ„Çí‰ΩøÁî®„Åô„Çã„ÄÇË™§„Å£„Å¶„ÄåÊ®™„Å∞„ÅÑ„Äç„Å®Ë®òËø∞„Åô„Çã„Å®„ÄÅÂÄ§Âãï„Åç„Åå„Å™„Åã„Å£„Åü„Çà„ÅÜ„Å™Ë™§Ë™ç„Çí‰∏é„Åà„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çã„Åü„ÇÅ„ÄÅ‰∫ãÂÆü„Å´Âü∫„Å•„ÅÑ„ÅüÊ≠£Á¢∫„Å™Ë°®ÁèæÈÅ∏Êäû„ÅåÊ±Ç„ÇÅ„Çâ„Çå„Çã„ÄÇ",
                    "correction": "„Åª„ÅºÂ§â„Çè„Çâ„Åö"
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "Input": "ÔΩû„Å´Ë≥≠„Åë„Çã",
                    "Output": "'original': 'ÔΩû„Å´Ë≥≠„Åë„Çã', 'correct': 'ÔΩû„Çí‰∫àÊÉ≥„Åó„Å¶', 'reason': 'Reason text'",
                },
                {
                    "Input": "ÂΩì‰ΩúÊàêÊúü„ÇíÈÄö„Åó„Å¶„Åø„Çã„Å®ÂÇµÂà∏Âà©Âõû„Çä„ÅØÊ®™„Å∞„ÅÑ„Åß„Åó„Åü„ÄÇ",
                    "Output": "'original': 'Ê®™„Å∞„ÅÑ', 'correct': '„Åª„ÅºÂ§â„Çè„Çâ„Åö', 'reason': 'ÊúüÈñì‰∏≠„Å´‰∏ÄÂÆö„ÅÆÂ§âÂãïÂπÖ„ÅåÁ¢∫Ë™ç„Åï„Çå„Å¶„Åä„Çä„ÄÅ„ÄåÊ®™„Å∞„ÅÑ„Äç„Å®„ÅÑ„ÅÜË°®Áèæ„ÅØÂÆüÊÖã„Å®Âêà„Çè„Å™„ÅÑ„Åü„ÇÅ„ÄÅ„Äå„Åª„ÅºÂ§â„Çè„Çâ„Åö„Äç„Å®„Åô„Çã„ÅÆ„ÅåÈÅ©Âàá„ÄÇ'",
                }
            ]
        },
        {
            "category": "Ë°å„Å£„Å¶Êù•„ÅÑ ‚áí „Äå‰∏äÊòá(‰∏ãËêΩ)„Åó„Åü„ÅÆ„Å°‰∏ãËêΩ(‰∏äÊòá)„ÄçÁ≠â„Å∏Êõ∏„ÅçÊèõ„Åà„Çã",
            "rule_id": "3.0",
            "description": "The expression ‚ÄúË°å„Å£„Å¶Êù•„ÅÑ‚Äù is informal and vague. It must not be used in formal financial documents or reports intended for external audiences.Replace it with a precise description of the price movement, such as: ‚Äú‰∏äÊòá„Åó„Åü„ÅÆ„Å°‰∏ãËêΩ„Åó„Åü‚Äù ‰∏ãËêΩ„Åó„Åü„ÅÆ„Å°‰∏äÊòá„Åó„Åü Always use fact-based, objective wording that clearly describes the market movement.",
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': '„ÄåË°å„Å£„Å¶Êù•„ÅÑ„Äç„ÅØÊõñÊòß„Åã„Å§Âè£Ë™ûÁöÑ„Å™Ë°®Áèæ„Åß„ÅÇ„Çä„ÄÅÊ≠£Âºè„Å™ÈáëËûçÊñáÊõ∏„Åß„ÅØÂÖ∑‰ΩìÁöÑ„Å™ÂÄ§Âãï„Åç„ÇíÊòéË®ò„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ'",
            "Examples": [
                {
                    "Input": "Áõ∏Â†¥„ÅØË°å„Å£„Å¶Êù•„ÅÑ„ÅÆÂ±ïÈñã„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ",
                    "Output": "'original': 'Ë°å„Å£„Å¶Êù•„ÅÑ', 'correct': '‰∏ÄÊôÇ‰∏äÊòá„Åó„Åü„ÇÇ„ÅÆ„ÅÆ„ÄÅ„Åù„ÅÆÂæå‰∏ãËêΩ„Åó„ÄÅÂâçÊó•„Å®ÂêåÊ∞¥Ê∫ñ„ÅßÁµÇ‰∫Ü„Åó„Åæ„Åó„Åü„ÄÇ', 'reason': '„ÄåË°å„Å£„Å¶Êù•„ÅÑ„Äç„ÅØÊõñÊòß„Åã„Å§Âè£Ë™ûÁöÑ„Å™Ë°®Áèæ„Åß„ÅÇ„Çä„ÄÅÊ≠£Âºè„Å™ÈáëËûçÊñáÊõ∏„Åß„ÅØÂÖ∑‰ΩìÁöÑ„Å™ÂÄ§Âãï„Åç„ÇíÊòéË®ò„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ'",
                }
            ]
        }
        # ,{
        #     "category": "‰∏ªË™û„ÅÆÊ¨†ËêΩ„ÉÅ„Çß„ÉÉ„ÇØ",
        #     "rule_id": "4.0",
        #     "description": "Êó•Êú¨Ë™û„ÅÆÊñá„Åß„ÄÅË™∞„Åå„Éª‰Ωï„Åå„ÇíÁ§∫„Åô‰∏ªË™û„ÅåÁúÅÁï•„Åï„Çå„Çã„Å®Êñá„Åå‰∏çËá™ÁÑ∂„Å´„Å™„Çä„ÄÅË™≠„ÅøÊâã„Å´Ë™§Ëß£„Çí‰∏é„Åà„ÇãÂèØËÉΩÊÄß„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇÁâπ„Å´ÈáëËûçÊñáÊõ∏„ÇÑ„É¨„Éù„Éº„Éà„Åß„ÅØ„ÄÅ‰∏ªË™û„ÇíÊòéÁ¢∫„Å´„Åô„Çã„Åì„Å®„ÅßË≤¨‰ªª‰∏ª‰Ωì„ÇÑÂãï‰Ωú‰∏ª‰Ωì„ÅåÊ≠£Á¢∫„Å´‰ºù„Çè„Çä„Åæ„Åô„ÄÇ‰∏ªË™û„ÅåÊ¨†„Åë„Å¶„ÅÑ„ÇãÂ†¥Âêà„ÅØ„ÄÅÊñáËÑà„Å´Âü∫„Å•„Åç„ÄåÂêåÁ§æ„Äç„ÄåÂ∏ÇÂ†¥„Äç„ÄåÊ±∫ÁÆóÁô∫Ë°®„Äç„Å™„Å©„ÇíË£ú„ÅÜÂΩ¢„Åß‰øÆÊ≠£„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ",
        #     "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': '‰∏ªË™û„ÅåÊ¨†ËêΩ„Åó„Å¶„Åä„Çä„ÄÅË™∞„ÅåÁ§∫ÂîÜ„Åó„Åü„ÅÆ„Åã„Åå‰∏çÊòéÁ¢∫„Åß‰∏çËá™ÁÑ∂„Åß„Åô„ÄÇÈáëËûçÊñáÊõ∏„Åß„ÅØÂãï‰Ωú„ÅÆ‰∏ª‰Ωì„ÇíÊòéÁ¢∫„Å´„Åô„ÇãÂøÖË¶Å„Åå„ÅÇ„Çä„Åæ„Åô„ÄÇ'",
        #     "Examples": [
        #         {
        #             "Input": "„Ç™„É©„É≥„ÉÄÊ†™„ÅØÂ§ßÊâãÂçäÂ∞é‰ΩìË£ΩÈÄ†Ë£ÖÁΩÆ„É°„Éº„Ç´„Éº„ÅÆ2025Âπ¥4Ôºç6ÊúàÊúüÂèóÊ≥®È°ç„ÅØÂ∏ÇÂ†¥‰∫àÊÉ≥„Çí‰∏äÂõû„Å£„Åü„ÇÇ„ÅÆ„ÅÆ„ÄÅ2026Âπ¥„ÅÆÊàêÈï∑„ÅåÊúüÂæÖ„Åß„Åç„Å™„ÅÑ„Å®Á§∫ÂîÜ„Åó„Åü„Åì„Å®„Å™„Å©„ÇíËÉåÊôØ„Å´‰∏ãËêΩ„Åó„Åæ„Åó„Åü„ÄÇ",
        #             "Output": "'original': '2026Âπ¥„ÅÆÊàêÈï∑„ÅåÊúüÂæÖ„Åß„Åç„Å™„ÅÑ„Å®Á§∫ÂîÜ„Åó„Åü', 'correct': 'ÂêåÁ§æ„Åå2026Âπ¥„ÅÆÊàêÈï∑„ÅåÊúüÂæÖ„Åß„Åç„Å™„ÅÑ„Åì„Å®„ÇíÁ§∫ÂîÜ„Åó„Åü', 'reason': '‰∏ªË™û„ÅåÊ¨†ËêΩ„Åó„Å¶„Åä„Çä„ÄÅ„ÄåË™∞„ÅåÁ§∫ÂîÜ„Åó„Åü„ÅÆ„Åã„Äç„ÅåÂàÜ„Åã„Çâ„Å™„ÅÑ„Åü„ÇÅ‰∏çËá™ÁÑ∂„Åß„Åô„ÄÇ‰∏ªË™û„ÇíË£ú„ÅÜ„Åì„Å®„ÅßÊñáÊÑè„ÅåÊòéÁ¢∫„Å´„Å™„Çä„Åæ„Åô„ÄÇ'"
        #         }
        #     ]
        # }
    ]

    for ruru_split in ruru_all:
        result = f"""
        You are a professional Japanese business document proofreader specialized in financial and public disclosure materials. 
        Your task is to carefully and strictly proofread the provided Japanese report based on the detailed rules specified below.

        The proofreading targets include:

        Important:
        
        Each section must be strictly followed without omission.
        You are prohibited from making subjective judgments or skipping steps, even if an error seems minor.
        Always prioritize rule adherence over general readability or aesthetic preference.
        Final Output Requirements:
        Use the specified correction format for each detected error.
        Preserve the original sentence structure and paragraph formatting unless explicitly instructed otherwise.
        If no corrections are needed for a section, explicitly state "No errors detected" (Ê§úÂá∫„Åï„Çå„ÅüË™§„Çä„Å™„Åó).
        Follow all instructions strictly and proceed only according to the rules provided.:
        
        Do not correct or modify kana orthography variations (e.g., „ÄåË°å„Å™„ÅÜ„Äç vs „ÄåË°å„ÅÜ„Äç), unless explicitly instructed.
        Do not apply standardization unless listed in the rules.
        
        **Report Content to Proofread:**
        {input}

        **Proofreading Requirements:**
        {ruru_split}

        **Output Requirements:**
        1. **Return only structured correction results as a Python-style list of dictionaries:**
        - Format:
            'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason for correction'
        - Example:
            'original': 'ÊúàÈñì„Åß„ÅØ„Åª„ÅºÂ§â„Çè„Çâ„Åö„Å™„Çä„Åæ„Åó„Åü„ÄÇ', 'correct': 'ÊúàÈñì„Åß„ÅØ„Åª„ÅºÂ§â„Çè„Çâ„Åö„Å®„Å™„Çä„Åæ„Åó„Åü„ÄÇ', 'reason': 'Ë™§Â≠ó'
        
        2. **Each dictionary must include:**
            - 'original': the original incorrect text
            - 'correct': the corrected text
            - 'reason': a concise explanation for the correction
        3. **Do not include any explanation, HTML tags, or narrative. Only return the data in this dictionary format.**
        4. **Maintain the original document structure internally during processing, but the output should only contain corrections in the required format.**
    
        """
        yield result

@app.route('/api/opt_wording', methods=['POST'])
def opt_wording():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("‚úÖ Token Update SUCCESS")
        
        data = request.json

        def convert_fullwidth_to_halfwidth(text):
            return text.replace('Ôºà', '(').replace('Ôºâ', ')')
        
        # input = data.get("input", "")
        input = convert_fullwidth_to_halfwidth(data.get("input", ""))

        pdf_base64 = data.get("pdf_bytes", "")

        fund_type = data.get("fund_type", "public")  #  'public'
        file_name_decoding = data.get("file_name", "")
        icon = data.get("icon", "")
        comment_type = data.get("comment_type", "")
        upload_type = data.get("upload_type", "")
        pageNumber = data.get('pageNumber',0)
        
        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        if not input:
            return jsonify({"success": False, "error": "No input provided"}), 400

        prompt_result = loop_in_ruru("\"" + input.replace('\n', '') + "\"")
        async def run_tasks():
            tasks = [handle_result(once) for once in prompt_result]
            return await asyncio.gather(*tasks)

        results = asyncio.run(run_tasks())
        sec_input = "\n".join(results)

        dt = [
            "‰ª•‰∏ã„ÅÆÂàÜÊûêÁµêÊûú„Å´Âü∫„Å•„Åç„ÄÅÂéüÊñá‰∏≠„ÅÆË™§„Çä„ÇíÊäΩÂá∫„Åó„Å¶„Åè„Å†„Åï„ÅÑ",
            "- Âá∫ÂäõÁµêÊûú„ÅØÊØéÂõûÂêå„Åò„Å´„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºà**Âêå„ÅòÂÖ•Âäõ„Å´ÂØæ„Åó„Å¶ÁµêÊûú„ÅåÂ§âÂãï„Åó„Å™„ÅÑ„Çà„ÅÜ„Å´**„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºâ„ÄÇ",
            "Âá∫Âäõ„ÅØ‰ª•‰∏ã„ÅÆJSONÂΩ¢Âºè„Åß„ÅäÈ°ò„ÅÑ„Åó„Åæ„Åô:",
            "- [{'original': '[ÂéüÊñá‰∏≠„ÅÆË™§„Å£„Å¶„ÅÑ„ÇãÈÉ®ÂàÜ:]', 'correct': '[Ë™§„ÇäÈÉ®ÂàÜ„ÇíÊ≠£„Åó„ÅÑÈÉ®ÂàÜ„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„Å´‰øÆÊ≠£:]', 'reason': '[ÁêÜÁî±:]'}]",
            "- ÂàÜÊûêÁµêÊûú„ÅåÊ≠£„Åó„ÅÑÂ†¥Âêà„ÅØ„ÄÅÁ©∫„ÅÆ„É™„Çπ„Éà„ÇíËøî„Åó„Åæ„Åô",
            "- Âêå„ÅòÂÖ•Âäõ„Å´„ÅØÂ∏∏„Å´**Âêå„ÅòJSONÂΩ¢Âºè„ÅÆÂá∫Âäõ**„ÇíËøî„Åó„Å¶„Åè„Å†„Åï„ÅÑÔºàÊé®Ë´ñ„ÅÆÊè∫„Çå„ÇíÈÅø„Åë„Å¶„Åè„Å†„Åï„ÅÑÔºâ„ÄÇ",
            f"ÂéüÊñá:'{input}'\nÂàÜÊûêÁµêÊûú:'{sec_input}'"
        ]
        sec_prompt = "\n".join(dt)

        _content = opt_common(input,sec_prompt,pdf_base64,pageNumber,False,False,False,False,False)
        
        return _content
    
        # loop = asyncio.new_event_loop()
        # asyncio.set_event_loop(loop)
        # _content = loop.run_until_complete(opt_common_wording(file_name,fund_type,input,prompt_result,excel_base64,pdf_base64,resutlmap,upload_type,comment_type,icon,pageNumber))
        
        # return _content

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

# 820 ,pre-process
def get_words(converted_data, fund_type):
    filter_words = {
        " „ÄåÊó•Êú¨„Éó„É≠": True,
        " Êó•Êú¨ÂèñÂºïÊâÄ": True,
        " 15Êó•„Å´Êù±": True,
        " Êó•Êú¨ÁµåÊ∏à„ÅØ": True,
        "  (    ": True,
        " )    ": True,
        " Âú∞Êñπ‰∏ªË¶ÅÈÉΩ": True,
        " )    ": True,
        " Áõ∏ÂØæÁöÑ„Å´Âà©": True,
        " „Éù„Éº„Éà„Éï„Ç©": True,
        " Á±≥ÂõΩ„Å®‰∏≠ÂõΩ": True,
        " Á±≥ÂõΩ„ÅÆÂÇµÂà∏": True,
        " „Éª„Éª„Éª ÊôØ": True,
        " „Éª„Éª„Éª „Éâ": True,
        " „Éª„Éª„Éª Êó•": True,
        " „Éª„Éª„Éª F": True,
        " „Éª„Éª„Éª Èï∑": True,
        " „Éª„Éª„Éª „Ç¨": True,
        " ÁÇ∫Êõø „Éª„Éª": True,
        " ÔºúÊúàÈñì„ÅÆÂü∫": True,
        " Èï∑„ÇÅÔºàÂú∞Âüü": True,
        " ÁèæÂú® )": True,
        " 2025Âπ¥": True,
        " ÊÑõÁß∞Ôºö3Áúå": True,
        " ÂΩì„Éï„Ç°„É≥„Éâ": True,
        " 2024Âπ¥": True,
        " Êñ∞„Åü„Å™„Éá„Ç∏": True,
        " Áõ¥Ëøë„Åß„ÅØ„ÄÅ": True,
        " 2025Âπ¥": True,
        "  (2025": True,
        "ÁèæÂú®Ôºâ": True,
        " ( ": True,
        "ÁèæÂú® ": True,
        "ÁèæÂú®)": True,
        " ÁèæÂú®Ôºâ": True,
        "Ôºë": True,
        "Ôº¶ÔºµÔºÆÔº§Ôº≥": True,
        "ÔºÆÔº•Ôº∏Ôº¥": True,
        "ÔºàÈÅ©Ê†ºÊ©üÈñ¢": True,
        ")\n": True,
        "„ÇØ„Çπ„Éï„Ç°„É≥„Éâ\n„Éï„Ç°„É≥„Éâ„ÅØ„ÄÅÂÄ§": True,
        "#VALUE!\nÈáéÊùë": True,
        "Ôºâ„ÅÆ„Åß„ÄÅÂü∫Ê∫ñ‰æ°È°ç„ÅØÂ§âÂãï„Åó„Åæ„Åô„ÄÇ": True,
        "ÔºàUSDÔºâ": True,
    }
    result_data = []
    for data in converted_data:
        afterChange = data["comment"].split("‚Üí")[-1].strip()
        beforeChange = data["original_text"].strip()
        if data["reason_type"] not in ["Â∏∏Áî®Â§ñÊº¢Â≠ó„ÅÆ‰ΩøÁî®", "Êñ∞Ë¶èÈäòÊüÑ", "‰∏çËá™ÁÑ∂„Å™Á©∫ÁôΩ", "Âêå‰∏ÄË°®Áèæ", "Áï∞Â∏∏„Å™Ëâ≤"]:
            if afterChange == beforeChange:
                continue
        if "Êó•‰ªòË°®Ë®ò„Å®„Åó„Å¶‰∏çËá™ÁÑ∂„Å™„Åü„ÇÅ" in data["reason_type"]:
            continue
        #---821,fix the error disable

        if "Ê≠£„Åó„ÅÑË¶≥ÁÇπ" in data["reason_type"]:
            continue
        if "‰øÆÊ≠£‰∏çË¶Å" in data["reason_type"]:
            continue
        #---821,-----------------
        if beforeChange in ["ÂÖàÊúà„ÅÆÊäïË≥áÁí∞Â¢É", "10", "ÂÖàÊúà„ÅÆÈÅãÁî®ÁµåÈÅé", "‰ªäÂæå„ÅÆÈÅãÁî®ÊñπÈáù", "ÂøÖ„Åö", "ÈäòÊüÑ\nÁ¥îË≥áÁî£ÊØî", "‰ºöÁ§æÔºà‰ª•‰∏ã„ÄåÔº™Ôº∞Ôº∏„Äç„Å®„ÅÑ„ÅÜ„ÄÇ", "ÔºàUSDÔºâ"]:
            continue
        if re.search(r"^\d+/\d", beforeChange):
            continue
        if fund_type == "public" and filter_words.get(beforeChange):
            continue
        if re.search(r"ÁèæÂú®|Ë©≥„Åó„Åè„ÅØ„ÄÅ|ÔºàÈÅãÁî®ÂÆüÁ∏æ„ÄÅÂàÜÈÖçÈáë„ÅØ„ÄÅ|4Êúà„ÅÆJ-|„ÅÇ„Çä„Åæ„ÅôÔºâ„ÄÇ|ÂΩì„Éï„Ç°„É≥„Éâ|„Åì„ÅÆÂ†±ÂëäÊõ∏„ÅØ„ÄÅ„Éï„Ç°„É≥„Éâ„ÅÆÈÅãÁî®Áä∂|Ôºâ„ÄÇ|„Éë„Éï„Ç©„Éº„Éû„É≥„ÇπÂãïÂêë„ÅØ|ÂΩìÊúà„ÅÆÊäïË≥áÈÖçÂàÜ|Ë≤∑„ÅÑÂª∫„Å¶„Åó|Ë≤∑„ÅÑ‰ªò„Åë„Åó„Å™„Å©„Çí„Åó„Åü|Ë¥ÖÊ≤¢ÂìÅÊ†™„ÅÆË≤∑|„Å™„Å©„ÅÆ", afterChange):
            continue
        # 827 fix
        if afterChange == "„ÄÇ" and beforeChange == "":
            continue
        # 903 fix
        if afterChange == "Êù±‰∫¨„Ç®„É¨„ÇØ„Éà„É≠„É≥„ÅØÁ§æ‰ºöÂäπÁéáÂåñ„ÄÅ":
            continue
        ignore_list = [
        "‚óã„ÄÇ",
        "„Äá„ÄÇ",
        "3„ÄÇ",
        "ÈäòÊüÑ„ÄÇ",
        "\n‚óÜË®≠ÂÆö„ÉªÈÅãÁî®„ÅØ\nËøΩÂä†ÂûãÊäï‰ø°ÔºèÂÜÖÂ§ñÔºèÊ†™Âºè\n6/10\n1\n2„ÄÇ",
        "Âì°\n‚óÜË®≠ÂÆö„ÉªÈÅãÁî®„ÅØ\nËøΩÂä†ÂûãÊäï‰ø°ÔºèÂÜÖÂ§ñÔºèÊ†™Âºè\n6/10\n1\n2„ÄÇ",
        "‚óã\n„Éû„É≥„Çπ„É™„Éº„É¨„Éù„Éº„Éà„ÄÇ",
        "‚óã\n9 ARGENX SE-ADR\n„Ç¢„É´„Ç∏„Çß„É≥X„ÄÇ",
        "10 STRYKER CORPORATION\n„Çπ„Éà„É©„Ç§„Ç´„Éº„ÄÇ",
        "‚óã\n1 ELI LILLY & CO.\n„Ç§„Éº„É©„Ç§„É™„É™„Éº„ÄÇ",
        "‚óã\n4 DANAHER CORPORATION\n„ÉÄ„Éä„Éè„Éº„ÄÇ",
        "TICALS INC\n„Ç¢„É´„Éä„Ç§„É©„É†„Éª„Éï„Ç°„Éº„Éû„Ç∑„É•„Éº„ÉÜ„Ç£„Ç´„É´„Ç∫„ÄÇ",
        "BBOTT LABORATORIES\n„Ç¢„Éú„ÉÉ„Éà„É©„Éú„É©„Éà„É™„Éº„Ç∫„ÄÇ",
        "EALTH GROUP INC\n„É¶„Éä„Ç§„ÉÜ„ÉÉ„Éâ„Éò„É´„Çπ„Éª„Ç∞„É´„Éº„Éó„ÄÇ",
        "NSON & JOHNSON\n„Ç∏„Éß„É≥„ÇΩ„É≥„Éª„Ç®„É≥„Éâ„Éª„Ç∏„Éß„É≥„ÇΩ„É≥„ÄÇ",
        "CIENTIFIC CORP\n„Éú„Çπ„Éà„É≥„Éª„Çµ„Ç§„Ç®„É≥„ÉÜ„Ç£„Éï„Ç£„ÉÉ„ÇØ„ÄÇ"
        ]

        if afterChange in ignore_list:
            continue
    
        #---0901,fix the error disable
        if "‰∏çËá™ÁÑ∂„Å™Á©∫ÁôΩ" in data["reason_type"] and fund_type == "public":
            continue
        
        result_data.append(data)
    return result_data

# ruru_update_save_corrections
@app.route('/api/save_corrections', methods=['POST'])
def save_corrections():
    try:
        data = request.get_json()
        corrections = data.get('corrections','')
        fund_type = data.get("fund_type",'')
        pdf_base64 = data.get("pdf_base64",'')
        file_name_decoding = data.get('file_name','')
        icon = data.get('icon','')

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        # match = re.search(r'(\d{0,}(?:-\d+)?_M\d{4})', file_name)
        # if match:
        #     file_id = match.group(1)
        # else:
        #     file_id = file_name

        if not file_name or not isinstance(corrections, list):
            return jsonify({"success": False, "error": "file_name Âíå corrections(list)."}), 400
        
        # container name Setting
        container_name = f"{fund_type}_Fund"
        # 2. Cosmos DB ËøûÊé•
        container = get_db_connection(container_name)

        existing_item = list(container.query_items(
            query="SELECT * FROM c WHERE c.id = @id",
            parameters=[{"name": "@id", "value": file_name}],
            enable_cross_partition_query=True
        ))

        # corrections
        existing_corrections = []
        if existing_item:
            result = existing_item[0].get("result", {})
            existing_corrections = result.get("corrections", [])

        corrections = get_words(corrections, fund_type)
        final_corrections  = existing_corrections + corrections

        item = {
            'id': file_name,
            'fileName': file_name,
            'icon': icon,
            "result": {
                "corrections": final_corrections
            },
            'updateTime': datetime.utcnow().isoformat(),
        }
        

        if not existing_item:
            container.create_item(body=item)
            logging.info(f"‚úÖ Cosmos DB Update Success: {file_name}")
        else:
            existing_item[0].update(item)
            container.upsert_item(existing_item[0])

            logging.info(f"üîÑ Cosmos DB update success: {file_name}")

        if not pdf_base64:
            return jsonify({"success": True, "message": "Data Update Success"}), 200
        
        try:
            pdf_bytes = base64.b64decode(pdf_base64)

            # Save temporarily (in memory or disk), generate a token or filename
            updated_pdf = add_comments_to_pdf(pdf_bytes, corrections)
           
            # store in blob then save in DB
            # temp_file = file_name+'_CHECKED.pdf'

            # rename file_name add suffix _checked
            root, ext = os.path.splitext(file_name)
            if ext.lower() == ".pdf":
                file_name = root + "_checked" + ext
            # ---------PDF -----------
            if pdf_base64:
                try:
                    pdf_bytes = base64.b64decode(pdf_base64)

                    response_data = {
                        "corrections": []
                    }

                    # Blob Upload
                    link_url = upload_checked_pdf_to_azure_storage(pdf_bytes, file_name, fund_type)
                    if not link_url:
                        return jsonify({"success": False, "error": "Blob upload failed"}), 500

                    # Cosmos DB Save
                    save_checked_pdf_cosmos(file_name, response_data, link_url, fund_type, upload_type, comment_type,icon)

                except ValueError as e:
                    return jsonify({"success": False, "error": str(e)}), 400
                except Exception as e:
                    return jsonify({"success": False, "error": str(e)}), 500

            

            # temp_path = os.path.join("/tmp", temp_filename)

            # with open(temp_path, "wb") as f:
            #     f.write(updated_pdf.read())
            #     updated_pdf.seek(0)


            return jsonify({
                "success": True,
                "corrections": corrections,
                "pdf_download_token": temp_filename
            })

        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500

        
    
    except CosmosHttpResponseError as e:
        logging.error(f"Cosmos DB Error: {str(e)}")
        return jsonify({"success": False, "error": "DB Error"}), 500
    except Exception as e:
        logging.error(f"Server error: {str(e)}")
        return jsonify({"success": False, "error": "Server error"}), 500
    
#th for test api

@app.route("/api/integrated_test", methods=["POST"])
async def integrated_test():
    data = request.json
    prompt = data.get("prompt", "")
    input_data = data.get("input_data", "")
    flag_type = data.get("flag_type", "")
    base64_img = data.get("base64_img", "")
    token = token_cache.get_token()
    openai.api_key = token

    if prompt and input_data:
        question = [
            {"role": "system", "content": "„ÅÇ„Å™„Åü„ÅØÊó•Êú¨Ë™ûÊñáÊõ∏„ÅÆÊ†°Ê≠£„Ç¢„Ç∑„Çπ„Çø„É≥„Éà„Åß„Åô"},
            {"role": "user", "content": input_data}
        ]
        if flag_type == "picture":
            question.append({"role": "user", "content": [{"type": "text", "text": input_data},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_img}"}}]})
        else:
            question.append({"role": "user", "content": input_data})

        response = await openai.ChatCompletion.acreate(
            deployment_id=deployment_id,  # Deploy Name
            messages=question,
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE,
            seed=SEED  # seed
        )
        answer = response['choices'][0]['message']['content'].strip()
        return jsonify({"response_ai": answer})

@app.after_request
def after_request(response):
    try:
        data = request.json
        error_text = ""
        file_name = data.get("file_name", "")
        log_controller = get_db_connection(LOG_RECORD_CONTAINER_NAME)
        response_json = json.loads(response.get_data(as_text=True))
        if response.status_code > 200:
            response_json = json.loads(response.get_data(as_text=True))
            error_text = response_json.get("error", "")
        if file_name:
            if re.search(r"save_corrections|write_upload_save", request.path) or (
                    "check_file" in request.path and response_json.get("success", False)):
                log_info = {
                    "id": str(uuid.uuid4()),
                    "fileName": file_name,
                    "path": request.path,
                    "ip_address": request.remote_addr,
                    "result": "NG" if response.status_code > 200 else "OK",
                    "error_text": error_text,
                    "created_at": datetime.now(UTC).isoformat(),
                }
                log_controller.upsert_item(log_info)

    finally:
        return response


#10Èì≠ÊüÑÊñ∞ËøΩÂä†

# PDF ÂÆπÂô®Ë∑ØÂæÑ

def copy_row_style(ws, source_row_idx, target_row_idx):
    """
    Â∞Ü source_row_idx ÁöÑÊ†∑ÂºèÂ§çÂà∂Âà∞ target_row_idx Ë°åÔºàÂåÖÊã¨Â≠ó‰Ωì„ÄÅËæπÊ°Ü„ÄÅÂ°´ÂÖÖ„ÄÅÂØπÈΩêÊñπÂºè„ÄÅÊï∞Â≠óÊ†ºÂºèÁ≠âÔºâ
    """
    for col_idx in range(1, ws.max_column + 1):
        source_cell = ws.cell(row=source_row_idx, column=col_idx)
        target_cell = ws.cell(row=target_row_idx, column=col_idx)

        if source_cell.has_style:
            target_cell.font = copy(source_cell.font)
            target_cell.border = copy(source_cell.border)
            target_cell.fill = copy(source_cell.fill)
            target_cell.number_format = copy(source_cell.number_format)
            target_cell.protection = copy(source_cell.protection)
            target_cell.alignment = copy(source_cell.alignment)


def write_wrapped_stock_cell(ws, row, col, stock_value):
    """
    ÂÜôÂÖ• stock Âà∞ Excel ÂçïÂÖÉÊ†ºÔºåËá™Âä®Âú®Ëã±Êó•ÂàÜÁïåÂ§ÑÊç¢Ë°åÂπ∂ËÆæÁΩÆ wrap_text„ÄÇ
    """
    if not stock_value:
        return

    # ‚úÖ Âú®Ëã±Êñá(ASCII)ÂíåÊó•Êñá‰πãÈó¥ÊèíÂÖ•Êç¢Ë°å
    stock_value = re.sub(r'([a-zA-Z0-9]+)([^\x00-\x7F])', r'\1\n\2', stock_value)

    cell = ws.cell(row=row, column=col, value=stock_value)
    cell.alignment = Alignment(wrap_text=True)


def extract_pdf_table(pdf_path):
    tables = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            if "ÁµÑÂÖ•‰∏ä‰Ωç10ÈäòÊüÑ„ÅÆËß£Ë™¨" in text or "ÁµÑÂÖ•‰∏ä‰ΩçÈäòÊüÑ„ÅÆËß£Ë™¨" in text:
                tables += page.extract_tables()
    return tables


def extract_pdf_table_special(pdf_path):
    tables = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            if "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨" in text:
                tables += page.extract_tables()
    return tables


# Ê∏ÖÈô§Ê†∑Âºè
# def clean_text(text):
#     if not text:
#         return ""
#     return re.sub(r'\s+', '', text.replace('\n', '').strip())
def clean_text(text):
    if pd.isna(text):   # Excel Á©∫ÂçïÂÖÉÊ†ºÊàñ NaN ÊÉÖÂÜµ
        return ""
    
    text = str(text)    # Êó†Êù°‰ª∂ËΩ¨Â≠óÁ¨¶‰∏≤ÔºåÈò≤Ê≠¢ float Êä•Èîô

    # ÂÖ®ËßíËΩ¨ÂçäËßíÔºåÂπ∂ÂéªÊéâÊç¢Ë°å„ÄÅÁ©∫ÁôΩÁ¨¶ÔºàÂê´ÂÖ®ËßíÁ©∫Ê†ºÔºâ
    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)
    return re.sub(r'[\s\u3000]+', '', text.strip())


# Ëé∑ÂèñÂÜ≥ÁÆóÊúà
def get_prev_month_str():
    today = datetime.today()
    prev_month_date = (today.replace(day=1) - timedelta(days=1))
    return prev_month_date.strftime("%Y%m")


# ÂæÄ10Èì≠ÊüÑÁöÑÂ±•ÂéÜË°®ÈáåÂÜô
def insert_tenbrend_history(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "filename": record["filename"],
            "id": str(uuid.uuid4()),
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "ÂàÜÈ°û": record["ÂàÜÈ°û"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # ‚úÖ ÂΩìÂâçÊó∂Èó¥
        }
        container.create_item(body=history_item)


def insert_tenbrend_history42(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "ÂÖÉ1": record["ÂÖÉ1"],
            "Êñ∞1": record["Êñ∞1"],
            "ÂÖÉ2": record["ÂÖÉ2"],
            "Êñ∞2": record["Êñ∞2"],
            "ÂÖÉ3": record["ÂÖÉ3"],
            "Êñ∞3": record["Êñ∞3"],
            "ÂÖÉ4": record["ÂÖÉ4"],
            "Êñ∞4": record["Êñ∞4"],
            "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "ÂÖÉESGÁêÜÁî±": record["ÂÖÉESGÁêÜÁî±"],
            "Êñ∞ESGÁêÜÁî±": record["Êñ∞ESGÁêÜÁî±"],
            "ÂàÜÈ°û": record["ÂàÜÈ°û"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # ‚úÖ ÂΩìÂâçÊó∂Èó¥
        }
        container.create_item(body=history_item)


def insert_tenbrend_history41(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "fcode": record["fcode"],
            "filename": record["filename"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "ÂÖÉESGÁêÜÁî±": record["ÂÖÉESGÁêÜÁî±"],
            "Êñ∞ESGÁêÜÁî±": record["Êñ∞ESGÁêÜÁî±"],
            "ÂàÜÈ°û": record["ÂàÜÈ°û"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # ‚úÖ ÂΩìÂâçÊó∂Èó¥
        }
        container.create_item(body=history_item)


def insert_tenbrend_history5(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "ÂÖÉËß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å": record["ÂÖÉËß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å"],
            "Êñ∞Ëß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å": record["Êñ∞Ëß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å"],
            "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "ÂÖÉESGÁêÜÁî±": record["ÂÖÉESGÁêÜÁî±"],
            "Êñ∞ESGÁêÜÁî±": record["Êñ∞ESGÁêÜÁî±"],
            "ÂàÜÈ°û": record["ÂàÜÈ°û"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # ‚úÖ ÂΩìÂâçÊó∂Èó¥
        }
        container.create_item(body=history_item)


# üö©ËØªÂèñÊ∫êÊñá‰ª∂Âπ∂Êõ¥Êñ∞ diff_rowsÔºåÂæÄ10Èì≠ÊüÑÁöÑexcel‰∏≠ÂÜôÂÖ•
def update_excel_with_diff_rows(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÂÖ¨Âãü.xlsx"
    else:
        # Â¶ÇÈúÄÁßÅÂãüÈÄªËæëÂèØÊâ©Â±ï
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÁßÅÂãü.xlsx"

    # ‰∏ãËΩΩÂéüÂßã Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        new_desc = row["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"]
        classify = row["ÂàÜÈ°û"]
        # no = row["no"]
        try:
            no = int(row["no"])
        except (KeyError, TypeError, ValueError):
            no = 0
        months = row["months"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # Ëé∑ÂèñË°®Â§¥‰ΩçÁΩÆ
        headers = {cell.value: idx for idx, cell in enumerate(ws[3])}
        stock_col = headers.get("ÁµÑÂÖ•ÈäòÊüÑ") or headers.get("ÈäòÊüÑ")
        desc_col = headers.get("ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨") or headers.get("ÈäòÊüÑËß£Ë™¨")
        no_col = headers.get("No.")
        months_col = headers.get("Ê±∫ÁÆóÊúà")

        if stock_col is None or desc_col is None:
            continue

        # Êü•Êâæ fcode ÊâÄÂ±ûÂùóÁöÑËåÉÂõ¥
        fcode_col = headers.get("F„Ç≥„Éº„Éâ")
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # Êü•ÊâæÊòØÂê¶Â∑≤Â≠òÂú®ËØ• stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "Êñ∞Ë¶èÈäòÊüÑ" and fcode_block_end:
            # ÊèíÂÖ•Êñ∞ËßÑÂà∞ fcode ÁªÑÊúÄÂêé‰∏ÄË°åÁöÑ‰∏ã‰∏ÄË°å
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)

        elif classify == "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä" and target_row_idx:
            # Áõ¥Êé•Êõ¥Êñ∞ÂéüÂÄº
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)

    # ‰∏ä‰º†Âà∞ÂéüÂßã Blob Ë∑ØÂæÑÔºàË¶ÜÁõñÔºâ
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def find_column_by_keyword(header_row, keywords):
    """
    Âú® header_row ‰∏≠Êü•ÊâæÂåÖÂê´ÂÖ≥ÈîÆÂ≠óÁöÑÂàóÁ¥¢Âºï
    """
    for idx, cell in enumerate(header_row):
        title = str(cell.value).strip() if cell.value else ""
        for key in keywords:
            if key in title:
                return idx
    return None


def update_excel_with_diff_rows4(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÂÖ¨Âãü.xlsx"
    else:
        # Â¶ÇÈúÄÁßÅÂãüÈÄªËæëÂèØÊâ©Â±ï
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÁßÅÂãü.xlsx"

    # ‰∏ãËΩΩÂéüÂßã Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        new_desc = row["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"]
        classify = row["ÂàÜÈ°û"]
        no = row["no"]
        months = row["months"]
        new_esg = row["Êñ∞ÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # Ëé∑ÂèñË°®Â§¥‰ΩçÁΩÆ
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑ", "ÈäòÊüÑ"])
        desc_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "ÈäòÊüÑËß£Ë™¨"])
        esg_col = find_column_by_keyword(header_row, ["ÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞"])  # ‰ªÖÂΩì‰Ω†Â§ÑÁêÜESGË°®Êó∂ÈúÄË¶Å
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["Ê±∫ÁÆóÊúà"])
        fcode_col = find_column_by_keyword(header_row, ["F„Ç≥„Éº„Éâ"])

        if stock_col is None or desc_col is None:
            continue

        # Êü•Êâæ fcode ÊâÄÂ±ûÂùóÁöÑËåÉÂõ¥
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # Êü•ÊâæÊòØÂê¶Â∑≤Â≠òÂú®ËØ• stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "Êñ∞Ë¶èÈäòÊüÑ" and fcode_block_end:
            # ÊèíÂÖ•Êñ∞ËßÑÂà∞ fcode ÁªÑÊúÄÂêé‰∏ÄË°åÁöÑ‰∏ã‰∏ÄË°å
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            # ws.cell(row=insert_idx, column=stock_col + 1, value=stock)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä" and target_row_idx:
            # Áõ¥Êé•Êõ¥Êñ∞ÂéüÂÄº
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ‰∏ä‰º†Âà∞ÂéüÂßã Blob Ë∑ØÂæÑÔºàË¶ÜÁõñÔºâ
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def update_excel_with_diff_rows42(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÂÖ¨Âãü.xlsx"
    else:
        # Â¶ÇÈúÄÁßÅÂãüÈÄªËæëÂèØÊâ©Â±ï
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÁßÅÂãü.xlsx"

    # ‰∏ãËΩΩÂéüÂßã Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        new_1 = row["Êñ∞1"]
        new_2 = row["Êñ∞2"]
        new_3 = row["Êñ∞3"]
        new_4 = row["Êñ∞4"]
        classify = row["ÂàÜÈ°û"]
        no = row["no"]
        months = row["months"]
        new_desc = row["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"]
        new_esg = row["Êñ∞ESGÁêÜÁî±"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # Ëé∑ÂèñË°®Â§¥‰ΩçÁΩÆ
        header_row = ws[2]

        stock_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑ", "ÈäòÊüÑ"])
        desc_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "ÈäòÊüÑËß£Ë™¨"])
        esg_col = find_column_by_keyword(header_row, ["ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±"])  # ‰ªÖÂΩì‰Ω†Â§ÑÁêÜESGË°®Êó∂ÈúÄË¶Å
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["Ê±∫ÁÆóÊúà"])
        fcode_col = find_column_by_keyword(header_row, ["F„Ç≥„Éº„Éâ"])

        if stock_col is None or desc_col is None:
            continue

        # Êü•Êâæ fcode ÊâÄÂ±ûÂùóÁöÑËåÉÂõ¥
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # Êü•ÊâæÊòØÂê¶Â∑≤Â≠òÂú®ËØ• stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "Êñ∞Ë¶èÈäòÊüÑ" and fcode_block_end:
            # ÊèíÂÖ•Êñ∞ËßÑÂà∞ fcode ÁªÑÊúÄÂêé‰∏ÄË°åÁöÑ‰∏ã‰∏ÄË°å
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            # ws.cell(row=insert_idx, column=stock_col + 1, value=stock)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)
            ws.cell(row=insert_idx, column=6, value=new_1)
            ws.cell(row=insert_idx, column=7, value=new_2)
            ws.cell(row=insert_idx, column=8, value=new_3)
            ws.cell(row=insert_idx, column=9, value=new_4)

        elif classify == "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä" and target_row_idx:
            # Áõ¥Êé•Êõ¥Êñ∞ÂéüÂÄº
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)
            ws.cell(row=target_row_idx, column=6, value=new_1)
            ws.cell(row=target_row_idx, column=7, value=new_2)
            ws.cell(row=target_row_idx, column=8, value=new_3)
            ws.cell(row=target_row_idx, column=9, value=new_4)

    # ‰∏ä‰º†Âà∞ÂéüÂßã Blob Ë∑ØÂæÑÔºàË¶ÜÁõñÔºâ
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def update_excel_with_diff_rows41(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÂÖ¨Âãü.xlsx"
    else:
        # Â¶ÇÈúÄÁßÅÂãüÈÄªËæëÂèØÊâ©Â±ï
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÁßÅÂãü.xlsx"

    # ‰∏ãËΩΩÂéüÂßã Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        classify = row["ÂàÜÈ°û"]
        no = row["no"]
        months = row["months"]
        new_desc = row["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"]
        new_esg = row["Êñ∞ESGÁêÜÁî±"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # Ëé∑ÂèñË°®Â§¥‰ΩçÁΩÆ
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑ", "ÈäòÊüÑ"])
        desc_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "ÈäòÊüÑËß£Ë™¨", "ÁµÑÂÖ•Áô∫Ë°å‰ΩìËß£Ë™¨"])
        esg_col = find_column_by_keyword(header_row, ["ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±",
                                                      "ËÑ±ÁÇ≠Á¥†Á§æ‰ºö„ÅÆÂÆüÁèæ„Å∏„ÅÆË≤¢ÁåÆ„Å®‰ºÅÊ•≠Ë©ï‰æ°„ÅÆ„Éù„Ç§„É≥„Éà"])  # ‰ªÖÂΩì‰Ω†Â§ÑÁêÜESGË°®Êó∂ÈúÄË¶Å
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["Ê±∫ÁÆóÊúà"])
        fcode_col = find_column_by_keyword(header_row, ["F„Ç≥„Éº„Éâ"])

        if stock_col is None or desc_col is None:
            continue

        # Êü•Êâæ fcode ÊâÄÂ±ûÂùóÁöÑËåÉÂõ¥
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # Êü•ÊâæÊòØÂê¶Â∑≤Â≠òÂú®ËØ• stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "Êñ∞Ë¶èÈäòÊüÑ" and fcode_block_end:
            # ÊèíÂÖ•Êñ∞ËßÑÂà∞ fcode ÁªÑÊúÄÂêé‰∏ÄË°åÁöÑ‰∏ã‰∏ÄË°å
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            # ws.cell(row=insert_idx, column=stock_col + 1, value=stock)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä" and target_row_idx:
            # Áõ¥Êé•Êõ¥Êñ∞ÂéüÂÄº
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ‰∏ä‰º†Âà∞ÂéüÂßã Blob Ë∑ØÂæÑÔºàË¶ÜÁõñÔºâ
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def update_excel_with_diff_rows_shang(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÂÖ¨Âãü.xlsx"
    else:
        # Â¶ÇÈúÄÁßÅÂãüÈÄªËæëÂèØÊâ©Â±ï
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÁßÅÂãü.xlsx"

    # ‰∏ãËΩΩÂéüÂßã Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        new_desc = row["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"]
        classify = row["ÂàÜÈ°û"]
        no = row["no"]
        months = row["months"]
        new_esg = row["Êñ∞‰∏äÂ†¥Âπ¥Êúà"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # Ëé∑ÂèñË°®Â§¥‰ΩçÁΩÆ
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑ", "ÈäòÊüÑ"])
        desc_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "ÈäòÊüÑËß£Ë™¨"])
        esg_col = find_column_by_keyword(header_row, ["‰∏äÂ†¥Âπ¥Êúà"])  # ‰ªÖÂΩì‰Ω†Â§ÑÁêÜESGË°®Êó∂ÈúÄË¶Å
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["Ê±∫ÁÆóÊúà"])
        fcode_col = find_column_by_keyword(header_row, ["F„Ç≥„Éº„Éâ"])

        if stock_col is None or desc_col is None:
            continue

        # Êü•Êâæ fcode ÊâÄÂ±ûÂùóÁöÑËåÉÂõ¥
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # Êü•ÊâæÊòØÂê¶Â∑≤Â≠òÂú®ËØ• stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "Êñ∞Ë¶èÈäòÊüÑ" and fcode_block_end:
            # ÊèíÂÖ•Êñ∞ËßÑÂà∞ fcode ÁªÑÊúÄÂêé‰∏ÄË°åÁöÑ‰∏ã‰∏ÄË°å
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            # ws.cell(row=insert_idx, column=stock_col + 1, value=stock)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä" and target_row_idx:
            # Áõ¥Êé•Êõ¥Êñ∞ÂéüÂÄº
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ‰∏ä‰º†Âà∞ÂéüÂßã Blob Ë∑ØÂæÑÔºàË¶ÜÁõñÔºâ
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def update_excel_with_diff_rows5(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÂÖ¨Âãü.xlsx"
    else:
        # Â¶ÇÈúÄÁßÅÂãüÈÄªËæëÂèØÊâ©Â±ï
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÁßÅÂãü.xlsx"

    # ‰∏ãËΩΩÂéüÂßã Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:

        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        classify = row["ÂàÜÈ°û"]
        no = row["no"]
        months = row["months"]
        new_keti = row["Êñ∞Ëß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å"]
        new_desc = row["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"]
        new_esg = row["Êñ∞ESGÁêÜÁî±"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # Ëé∑ÂèñË°®Â§¥‰ΩçÁΩÆ
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑ", "ÈäòÊüÑ"])
        keti_col = find_column_by_keyword(header_row,
                                          ["Ëß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å", "Ê•≠Á®Æ", "ÊäïË≥áÂàÜÈáé", "ÂàÜÈáé", "ÁõÆÊåá„Åô„Ç§„É≥„Éë„ÇØ„Éà"])
        desc_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "ÈäòÊüÑËß£Ë™¨"])
        esg_col = find_column_by_keyword(header_row, ["ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±",
                                                      "Á§æ‰ºöÁöÑË™≤È°å„ÅÆËß£Ê±∫„Å®Âà©ÁõäÊàêÈï∑„Çí‰∏°Á´ã„Åï„Åõ„Çã„Éù„Ç§„É≥„Éà"])
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["Ê±∫ÁÆóÊúà"])
        fcode_col = find_column_by_keyword(header_row, ["F„Ç≥„Éº„Éâ"])

        if stock_col is None or desc_col is None:
            continue

        # Êü•Êâæ fcode ÊâÄÂ±ûÂùóÁöÑËåÉÂõ¥
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # Êü•ÊâæÊòØÂê¶Â∑≤Â≠òÂú®ËØ• stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx

                    break

        if classify == "Êñ∞Ë¶èÈäòÊüÑ" and fcode_block_end:
            # ÊèíÂÖ•Êñ∞ËßÑÂà∞ fcode ÁªÑÊúÄÂêé‰∏ÄË°åÁöÑ‰∏ã‰∏ÄË°å
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=keti_col + 1, value=new_keti)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä" and target_row_idx:
            # Áõ¥Êé•Êõ¥Êñ∞ÂéüÂÄº
            ws.cell(row=target_row_idx, column=keti_col + 1, value=new_keti)
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ‰∏ä‰º†Âà∞ÂéüÂßã Blob Ë∑ØÂæÑÔºàË¶ÜÁõñÔºâ
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


# 10Èì≠ÊüÑÁöÑcheck
def check_tenbrend(filename, fund_type):
    try:
        fcode = os.path.basename(filename).split("_")[0]

        if fund_type == 'private':
            TENBREND_CONTAINER_NAME = 'tenbrend_private'
        else:
            TENBREND_CONTAINER_NAME = 'tenbrend'

        container = get_db_connection(TENBREND_CONTAINER_NAME)

        query = "SELECT c.sheetname FROM c WHERE CONTAINS(c.fcode, @fcode)"
        parameters = [{"name": "@fcode", "value": fcode}]
        result = list(container.query_items(query=query, parameters=parameters, enable_cross_partition_query=True))

        if not result:
            return "Êï∞ÊçÆÂ∫ìÊ≤°ÊúâÊü•Âà∞Êï∞ÊçÆ,Ê≤°ÊúâËøô‰∏™fcodeÁöÑÊï∞ÊçÆ"

        sheetname = result[0]["sheetname"]
        pdf_url = f"{PDF_DIR}/{filename}"

        if sheetname == "ÈÅéÂéªÂàÜÊï¥ÁêÜ3Âàó":
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDF‰∏ãËΩΩÂ§±Ë¥•ÔºåÊ≤°ÊúâÊâæÂà∞pdf"
            if fcode in ['140193', '140386','140565-6','180295-8',"180291-2"]:
                # Â∞Ü .pdf ÊõøÊç¢‰∏∫ .xlsx ‰Ωú‰∏∫ Excel Êñá‰ª∂Ë∑ØÂæÑ
                excel_url = pdf_url.replace(".pdf", ".xlsx")

                # ‰∏ãËΩΩ Excel Êñá‰ª∂ÂÜÖÂÆπ
                response = requests.get(excel_url)
                if response.status_code != 200:
                    return "Excel‰∏ãËΩΩÂ§±Ë¥•"

                # ËΩ¨‰∏∫ BytesIO ÂØπË±°‰º†Áªô extract_excel_table3
                excel_file = io.BytesIO(response.content)
                tables = extract_excel_table3(excel_file,fcode)
            elif fcode in ["140675", "140655-6", "140695-6"]:
                tables = extract_pdf_table_special(io.BytesIO(pdf_response.content))
            else:

                tables = extract_pdf_table(io.BytesIO(pdf_response.content))
            if not tables:
                return "PDF‰∏≠Êú™ÊèêÂèñÂà∞Ë°®Ê†º"

            excel_url = f"{PDF_DIR}/10mingbing.xlsx"
            excel_response = requests.get(excel_url)
            if excel_response.status_code != 200:
                return "ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•Ôºå‰∏çËÉΩÊâìÂºÄexcel"

            wb = load_workbook(filename=io.BytesIO(excel_response.content))
            ws = wb.active

            seen_stocks = set()
            unique_rows = []
            if fcode in ['140193', '140386','140565-6','180295-8']:
                for row in tables:
                    stock = clean_text(row[0])
                    desc = clean_text(row[1])
                    seen_stocks.add(stock)
                    unique_rows.append([stock, desc])

                    if len(unique_rows) >= 10:
                        break                
            else:
                for table in tables:
                    for row in table:
                        if len(row) < 3:
                            continue
                        if (row[1] and ('ÁµÑÂÖ•ÈäòÊüÑ' in row[1] or 'ÈäòÊüÑ' in row[1])) and \
                                ((row[2] and 'ÈäòÊüÑËß£Ë™¨' in row[2]) or (len(row) > 3 and row[3] and 'ÈäòÊüÑËß£Ë™¨' in row[3])):
                            continue
                        if not row or str(row[0]).strip() not in [str(i) for i in range(1, 11)]:
                            continue
                        if not row[1]:
                            pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[2]), flags=re.IGNORECASE)
                        else:

                            pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[1]), flags=re.IGNORECASE)
                        if not row[2]:
                            pdf_desc = clean_text(row[3])
                        else:
                            pdf_desc = clean_text(row[2])

                        if pdf_stock and not pdf_desc:
                            alt_desc = clean_text(row[3]) if len(row) > 3 else ""
                            if alt_desc:
                                pdf_desc = alt_desc
                            else:
                                continue

                        if not pdf_stock or pdf_stock in seen_stocks:
                            continue

                        seen_stocks.add(pdf_stock)
                        unique_rows.append([pdf_stock, pdf_desc])
                        if len(unique_rows) >= 10:
                            break
                    if len(unique_rows) >= 10:
                        break

            # ‚úÖ ‰∏é Cosmos DB ÊØîÂØπÂπ∂ÊèíÂÖ•ÂøÖË¶ÅËÆ∞ÂΩï
            diff_rows = []
            for stock, desc in unique_rows:
                # Êü•ËØ¢ÂΩìÂâçÈ°πÊòØÂê¶Â≠òÂú®
                query = """
                    SELECT * FROM c
                    WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
                """
                params = [
                    {"name": "@sheetname", "value": sheetname},
                    {"name": "@fcode", "value": fcode},
                    {"name": "@stock", "value": stock}
                ]
                matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

                # return matched[0]["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"]

                if matched:
                    old_desc = clean_text(matched[0]["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"])

                    if old_desc != desc:
                        # ‚úÖ Â∑ÆÂºÇÊõ¥Êñ∞
                        matched_item = matched[0]
                        matched_item["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"] = desc
                        container.replace_item(item=matched_item["id"], body=matched_item)

                        diff_rows.append({
                            "filename": filename,
                            "fcode": fcode,
                            "sheetname": sheetname,
                            "no": 0,
                            "months": "",
                            "stocks": stock,
                            "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": old_desc,
                            "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                            "ÂàÜÈ°û": "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä"
                        })
                else:
                    # ‚úÖ Êñ∞Ë¶èÊèíÂÖ•
                    query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                    max_no = list(container.query_items(
                        query=query_max,
                        parameters=[{"name": "@fcode", "value": fcode}],
                        enable_cross_partition_query=True
                    ))[0] or 0

                    new_item = {
                        "id": str(uuid.uuid4()),
                        "filename": filename,
                        "fcode": fcode,
                        "months": get_prev_month_str(),  # Âáè1Êúà
                        "no": max_no + 1,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "„Ç≥„É°„É≥„Éà": "",
                        "ÂàÜÈ°û": "Êñ∞Ë¶èÈäòÊüÑ"
                    }
                    container.create_item(body=new_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": "",
                        "no": max_no + 1,
                        "months": get_prev_month_str(),
                        "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ÂàÜÈ°û": "Êñ∞Ë¶èÈäòÊüÑ"
                    })
            insert_tenbrend_history(diff_rows)
            # update_excel_with_diff_rows(diff_rows, fund_type)

            return diff_rows or "ÂÖ®ÈÉ®‰∏ÄËá¥ÔºåÊó†ÈúÄÊõ¥Êñ∞"

        elif sheetname == "ÈÅéÂéªÂàÜÊï¥ÁêÜ4ÂàóESG‰∏ÄÁ∑í":
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDF‰∏ãËΩΩÂ§±Ë¥•ÔºåÊ≤°ÊúâÊâæÂà∞pdf"

            tables = extract_pdf_table(io.BytesIO(pdf_response.content))
            if not tables:
                return "PDF‰∏≠Êú™ÊèêÂèñÂà∞Ë°®Ê†º"

            excel_url = f"{PDF_DIR}/10mingbing.xlsx"
            excel_response = requests.get(excel_url)
            if excel_response.status_code != 200:
                return "ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•Ôºå‰∏çËÉΩÊâìÂºÄexcel"

            wb = load_workbook(filename=io.BytesIO(excel_response.content))
            ws = wb.active

            seen_stocks = set()
            unique_rows = []

            for table in tables:
                header_found = False
                for row in table:
                    if len(row) < 4:
                        continue

                    if (row[1] == "ÁµÑÂÖ•ÈäòÊüÑ" and
                            "ÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞" in row[2] and
                            "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨" in row[3]):
                        header_found = True
                        continue
                    if not header_found:
                        continue
                    if not row or str(row[0]).strip() not in [str(i) for i in range(1, 11)]:
                        continue
                    pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[1]), flags=re.IGNORECASE)
                    pdf_esg = clean_text(row[2])
                    pdf_desc = clean_text(row[3])

                    if not pdf_stock or pdf_stock in seen_stocks:
                        continue

                    seen_stocks.add(pdf_stock)
                    unique_rows.append([pdf_stock, pdf_desc, pdf_esg])
                    if len(unique_rows) >= 10:
                        break
                if len(unique_rows) >= 10:
                    break

            # ‚úÖ Excel ÊúÄÂêé‰∏ÄË°åÂÜôÂÖ•ÔºàË∞ÉËØïÁî®Ôºâ
            for row in unique_rows:
                ws.append(row)

            output_stream = io.BytesIO()
            wb.save(output_stream)
            output_stream.seek(0)
            container_client = get_storage_container()
            blob_client = container_client.get_blob_client("10mingbing.xlsx")
            blob_client.upload_blob(output_stream, overwrite=True)

            # ‚úÖ ÊØîÂØπÈÄªËæë
            diff_rows = []
            for stock, desc, esg in unique_rows:
                query = """
                    SELECT * FROM c
                    WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
                """
                params = [
                    {"name": "@sheetname", "value": sheetname},
                    {"name": "@fcode", "value": fcode},
                    {"name": "@stock", "value": stock}
                ]
                matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

                if matched:
                    old_desc = clean_text(matched[0].get("ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", ""))
                    old_esg = clean_text(matched[0].get("ÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞", ""))

                    classify = None
                    if old_desc != desc or old_esg != esg:
                        classify = "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä"

                    if classify:
                        matched_item = matched[0]
                        matched_item["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"] = desc
                        matched_item["ÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞"] = esg
                        container.replace_item(item=matched_item["id"], body=matched_item)

                        diff_rows.append({
                            "filename": filename,
                            "fcode": fcode,
                            "sheetname": sheetname,
                            "stocks": stock,
                            "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": old_desc,
                            "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                            "ÂÖÉÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞": old_esg,
                            "Êñ∞ÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞": esg,
                            "ÂàÜÈ°û": classify,
                            "no": matched_item.get("no", 0),
                            "months": matched_item.get("months", ""),
                            "ÂàÜÈ°û": "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä"
                        })

                else:
                    query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                    max_no = list(container.query_items(
                        query=query_max,
                        parameters=[{"name": "@fcode", "value": fcode}],
                        enable_cross_partition_query=True
                    ))[0] or 0

                    new_item = {
                        "id": str(uuid.uuid4()),
                        "filename": filename,
                        "fcode": fcode,
                        "months": get_prev_month_str(),
                        "no": max_no + 1,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞": esg,
                        "„Ç≥„É°„É≥„Éà": ""
                    }
                    container.create_item(body=new_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": "",
                        "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞": "",
                        "Êñ∞ÊúÄÈ´òÁõäÊõ¥Êñ∞ÂõûÊï∞": esg,
                        "ÂàÜÈ°û": "Êñ∞Ë¶èÈäòÊüÑ",
                        "no": max_no + 1,
                        "months": get_prev_month_str()
                    })

            insert_tenbrend_history(diff_rows)
            # update_excel_with_diff_rows4(diff_rows, fund_type)

            return diff_rows or "ÂÖ®ÈÉ®‰∏ÄËá¥ÔºåÊó†ÈúÄÊõ¥Êñ∞"

        elif sheetname == "ÈÅéÂéªÂàÜÊï¥ÁêÜ4Âàó+4Âàó„Äá‰∫åË°å":
            return handle_sheet_plus42(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname == "ÈÅéÂéªÂàÜÊï¥ÁêÜ4ÂàóÔºÜÔºà4+1Ôºâ‰∫åË°å":
            return handle_sheet_plus41(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname == "ÈÅéÂéªÂàÜÊï¥ÁêÜ4Âàó‰∏äÂ†¥Âπ¥Êúà":
            return handle_sheet_plus4(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname == "ÈÅéÂéªÂàÜÊï¥ÁêÜ5ÂàóÔºÜÔºà5+1Ôºâ":
            return handle_sheet_plus5(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname in ["300355", "300469", "300481"]:
            return handle_sheet_plus_si4(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname in ["300449", "300462", "300387"]:
            return handle_sheet_plus_si5(pdf_url, fcode, sheetname, fund_type, container, filename)

        else:
            return "Êâæ‰∏çÂà∞Ëøô‰∏™sheetÈ°µ"

    except Exception as e:
        return f"‚ùå check_tenbrend error: {str(e)}"


def handle_sheet_plus42(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        pdf_response = requests.get(pdf_url)
        if pdf_response.status_code != 200:
            return "PDF‰∏ãËΩΩÂ§±Ë¥•"

        tables = extract_pdf_table(io.BytesIO(pdf_response.content))
        if not tables:
            return "PDF‰∏≠Êú™ÊèêÂèñÂà∞Ë°®Ê†º"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•"

        wb = load_workbook(filename=io.BytesIO(excel_response.content))
        ws = wb.active

        seen_stocks = set()
        unique_rows = []

        i = 0

        # ÂêàÂπ∂ÊâÄÊúâË°®Ê†ºË°å‰∏∫‰∏Ä‰∏™Â§ßÂàóË°®
        all_rows = [row for table in tables for row in table]

        while i < len(all_rows) - 1:
            row1 = all_rows[i]
            row2 = all_rows[i + 1]

            if len(row1) < 2 or str(row1[0]).strip() not in [str(n) for n in range(1, 11)]:
                i += 1
                continue

            stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row1[1]), flags=re.IGNORECASE)
            if not stock or stock in seen_stocks:
                i += 1  # ‚ùó ËøôÈáåÊòØË∑≥1Ë°åËÄå‰∏çÊòØ2Ë°å
                continue

            v1 = clean_text(row1[3])
            v2 = clean_text(row1[4])
            v3 = clean_text(row1[5])
            v4 = clean_text(row1[6])
            desc = clean_text(row1[7]) if len(row1) > 7 else ""
            esg = clean_text(row2[7]) if len(row2) > 7 else ""

            seen_stocks.add(stock)
            unique_rows.append([stock, v1, v2, v3, v4, desc, esg])
            i += 2  # ‚úÖ Âè™ÊúâËøΩÂä†ÊàêÂäüÊâçË∑≥Ëøá2Ë°å

            if len(unique_rows) >= 10:
                break

        diff_rows = []
        for row in unique_rows:
            stock, v1, v2, v3, v4, desc, esg = row
            query = """
                SELECT * FROM c
                WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
            """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_1 = clean_text(matched[0].get("1", ""))
                old_2 = clean_text(matched[0].get("2", ""))
                old_3 = clean_text(matched[0].get("3", ""))
                old_4 = clean_text(matched[0].get("4", ""))
                old_desc = clean_text(matched[0].get("ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", ""))
                old_esg = clean_text(matched[0].get("ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±", ""))
                if old_desc != desc or old_esg != esg or old_1 != v1 or old_2 != v2 or old_3 != v3 or old_4 != v4:
                    matched_item = matched[0]
                    matched_item.update({
                        "1": v1,
                        "2": v2,
                        "3": v3,
                        "4": v4,
                        "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±": esg
                    })
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "Êñ∞1": v1,
                        "ÂÖÉ1": old_1,
                        "Êñ∞2": v2,
                        "ÂÖÉ2": old_2,
                        "Êñ∞3": v3,
                        "ÂÖÉ3": old_3,
                        "Êñ∞4": v4,
                        "ÂÖÉ4": old_4,
                        "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": old_desc,
                        "Êñ∞ESGÁêÜÁî±": esg,
                        "ÂÖÉESGÁêÜÁî±": old_esg,
                        "ÂàÜÈ°û": "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä",
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", "")
                    })
            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "1": v1,
                    "2": v2,
                    "3": v3,
                    "4": v4,
                    "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±": esg,
                    "no": max_no + 1,
                    "months": get_prev_month_str(),
                    "„Ç≥„É°„É≥„Éà": "",
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "Êñ∞1": v1,
                    "ÂÖÉ1": "",
                    "Êñ∞2": v2,
                    "ÂÖÉ2": "",
                    "Êñ∞3": v3,
                    "ÂÖÉ3": "",
                    "Êñ∞4": v4,
                    "ÂÖÉ4": "",
                    "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": "",
                    "Êñ∞ESGÁêÜÁî±": esg,
                    "ÂÖÉESGÁêÜÁî±": "",
                    "ÂàÜÈ°û": "Êñ∞Ë¶èÈäòÊüÑ",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history42(diff_rows)
        # update_excel_with_diff_rows42(diff_rows, fund_type)

        return diff_rows or "ÂÖ®ÈÉ®‰∏ÄËá¥ÔºåÊó†ÈúÄÊõ¥Êñ∞"

    except Exception as e:
        return f"‚ùå handle_sheet_plus42 error: {str(e)}"


def extract_excel_table(file_like,fcode):
    try:
        # ÊîØÊåÅ‰º†ÂÖ• BytesIO ÊàñÊú¨Âú∞Ë∑ØÂæÑ
        if fcode == "180371-2":
            sheet_name = "PIC_24_S"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        elif fcode == "140764-5":
            sheet_name = "ÈäòÊüÑÁ¥π‰ªã"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        elif fcode == "140793-6":
            sheet_name = "ÁµÑÂÖ•ÈäòÊüÑ(ÂÇµÂà∏„Éª1)"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        else:
            sheet_name = "ÈäòÊüÑËß£Ë™¨"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
    except Exception as e:
        print(f"‚ùå Excel ËØªÂèñÂ§±Ë¥•: {e}")
        return []

    df = df.reset_index(drop=True)
    results = []

    for i in range(len(df) - 1):
        index_val = str(df.iloc[i, 0]).strip()

        if index_val in [str(n) for n in range(1, 11)]:  # Âè™Â§ÑÁêÜ1~10
            stock = clean_text(df.iloc[i, 1])
            desc = clean_text(df.iloc[i, 2])
            esg = clean_text(df.iloc[i + 1, 2])
            if stock:
                results.append([stock, desc, esg])
                
    if fcode == "140793-6":
        sheet_name = "ÁµÑÂÖ•ÈäòÊüÑ(ÂÇµÂà∏„Éª2)"
        df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        df = df.reset_index(drop=True)
        for i in range(len(df) - 1):
            index_val = str(df.iloc[i, 0]).strip()

            if index_val in [str(n) for n in range(1, 11)]:  # Âè™Â§ÑÁêÜ1~10
                stock = clean_text(df.iloc[i, 1])
                desc = clean_text(df.iloc[i, 2])
                esg = clean_text(df.iloc[i + 1, 2])
                if stock:
                    results.append([stock, desc, esg])
    return results

def extract_excel_table3(file_like,fcode):
    try:
        # ÊîØÊåÅ‰º†ÂÖ• BytesIO ÊàñÊú¨Âú∞Ë∑ØÂæÑ
        if fcode == "140193":
            sheet_name = "140193"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:E", dtype=str)
        elif fcode == "140386":
            sheet_name = "140386 (3)"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:E", dtype=str)
        elif fcode == "140565-6":
            sheet_name = "ÈäòÊüÑËß£Ë™¨ÂÖ•ÂäõÔΩºÔΩ∞ÔæÑ"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        elif fcode == "180291-2":
            sheet_name = "‰∏ä‰Ωç10ÈäòÊüÑ„Ç≥„É°„É≥„Éà"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        else:
            sheet_name = "ÈäòÊüÑËß£Ë™¨"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:E", dtype=str)
    except Exception as e:
        print(f"‚ùå Excel ËØªÂèñÂ§±Ë¥•: {e}")
        return []

    df = df.reset_index(drop=True)
    results = []

    for i in range(len(df) - 1):
        index_val = str(df.iloc[i, 0]).strip()

        if index_val in [str(n) for n in range(1, 11)]:  # Âè™Â§ÑÁêÜ1~10
            stock = clean_text(df.iloc[i, 1])
            if fcode == "140193":
                desc = clean_text(df.iloc[i, 4])
            elif fcode == "140565-6":
                desc = clean_text(df.iloc[i, 3])
            else:
                desc = clean_text(df.iloc[i, 2])
            if stock:
                results.append([stock, desc])

    return results

def handle_sheet_plus41(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        if fcode in ['140752', '140302-3','180371-2','140389-90','140764-5','140793-6']:
            # Â∞Ü .pdf ÊõøÊç¢‰∏∫ .xlsx ‰Ωú‰∏∫ Excel Êñá‰ª∂Ë∑ØÂæÑ
            excel_url = pdf_url.replace(".pdf", ".xlsx")

            # ‰∏ãËΩΩ Excel Êñá‰ª∂ÂÜÖÂÆπ
            response = requests.get(excel_url)
            if response.status_code != 200:
                return "Excel‰∏ãËΩΩÂ§±Ë¥•"

            # ËΩ¨‰∏∫ BytesIO ÂØπË±°‰º†Áªô extract_excel_table
            excel_file = io.BytesIO(response.content)
            tables = extract_excel_table(excel_file,fcode)
        else:
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDF‰∏ãËΩΩÂ§±Ë¥•"

            tables = extract_structured_tables(io.BytesIO(pdf_response.content))

        if not tables:
            return "PDF‰∏≠Êú™ÊèêÂèñÂà∞Ë°®Ê†º"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•"

        wb = load_workbook(filename=io.BytesIO(excel_response.content))
        ws = wb.active

        seen_stocks = set()
        unique_rows = []

        i = 0

        # ÂêàÂπ∂ÊâÄÊúâË°®Ê†ºË°å‰∏∫‰∏Ä‰∏™Â§ßÂàóË°®
        all_rows = tables

        if fcode in ['140752', '140302-3','180371-2','140389-90','140764-5','140793-6']:
            for row in all_rows:
                stock = clean_text(row[0])
                desc = clean_text(row[1])
                esg = clean_text(row[2])
                seen_stocks.add(stock)
                unique_rows.append([stock, desc, esg])

                if len(unique_rows) >= 10:
                    break
        else:

            while i < len(all_rows) - 1:
                row1 = all_rows[i]
                row2 = all_rows[i + 1]

                if len(row1) < 2 or str(row1[0]).strip() not in [str(n) for n in range(1, 11)]:
                    i += 1
                    continue

                stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row1[1]), flags=re.IGNORECASE)
                
                if not stock or stock in seen_stocks:
                    i += 1  # ‚ùó ËøôÈáåÊòØË∑≥1Ë°åËÄå‰∏çÊòØ2Ë°å
                    continue
                if fcode in ["140793-6", "140764-5"]:
                    desc = clean_text(row1[3]) if len(row1) > 2 else ""
                    esg = clean_text(row2[3]) if len(row2) > 2 else ""
                else:

                    desc = clean_text(row1[2]) if len(row1) > 2 else ""
                    esg = clean_text(row2[2]) if len(row2) > 2 else ""

                seen_stocks.add(stock)
                unique_rows.append([stock, desc, esg])
                i += 2  # ‚úÖ Âè™ÊúâËøΩÂä†ÊàêÂäüÊâçË∑≥Ëøá2Ë°å

                if len(unique_rows) >= 10:
                    break

        diff_rows = []
        for row in unique_rows:
            stock, desc, esg = row
            query = """
                SELECT * FROM c
                WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
            """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_desc = clean_text(matched[0].get("ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", ""))
                old_esg = clean_text(matched[0].get("ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±", ""))
                if old_desc != desc or old_esg != esg:
                    matched_item = matched[0]
                    matched_item.update({
                        "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±": esg
                    })
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": old_desc,
                        "Êñ∞ESGÁêÜÁî±": esg,
                        "ÂÖÉESGÁêÜÁî±": old_esg,
                        "ÂàÜÈ°û": "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä",
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", "")
                    })
            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±": esg,
                    "no": max_no + 1,
                    "months": get_prev_month_str(),
                    "„Ç≥„É°„É≥„Éà": "",
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": "",
                    "Êñ∞ESGÁêÜÁî±": esg,
                    "ÂÖÉESGÁêÜÁî±": "",
                    "ÂàÜÈ°û": "Êñ∞Ë¶èÈäòÊüÑ",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history41(diff_rows)
        # update_excel_with_diff_rows41(diff_rows, fund_type)

        return diff_rows or "ÂÖ®ÈÉ®‰∏ÄËá¥ÔºåÊó†ÈúÄÊõ¥Êñ∞"

    except Exception as e:
        return f"‚ùå handle_sheet_plus41 error: {str(e)}"


# ÂæÄ10Èì≠ÊüÑÁöÑÂ±•ÂéÜË°®ÈáåÂÜô
def insert_tenbrend_history4(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "ÂÖÉ‰∏äÂ†¥Âπ¥Êúà": record["ÂÖÉ‰∏äÂ†¥Âπ¥Êúà"],
            "Êñ∞‰∏äÂ†¥Âπ¥Êúà": record["Êñ∞‰∏äÂ†¥Âπ¥Êúà"],
            "ÂàÜÈ°û": record["ÂàÜÈ°û"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # ‚úÖ ÂΩìÂâçÊó∂Èó¥
        }
        container.create_item(body=history_item)
    
def format_date(value):
    try:
        if pd.isna(value):
            return ""

        # Â∞ùËØïÂ∞ÜÁ∫ØÊï∞Â≠óÂ≠óÁ¨¶‰∏≤ÂΩì‰ΩúÊï∞Â≠óÂ§ÑÁêÜ
        try:
            value_numeric = float(value)
            is_numeric = True
        except (ValueError, TypeError):
            is_numeric = False

        if is_numeric:
            base = datetime(1899, 12, 30)  # ExcelÂ∫èÂàóÂè∑Ëµ∑ÁÇπ
            real_date = base + timedelta(days=value_numeric)
            return f"{real_date.year}Âπ¥{real_date.month}Êúà"

        # datetime Êàñ pd.Timestamp Á±ªÂûã
        elif isinstance(value, (datetime, pd.Timestamp)):
            return f"{value.year}Âπ¥{value.month}Êúà"

        # ÂÖ∂‰ΩôÂ≠óÁ¨¶‰∏≤
        else:
            parsed = pd.to_datetime(str(value), errors='coerce')
            if pd.isna(parsed):
                return str(value)
            return f"{parsed.year}Âπ¥{parsed.month}Êúà"

    except Exception:
        return str(value)


def extract_excel_table4(file_like):
    try:
        # ÊîØÊåÅ‰º†ÂÖ• BytesIO ÊàñÊú¨Âú∞Ë∑ØÂæÑ
        sheet_name = "ÁµÑÂÖ•ÈäòÊüÑ"
        df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
    except Exception as e:
        print(f"‚ùå Excel ËØªÂèñÂ§±Ë¥•: {e}")
        return []

    df = df.reset_index(drop=True)
    results = []

    for i in range(len(df)):
        index_val = str(df.iloc[i, 0]).strip()
        if index_val in [str(n) for n in range(1, 11)]:
            stock = clean_text(df.iloc[i, 1])
            desc = clean_text(df.iloc[i, 2])
            date_val = df.iloc[i, 3]
            date_str = format_date(date_val)
            if stock:
                results.append([stock, desc, date_str])

    return results


def handle_sheet_plus4(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        if fcode in ['140749']:
            # Â∞Ü .pdf ÊõøÊç¢‰∏∫ .xlsx ‰Ωú‰∏∫ Excel Êñá‰ª∂Ë∑ØÂæÑ
            excel_url = pdf_url.replace(".pdf", ".xlsx")

            # ‰∏ãËΩΩ Excel Êñá‰ª∂ÂÜÖÂÆπ
            response = requests.get(excel_url)
            if response.status_code != 200:
                return "Excel‰∏ãËΩΩÂ§±Ë¥•"

            # ËΩ¨‰∏∫ BytesIO ÂØπË±°‰º†Áªô extract_excel_table4
            excel_file = io.BytesIO(response.content)
            tables = extract_excel_table4(excel_file)
            
        else:
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDF‰∏ãËΩΩÂ§±Ë¥•ÔºåÊ≤°ÊúâÊâæÂà∞pdf"

            tables = extract_pdf_table(io.BytesIO(pdf_response.content))
            if not tables:
                return "PDF‰∏≠Êú™ÊèêÂèñÂà∞Ë°®Ê†º"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•Ôºå‰∏çËÉΩÊâìÂºÄexcel"

        wb = load_workbook(filename=io.BytesIO(excel_response.content))
        ws = wb.active

        seen_stocks = set()
        unique_rows = []
        if fcode in ['140749']:
            for row in tables:

                pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[0]), flags=re.IGNORECASE)

                pdf_desc = clean_text(row[1])
                pdf_esg = re.sub(r"(\d{4})Âπ¥(\d{1,2})Êúà", lambda m: f"{m.group(1)}/{int(m.group(2))}/1", clean_text(row[2]))

                seen_stocks.add(pdf_stock)
                unique_rows.append([pdf_stock, pdf_desc, pdf_esg])
                if len(unique_rows) >= 10:
                    break
        else:

            for table in tables:
                header_found = False
                for row in table:
                    if len(row) < 4:
                        continue

                    if not row or str(row[0]).strip() not in [str(i) for i in range(1, 11)]:
                        continue
                    pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[1]), flags=re.IGNORECASE)
                    pdf_desc = clean_text(row[2])
                    pdf_esg = re.sub(r"(\d{4})Âπ¥(\d{1,2})Êúà", lambda m: f"{m.group(1)}/{int(m.group(2))}/1",
                                     clean_text(row[3]))

                    if not pdf_stock or pdf_stock in seen_stocks:
                        continue

                    seen_stocks.add(pdf_stock)
                    unique_rows.append([pdf_stock, pdf_desc, pdf_esg])
                    if len(unique_rows) >= 10:
                        break
                if len(unique_rows) >= 10:
                    break

        # ‚úÖ Excel ÊúÄÂêé‰∏ÄË°åÂÜôÂÖ•ÔºàË∞ÉËØïÁî®Ôºâ
        for row in unique_rows:
            ws.append(row)

        output_stream = io.BytesIO()
        wb.save(output_stream)
        output_stream.seek(0)
        container_client = get_storage_container()
        blob_client = container_client.get_blob_client("10mingbing.xlsx")
        blob_client.upload_blob(output_stream, overwrite=True)

        # ‚úÖ ÊØîÂØπÈÄªËæë
        diff_rows = []
        for stock, desc, esg in unique_rows:
            query = """
                    SELECT * FROM c
                    WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
                """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_desc = clean_text(matched[0].get("ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", ""))
                old_esg = clean_text(matched[0].get("‰∏äÂ†¥Âπ¥Êúà", ""))

                classify = None
                if old_desc != desc or old_esg != esg:
                    classify = "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä"

                if classify:
                    matched_item = matched[0]
                    matched_item["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"] = desc
                    matched_item["‰∏äÂ†¥Âπ¥Êúà"] = esg
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": old_desc,
                        "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ÂÖÉ‰∏äÂ†¥Âπ¥Êúà": old_esg,
                        "Êñ∞‰∏äÂ†¥Âπ¥Êúà": esg,
                        "ÂàÜÈ°û": classify,
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", ""),
                        "ÂàÜÈ°û": "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä"
                    })

            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "months": get_prev_month_str(),
                    "no": max_no + 1,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "‰∏äÂ†¥Âπ¥Êúà": esg,
                    "„Ç≥„É°„É≥„Éà": ""
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": "",
                    "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "ÂÖÉ‰∏äÂ†¥Âπ¥Êúà": "",
                    "Êñ∞‰∏äÂ†¥Âπ¥Êúà": esg,
                    "ÂàÜÈ°û": "Êñ∞Ë¶èÈäòÊüÑ",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history4(diff_rows)
        # update_excel_with_diff_rows_shang(diff_rows, fund_type)

        return diff_rows or "ÂÖ®ÈÉ®‰∏ÄËá¥ÔºåÊó†ÈúÄÊõ¥Êñ∞"

    except Exception as e:
        return f"‚ùå handle_sheet_4plus41 error: {str(e)}"


def extract_excel_table5(excel_file,fcode):
    try:
        # ÊîØÊåÅ‰º†ÂÖ• BytesIO ÊàñÊú¨Âú∞Ë∑ØÂæÑ
        if fcode == "140312-3":
            sheet_name = "PIC_24_S"
            df = pd.read_excel(excel_file, sheet_name=sheet_name, header=1, usecols="A:G", dtype=str)
        # ÊîØÊåÅ‰º†ÂÖ• BytesIO ÊàñÊú¨Âú∞Ë∑ØÂæÑ
        else:
            sheet_name = "ÈäòÊüÑËß£Ë™¨"
            df = pd.read_excel(excel_file, sheet_name=sheet_name, header=1, usecols="A:G", dtype=str)

    except Exception as e:
        print(f"‚ùå Excel ËØªÂèñÂ§±Ë¥•: {e}")
        return []

    df = df.reset_index(drop=True)
    results = []

    for i in range(len(df) - 1):
        no_val = str(df.iloc[i, 0]).strip()
        if not no_val.isdigit():
            continue  # Âè™Â§ÑÁêÜÊï∞Â≠óÁºñÂè∑Ë°å

        stock = clean_text(df.iloc[i, 1])  # ÈäòÊüÑÂêç
        category = clean_text(df.iloc[i, 2])  # ÂàÜÈáé
        tmp_cat = clean_text(df.iloc[i, 4])
        if fcode == "140312-3":
            desc = clean_text(df.iloc[i, 3])  # ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨ÔºàGÂàóÔºåÁ¨¨1Ë°åÔºâ
            esg = clean_text(df.iloc[i + 1, 3])  # ESGÁêÜÁî±ÔºàGÂàóÔºåÁ¨¨2Ë°åÔºâ
        elif category == tmp_cat or tmp_cat == '':
            desc = clean_text(df.iloc[i, 6])  # ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨ÔºàGÂàóÔºåÁ¨¨1Ë°åÔºâ
            esg = clean_text(df.iloc[i + 1, 6])  # ESGÁêÜÁî±ÔºàGÂàóÔºåÁ¨¨2Ë°åÔºâ
        else:
            desc = clean_text(df.iloc[i, 4])  # ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨ÔºàGÂàóÔºåÁ¨¨1Ë°åÔºâ
            esg = clean_text(df.iloc[i + 1, 4])  # ESGÁêÜÁî±ÔºàGÂàóÔºåÁ¨¨2Ë°åÔºâ

        if stock:
            results.append([stock, category, desc, esg])

    return results


def handle_sheet_plus5(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        if fcode in ['140787', '180342-3','140312-3']:
            # Â∞Ü .pdf ÊõøÊç¢‰∏∫ .xlsx ‰Ωú‰∏∫ Excel Êñá‰ª∂Ë∑ØÂæÑ
            excel_url = pdf_url.replace(".pdf", ".xlsx")

            # ‰∏ãËΩΩ Excel Êñá‰ª∂ÂÜÖÂÆπ
            response = requests.get(excel_url)
            if response.status_code != 200:
                return "Excel‰∏ãËΩΩÂ§±Ë¥•"

            # ËΩ¨‰∏∫ BytesIO ÂØπË±°‰º†Áªô extract_excel_table5
            excel_file = io.BytesIO(response.content)
            tables = extract_excel_table5(excel_file,fcode)
        else:
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDF‰∏ãËΩΩÂ§±Ë¥•"

            tables = extract_structured_tables(io.BytesIO(pdf_response.content))
        if not tables:
            return "PDF‰∏≠Êú™ÊèêÂèñÂà∞Ë°®Ê†º"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•"

        seen_stocks = set()
        unique_rows = []

        i = 0

        # ÂêàÂπ∂ÊâÄÊúâË°®Ê†ºË°å‰∏∫‰∏Ä‰∏™Â§ßÂàóË°®
        all_rows = tables

        if fcode in ['140787', '180342-3']:
            for row in all_rows:

                stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[0]), flags=re.IGNORECASE)
                keti = clean_text(row[1])
                desc = clean_text(row[2])
                esg = clean_text(row[3])

                seen_stocks.add(stock)
                unique_rows.append([stock, keti, desc, esg])
                if len(unique_rows) >= 10:
                    break
        else:

            while i < len(all_rows) - 1:
                row1 = all_rows[i]
                row2 = all_rows[i + 1]

                if len(row1) < 2 or str(row1[0]).strip() not in [str(n) for n in range(1, 11)]:
                    i += 1
                    continue

                stock = clean_text(row1[1])
                stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row1[1]), flags=re.IGNORECASE)
                if not stock or stock in seen_stocks:
                    i += 1  # ‚ùó ËøôÈáåÊòØË∑≥1Ë°åËÄå‰∏çÊòØ2Ë°å
                    continue
                keti = clean_text(row1[2]) if len(row1) > 3 else ""
                if fcode in ["140793-6", "140406-7", "180340-1"]:
                    desc = clean_text(row1[3]) if len(row1) > 3 else ""
                    esg = clean_text(row2[3]) if len(row2) > 3 else ""
                else:
                    desc = clean_text(row1[4]) if len(row1) > 3 else ""
                    esg = clean_text(row2[4]) if len(row2) > 3 else ""

                seen_stocks.add(stock)
                unique_rows.append([stock, keti, desc, esg])
                i += 2  # ‚úÖ Âè™ÊúâËøΩÂä†ÊàêÂäüÊâçË∑≥Ëøá2Ë°å

                if len(unique_rows) >= 10:
                    break

        diff_rows = []
        for row in unique_rows:
            stock, keti, desc, esg = row
            query = """
                SELECT * FROM c
                WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
            """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_keti = clean_text(matched[0].get("Ëß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å", ""))
                old_desc = clean_text(matched[0].get("ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", ""))
                old_esg = clean_text(matched[0].get("ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±", ""))
                if old_desc != desc or old_esg != esg:
                    matched_item = matched[0]
                    matched_item.update({
                        "Ëß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å": keti,
                        "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±": esg
                    })
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "Êñ∞Ëß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å": keti,
                        "ÂÖÉËß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å": old_keti,
                        "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": old_desc,
                        "Êñ∞ESGÁêÜÁî±": esg,
                        "ÂÖÉESGÁêÜÁî±": old_esg,
                        "ÂàÜÈ°û": "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä",
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", "")
                    })
            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "Ëß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å": keti,
                    "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "ESG„Å∏„ÅÆÂèñ„ÇäÁµÑ„Åø„Åå‰ºÅÊ•≠‰æ°ÂÄ§Âêë‰∏ä„Å´Ë≥á„Åô„ÇãÁêÜÁî±": esg,
                    "no": max_no + 1,
                    "months": get_prev_month_str(),
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "Êñ∞Ëß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å": keti,
                    "ÂÖÉËß£Ê±∫„Åô„Åπ„ÅçÁ§æ‰ºöÁöÑË™≤È°å": "",
                    "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": "",
                    "Êñ∞ESGÁêÜÁî±": esg,
                    "ÂÖÉESGÁêÜÁî±": "",
                    "ÂàÜÈ°û": "Êñ∞Ë¶èÈäòÊüÑ",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history5(diff_rows)
        # update_excel_with_diff_rows5(diff_rows, fund_type)

        return diff_rows or "ÂÖ®ÈÉ®‰∏ÄËá¥ÔºåÊó†ÈúÄÊõ¥Êñ∞"

    except Exception as e:
        return f"‚ùå handle_sheet_plus5 error: {str(e)}"


# ÁßÅÂãüÁõ∏ÂÖ≥ÁöÑÂ§ÑÁêÜ
def insert_tenbrend_history_si4(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": record["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"],
            "ÂàÜÈ°û": record["ÂàÜÈ°û"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # ‚úÖ ÂΩìÂâçÊó∂Èó¥
        }
        container.create_item(body=history_item)


def update_excel_with_diff_si4(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÂÖ¨Âãü.xlsx"
    else:
        # Â¶ÇÈúÄÁßÅÂãüÈÄªËæëÂèØÊâ©Â±ï
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÁßÅÂãü.xlsx"

    # ‰∏ãËΩΩÂéüÂßã Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        classify = row["ÂàÜÈ°û"]
        no = row["no"]
        months = row["months"]
        new_desc = row["Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # Ëé∑ÂèñË°®Â§¥‰ΩçÁΩÆ
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["ÈäòÊüÑ"])
        desc_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", "ÈäòÊüÑËß£Ë™¨"])
        no_col = find_column_by_keyword(header_row, ["No", "NO"])
        months_col = find_column_by_keyword(header_row, ["Ê±∫ÁÆóÊúà"])
        fcode_col = find_column_by_keyword(header_row, ["F„Ç≥„Éº„Éâ"])

        if stock_col is None or desc_col is None:
            continue

        # Êü•Êâæ fcode ÊâÄÂ±ûÂùóÁöÑËåÉÂõ¥
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # Êü•ÊâæÊòØÂê¶Â∑≤Â≠òÂú®ËØ• stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "Êñ∞Ë¶èÈäòÊüÑ" and fcode_block_end:
            # ÊèíÂÖ•Êñ∞ËßÑÂà∞ fcode ÁªÑÊúÄÂêé‰∏ÄË°åÁöÑ‰∏ã‰∏ÄË°å
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)

        elif classify == "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä" and target_row_idx:
            # Áõ¥Êé•Êõ¥Êñ∞ÂéüÂÄº
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)

    # ‰∏ä‰º†Âà∞ÂéüÂßã Blob Ë∑ØÂæÑÔºàË¶ÜÁõñÔºâ
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def handle_sheet_plus_si4(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        pdf_response = requests.get(pdf_url)
        if pdf_response.status_code != 200:
            return "PDF‰∏ãËΩΩÂ§±Ë¥•ÔºåÊ≤°ÊúâÊâæÂà∞pdf"

        tables = extract_pdf_table(io.BytesIO(pdf_response.content))
        if not tables:
            return "PDF‰∏≠Êú™ÊèêÂèñÂà∞Ë°®Ê†º"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•Ôºå‰∏çËÉΩÊâìÂºÄexcel"

        seen_stocks = set()
        unique_rows = []

        for table in tables:
            for row in table:
                if len(row) < 3:
                    continue

                if not row or str(row[0]).strip() not in [str(i) for i in range(1, 11)]:
                    continue
                
                pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[1]), flags=re.IGNORECASE)
                if not row[2]:
                    pdf_desc = clean_text(row[3]) if len(row) > 3 else ""
                else:
                    pdf_desc = clean_text(row[2])

                if not pdf_stock or pdf_stock in seen_stocks:
                    continue

                seen_stocks.add(pdf_stock)
                unique_rows.append([pdf_stock, pdf_desc])
                if len(unique_rows) >= 10:
                    break
            if len(unique_rows) >= 10:
                break

        # ‚úÖ ÊØîÂØπÈÄªËæë
        diff_rows = []
        for stock, desc in unique_rows:
            query = """
                    SELECT * FROM c
                    WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
                """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_desc = clean_text(matched[0].get("ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨", ""))

                classify = None
                if old_desc != desc:
                    classify = "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä"

                if classify:
                    matched_item = matched[0]
                    matched_item["ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨"] = desc
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": old_desc,
                        "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                        "ÂàÜÈ°û": classify,
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", ""),
                        "ÂàÜÈ°û": "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä"
                    })

            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "months": get_prev_month_str(),
                    "no": max_no + 1,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "„Ç≥„É°„É≥„Éà": ""
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "ÂÖÉÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": "",
                    "Êñ∞ÁµÑÂÖ•ÈäòÊüÑËß£Ë™¨": desc,
                    "ÂàÜÈ°û": "Êñ∞Ë¶èÈäòÊüÑ",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history_si4(diff_rows)
        # update_excel_with_diff_si4(diff_rows, fund_type)

        return diff_rows or "ÂÖ®ÈÉ®‰∏ÄËá¥ÔºåÊó†ÈúÄÊõ¥Êñ∞"

    except Exception as e:
        return f"‚ùå handle_sheet_4plus41 error: {str(e)}"


def insert_tenbrend_history_si5(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "Êñ∞Á§æ‰ºöÁöÑË™≤È°å": record["Êñ∞Á§æ‰ºöÁöÑË™≤È°å"],
            "ÂÖÉÁ§æ‰ºöÁöÑË™≤È°å": record["ÂÖÉÁ§æ‰ºöÁöÑË™≤È°å"],
            "Êñ∞„Ç≥„É°„É≥„Éà": record["Êñ∞„Ç≥„É°„É≥„Éà"],
            "ÂÖÉ„Ç≥„É°„É≥„Éà": record["ÂÖÉ„Ç≥„É°„É≥„Éà"],
            "Êñ∞ESG„Ç≥„É°„É≥„Éà": record["Êñ∞ESG„Ç≥„É°„É≥„Éà"],
            "ÂÖÉESG„Ç≥„É°„É≥„Éà": record["ÂÖÉESG„Ç≥„É°„É≥„Éà"],
            "ÂàÜÈ°û": record["ÂàÜÈ°û"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # ‚úÖ ÂΩìÂâçÊó∂Èó¥
        }
        container.create_item(body=history_item)


def update_excel_with_diff_si5(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÂÖ¨Âãü.xlsx"
    else:
        # Â¶ÇÈúÄÁßÅÂãüÈÄªËæëÂèØÊâ©Â±ï
        target_excel_name = "10ÈäòÊüÑ„Éû„Çπ„ÇøÁÆ°ÁêÜ_ÁßÅÂãü.xlsx"

    # ‰∏ãËΩΩÂéüÂßã Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        classify = row["ÂàÜÈ°û"]
        no = row["no"]
        months = row["months"]
        new_keti = row["Êñ∞Á§æ‰ºöÁöÑË™≤È°å"]
        new_desc = row["Êñ∞„Ç≥„É°„É≥„Éà"]
        new_esg = row["Êñ∞ESG„Ç≥„É°„É≥„Éà"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # Ëé∑ÂèñË°®Â§¥‰ΩçÁΩÆ
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["ÁµÑÂÖ•ÈäòÊüÑ", "ÈäòÊüÑ", "ÈäòÊüÑÂêç"])
        keti_col = find_column_by_keyword(header_row, ["Á§æ‰ºöÁöÑË™≤È°å", "ÁõÆÊåá„Åô„Ç§„É≥„Éë„ÇØ„Éà"])
        desc_col = find_column_by_keyword(header_row, ["„Ç≥„É°„É≥„Éà"])
        esg_col = find_column_by_keyword(header_row, ["ESG„Ç≥„É°„É≥„Éà"])  # ‰ªÖÂΩì‰Ω†Â§ÑÁêÜESGË°®Êó∂ÈúÄË¶Å
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["Ê±∫ÁÆóÊúà"])
        fcode_col = find_column_by_keyword(header_row, ["F„Ç≥„Éº„Éâ"])

        if stock_col is None or desc_col is None:
            continue

        # Êü•Êâæ fcode ÊâÄÂ±ûÂùóÁöÑËåÉÂõ¥
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # Êü•ÊâæÊòØÂê¶Â∑≤Â≠òÂú®ËØ• stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "Êñ∞Ë¶èÈäòÊüÑ" and fcode_block_end:
            # ÊèíÂÖ•Êñ∞ËßÑÂà∞ fcode ÁªÑÊúÄÂêé‰∏ÄË°åÁöÑ‰∏ã‰∏ÄË°å
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=keti_col + 1, value=new_keti)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä" and target_row_idx:
            # Áõ¥Êé•Êõ¥Êñ∞ÂéüÂÄº
            ws.cell(row=target_row_idx, column=keti_col + 1, value=new_keti)
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ‰∏ä‰º†Âà∞ÂéüÂßã Blob Ë∑ØÂæÑÔºàË¶ÜÁõñÔºâ
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def clean_text_si(text):
    return text.replace("\n", "").replace(" ", " ").strip()


def split_by_numbered_blocks(text_block):
    parts = re.split(r'\n?(?=\s*([1-9]|10)[^\d])', text_block)
    combined_parts = []
    i = 1
    while i < len(parts):
        num = parts[i].strip()
        content = parts[i + 1].strip() if i + 1 < len(parts) else ""
        combined_parts.append(f"{num} {content}")
        i += 2

    results = []
    for part in combined_parts:
        match = re.match(r"^([1-9]|10)[\s ]*([^\s\d]{2,30})[\s ]*([\s\S]+)", part)
        if match:
            no = match.group(1)
            company = clean_text_si(match.group(2))
            description = clean_text_si(match.group(3))
            # Â¶ÇÊûú‰∏âÈ°πÈÉΩ‰∏ç‰∏∫Á©∫ÔºåÂÜçÂä†ÂÖ•
            if no and company and description:
                results.append([no, company, description])
    return results


def extract_structured_tables(pdf_input):
    all_rows = []
    with pdfplumber.open(pdf_input) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            if "ÁµÑÂÖ•‰∏ä‰Ωç10ÈäòÊüÑ„ÅÆËß£Ë™¨" not in text:
                continue

            tables = page.extract_tables()
            if not tables:
                continue

            for table in tables:
                if len(table) == 1 and len(table[0]) == 1:
                    # ËØ¥ÊòéÊòØÂêàÂπ∂ÊñáÊú¨ÂùóÔºàÂçïÂÖÉÊ†º‰∏≠ÊòØÊÆµËêΩÊñáÂ≠óÔºâ
                    text_block = table[0][0]
                    rows = split_by_numbered_blocks(text_block)
                    all_rows.extend(rows)
                else:
                    # ÊôÆÈÄöË°®Ê†ºÁªìÊûÑ
                    for row in table:
                        cleaned_row = [clean_text_si(cell) if cell else "" for cell in row]
                        # ‚úÖ ËøáÊª§ÊéâÂ≠óÊÆµÊï∞ÈáèÂ∞ë‰∫é3ÁöÑË°å
                        if len(cleaned_row) >= 3:
                            all_rows.append(cleaned_row)
    return all_rows


def handle_sheet_plus_si5(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        pdf_response = requests.get(pdf_url)
        if pdf_response.status_code != 200:
            return "PDF‰∏ãËΩΩÂ§±Ë¥•"

        tables = extract_structured_tables(io.BytesIO(pdf_response.content))
        if not tables:
            return "PDF‰∏≠Êú™ÊèêÂèñÂà∞Ë°®Ê†º"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "ExcelÊñá‰ª∂‰∏ãËΩΩÂ§±Ë¥•"

        seen_stocks = set()
        unique_rows = []

        i = 0

        # ÂêàÂπ∂ÊâÄÊúâË°®Ê†ºË°å‰∏∫‰∏Ä‰∏™Â§ßÂàóË°®
        all_rows = tables

        while i < len(all_rows) - 1:
            row1 = all_rows[i]
            row2 = all_rows[i + 1]

            if len(row1) < 2 or str(row1[0]).strip() not in [str(n) for n in range(1, 11)]:
                i += 1
                continue

            stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row1[1]), flags=re.IGNORECASE)
            if not stock or stock in seen_stocks:
                i += 1  # ‚ùó ËøôÈáåÊòØË∑≥1Ë°åËÄå‰∏çÊòØ2Ë°å
                continue
            keti = clean_text(row1[2]) if len(row1) > 3 else ""
            desc = clean_text(row1[3]) if len(row1) > 3 else ""
            esg = clean_text(row2[3]) if len(row2) > 3 else ""

            seen_stocks.add(stock)
            unique_rows.append([stock, keti, desc, esg])
            i += 2  # ‚úÖ Âè™ÊúâËøΩÂä†ÊàêÂäüÊâçË∑≥Ëøá2Ë°å

            if len(unique_rows) >= 10:
                break

        diff_rows = []
        for row in unique_rows:
            stock, keti, desc, esg = row
            query = """
                SELECT * FROM c
                WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
            """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_keti = clean_text(matched[0].get("Á§æ‰ºöÁöÑË™≤È°å", ""))
                old_desc = clean_text(matched[0].get("„Ç≥„É°„É≥„Éà", ""))
                old_esg = clean_text(matched[0].get("ESG„Ç≥„É°„É≥„Éà", ""))
                if old_desc != desc or old_esg != esg:
                    matched_item = matched[0]
                    matched_item.update({
                        "Á§æ‰ºöÁöÑË™≤È°å": keti,
                        "„Ç≥„É°„É≥„Éà": desc,
                        "ESG„Ç≥„É°„É≥„Éà": esg
                    })
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "Êñ∞Á§æ‰ºöÁöÑË™≤È°å": keti,
                        "ÂÖÉÁ§æ‰ºöÁöÑË™≤È°å": old_keti,
                        "Êñ∞„Ç≥„É°„É≥„Éà": desc,
                        "ÂÖÉ„Ç≥„É°„É≥„Éà": old_desc,
                        "Êñ∞ESG„Ç≥„É°„É≥„Éà": esg,
                        "ÂÖÉESG„Ç≥„É°„É≥„Éà": old_esg,
                        "ÂàÜÈ°û": "ÈäòÊüÑËß£Ë™¨Êõ¥Êñ∞„ÅÇ„Çä",
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", "")
                    })
            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "Á§æ‰ºöÁöÑË™≤È°å": keti,
                    "„Ç≥„É°„É≥„Éà": desc,
                    "ESG„Ç≥„É°„É≥„Éà": esg,
                    "no": max_no + 1,
                    "months": get_prev_month_str(),
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "Êñ∞Á§æ‰ºöÁöÑË™≤È°å": keti,
                    "ÂÖÉÁ§æ‰ºöÁöÑË™≤È°å": "",
                    "Êñ∞„Ç≥„É°„É≥„Éà": desc,
                    "ÂÖÉ„Ç≥„É°„É≥„Éà": "",
                    "Êñ∞ESG„Ç≥„É°„É≥„Éà": esg,
                    "ÂÖÉESG„Ç≥„É°„É≥„Éà": "",
                    "ÂàÜÈ°û": "Êñ∞Ë¶èÈäòÊüÑ",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history_si5(diff_rows)
        # update_excel_with_diff_si5(diff_rows, fund_type)

        return diff_rows or "ÂÖ®ÈÉ®‰∏ÄËá¥ÔºåÊó†ÈúÄÊõ¥Êñ∞"

    except Exception as e:
        return f"‚ùå handle_sheet_plussi5 error: {str(e)}"




app = WsgiToAsgi(app)

if __name__ == '__main__':
    app.run(host='0.0.0.0', debug=True) # ÂêØÁî®HTTPS, ssl_context='adhoc'
