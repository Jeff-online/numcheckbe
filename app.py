import os
import openai
from flask import Flask, request, jsonify, send_file,session,redirect
from flask_cors import CORS

from PyPDF2 import PdfReader
import pandas as pd

from azure.identity import DefaultAzureCredential
from azure.cosmos import CosmosClient, PartitionKey
from azure.storage.blob import BlobServiceClient

import logging
from datetime import datetime,timezone,timedelta,UTC
import uuid
import json
import io
import re
import fitz  # PyMuPDF
import base64
from openpyxl import Workbook
from openpyxl.styles import PatternFill, Font
from openpyxl.comments import Comment
import openpyxl
from openpyxl import load_workbook
from openpyxl.styles import Alignment

import time
import threading

import zipfile
import lxml.etree as ET
import os
import io
import ast
from azure.cosmos.exceptions import CosmosResourceNotFoundError, CosmosHttpResponseError
import secrets
from flask_session import Session
from werkzeug.security import generate_password_hash, check_password_hash
import urllib.parse
from io import StringIO
from asgiref.wsgi import WsgiToAsgi
import asyncio
import requests
import pdfplumber
from openpyxl.utils import get_column_letter
from copy import copy
from difflib import SequenceMatcher
import jaconv
import regex as regcheck

# æ—¥å¿—æ ¼å¼å®šä¹‰ (æ—¶é—´æ ¼å¼ï¼Œæ—¥å¿—çº§åˆ«ï¼Œæ¶ˆæ¯)
log_format = '%(asctime)sZ: [%(levelname)s] %(message)s'

# æ—¥å¿—è®¾å®š: æ—¶é—´æ ¼å¼ï¼Œæ—¥å¿—çº§åˆ«ï¼Œæ¶ˆæ¯
logging.basicConfig(
    level=logging.INFO,  # æ—¥å¿—çº§åˆ« (DEBUG, INFO, WARNING, ERROR, CRITICAL)
    format=log_format,   # æ—¥å¿—æ ¼å¼
    handlers=[logging.StreamHandler()]
)

# Managed Identity Auth
credential = DefaultAzureCredential()
token_OPENAI = credential.get_token("https://cognitiveservices.azure.com/.default")
token_COSMOS = credential.get_token("https://cosmos.azure.com/.default")

# Flask app init
app = Flask(__name__)
app.secret_key = secrets.token_hex(16)  # å®‰å…¨å¯†é’¥
app.config['PERMANENT_SESSION_LIFETIME'] = timedelta(minutes=30)  # ä¼šè¯æœ‰æ•ˆæœŸ30åˆ†é’Ÿ

# ğŸ”¹ Flask sesstion settings (save to file system)
app.config["SESSION_TYPE"] = "filesystem"
app.config["SESSION_COOKIE_SECURE"] = False 
app.config["SESSION_COOKIE_HTTPONLY"] = True
app.config["SESSION_COOKIE_SAMESITE"] = "None"
app.config["SESSION_COOKIE_NAME"] = "secure_session"  # session cookie name


Session(app)

# CORS(app, resources={r"/api/*": {"origins": "*"}})
CORS(app, supports_credentials=True, resources={
    r"*": {
        "origins": "*"  # need change to real domain
    }
})



# æ¨¡æ‹Ÿç”¨æˆ·æ•°æ®åº“
users = {
    "admin": {"password": "123"},
    "user": {"password": "123"}
}

#-----------------------------------------------------------------
# Azure OpenAI Setting
# openai.api_type = "azure"
# openai.api_key = os.getenv("AZURE_OPENAI_KEY")  # Get ENV API Key

# COSMOS_DB_KEY = os.getenv("COSMOS_DB_KEY")  # Cosmos DB Key
#-----------------------------------------------------------------


# AzureTokenCache class define
class AzureTokenCache:
    def __init__(self):
        self._lock = threading.Lock() # thredd safe lock
        self.credential = DefaultAzureCredential()
        self.scope = "https://cognitiveservices.azure.com/.default"
        
        self.cached_token = None
        self.token_expires = 0
        self.last_refreshed = 0
        
        self._refresh_token()
        self._start_refresh_thread()

    def get_token(self):
        with self._lock:
            # token 10 minute end before
            if time.time() >= self.token_expires - 600:  # 10 mintute befor end
                self._refresh_token()
            return self.cached_token

    def _acquire_new_token(self):
        """Get new token"""
        return self.credential.get_token(self.scope)

    def _refresh_token(self):
        """update token"""
        new_token = self._acquire_new_token()
        with self._lock:
            self.cached_token = new_token.token
            self.token_expires = new_token.expires_on
            self.last_refreshed = time.time()
        print(f"ğŸ”„Updated Token (END of at:,haha, {self._format_time(self.token_expires)})")

    def _start_refresh_thread(self):
        thread = threading.Thread(target=self._refresh_loop, daemon=True)
        thread.start()

    def _refresh_loop(self):
        while True:
            time.sleep(30)
            if time.time() >= self.token_expires - 600:
                self._refresh_token()

    def _format_time(self, timestamp):
        local_time = time.localtime(timestamp)
        adjusted_time = time.mktime(local_time) + (8 * 3600)  # 8å°æ—¶
        return time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(adjusted_time))
# -------------------------------------------------------------------
token_cache = AzureTokenCache()
#---------

# token method
openai.api_type = "azure_ad"
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")  # Get Env
openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION")  # API Version
deployment_id = os.getenv("AZURE_OPENAI_MODEL")  # Get Deploy Name(mini-ZZ)
_deployment_id = os.getenv("AZURE_OPENAI_MODEL_4")  # Get Deploy Name(mini-ZZ)

# Cosmos DB è¿æ¥ 
COSMOS_DB_URI = os.getenv("COSMOS_DB_URI")
DATABASE_NAME = os.getenv("DATABASE_NAME")
CONTAINER_NAME = os.getenv("CONTAINER_NAME")  # debug not used

# Azure Storage
ACCOUNT_URL = os.getenv("ACCOUNT_URL")
STORAGE_CONTAINER_NAME = os.getenv("STORAGE_CONTAINER_NAME")

MAX_TOKENS=32768 # 16384 for _deployment_id
TEMPERATURE=0
SEED=42
PDF_DIR = ACCOUNT_URL + STORAGE_CONTAINER_NAME

# Cosmos DB
def get_db_connection(CONTAINER):
    # Cosmos DB é“¾æ¥å®¢æˆ·ç«¯
    client = CosmosClient(COSMOS_DB_URI, credential=credential)
    database = client.get_database_client(DATABASE_NAME)
    container = database.get_container_client(CONTAINER)
    print("Connected to Azure Cosmos DB SQL API")
    logging.info("Connected to Azure Cosmos DB SQL API")
    return container  # Cosmos DB

#-----------------------------------------------------------------
LOG_RECORD_CONTAINER_NAME = "log_record"
FILE_MONITOR_ITEM = "file_monitor_item"
TENBREND_CONTAINER_NAME = 'tenbrend_history'
PROXYINFO_CONTAINER_NAME = 'proxyInfo'
INTEGERATION_RURU_CONTAINER_NAME = 'integeration_ruru'
#-----------------------------------------------------------------
integeration_container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

# List proxy
@app.route('/api/proxyinfo', methods=['GET'])
def get_proxyinfos():
    # Cosmos DB è¿æ¥
    container = get_db_connection(PROXYINFO_CONTAINER_NAME)
    
    query = "SELECT * FROM c"
    users = list(container.query_items(query=query, enable_cross_partition_query=True))
    response = {
        "code": 200,
        "data": users
    }
    return jsonify(response), 200

# Create proxy
@app.route('/api/proxyinfo', methods=['POST'])
def create_proxyuser():
    data = request.get_json()
    username = data.get('username')
    password = data.get('password')

    if not username or not password:
        return jsonify({"error": "Missing username or password"}), 400

    # Cosmos DB è¿æ¥
    container = get_db_connection(PROXYINFO_CONTAINER_NAME)

    # ç¡®è®¤ç”¨æˆ·
    query = "SELECT * FROM c WHERE c.username = @username"
    params = [dict(name="@username", value=username)]
    existing_users = list(container.query_items(
        query=query, 
        parameters=params, 
        enable_cross_partition_query=True
    ))

    if existing_users:
        return jsonify({"error": "Username already exists"}), 409  # HTTP 409 Conflict

    user_item = {
        'id': str(uuid.uuid4()),
        'username': username,
        'password': password  # å¯†ç  hashing
    }
    container.create_item(body=user_item)
    response = {
        "code": 200,
        "data": user_item
    }

    return jsonify(response), 201

# update proxy
@app.route('/api/proxyinfo', methods=['PUT'])
def update_proxyuser():
    try:
        data = request.get_json()
        new_username = data.get('username')
        new_password = data.get('password')

        if not all([new_username, new_password]):
            return jsonify({"error": "Required fields: proxyuserName and Password"}), 400

        container = get_db_connection(PROXYINFO_CONTAINER_NAME)

        try:
            query = f"SELECT * FROM c"
            existing_user = list(container.query_items(
                query=query,
                enable_cross_partition_query=True
            ))[0]
        except IndexError:
            return jsonify({"error": "Find error error"}), 404

        proxy_data = dict(username=new_username, password=new_password)
        if existing_user:
            existing_user.update(proxy_data)
            container.upsert_item(existing_user)
        else:
            proxy_data.update(id=str(uuid.uuid4()))
            container.upsert_item(proxy_data)

        return jsonify({
            "username": new_username,
            "code": 200
        }), 200

    except CosmosHttpResponseError as e:
        logging.error(f"Cosmos DB Error: {str(e)}")
        return jsonify({"error": "DB Error"}), 500
    except Exception as e:
        logging.error(f"server error: {str(e)}")
        return jsonify({"error": "server error"}), 500
    
USERINFO_CONTAINER_NAME = 'userInfo'
#----------------------User CRUD--------
@app.route('/api/users', methods=['GET'])
def get_users():
    # Cosmos DB è¿æ¥
    container = get_db_connection(USERINFO_CONTAINER_NAME)

    query = "SELECT * FROM c"
    users = list(container.query_items(query=query, enable_cross_partition_query=True))
    response = {
        "code": 200,
        "data": users
    }
    return jsonify(response), 200

@app.route('/api/users', methods=['POST'])
def create_user():
    data = request.get_json()
    username = data.get('username')
    password = data.get('password')

    if not username or not password:
        return jsonify({"error": "Missing username or password"}), 400

    container = get_db_connection(USERINFO_CONTAINER_NAME)

    # ç¡®è®¤ç”¨æˆ·
    query = "SELECT * FROM c WHERE c.username = @username"
    params = [dict(name="@username", value=username)]
    existing_users = list(container.query_items(
        query=query, 
        parameters=params, 
        enable_cross_partition_query=True
    ))

    if existing_users:
        return jsonify({"error": "Username already exists"}), 409  # HTTP 409 Conflict

    user_item = {
        'id': str(uuid.uuid4()),
        'username': username,
        'password': generate_password_hash(password)
    }
    container.create_item(body=user_item)
    response = {
        "code": 200,
        "data": user_item
    }

    return jsonify(response), 201

@app.route('/api/users/<user_id>', methods=['PUT'])
def update_user(user_id):
    try:
        data = request.get_json()
        new_username = data.get('username')
        new_password = data.get('password')

        if not all([new_username, new_password]):
            return jsonify({"error": "username and password need input"}), 400

        container = get_db_connection(USERINFO_CONTAINER_NAME)

        try:
            query = f"SELECT * FROM c WHERE c.id = '{user_id}'"
            existing_user = list(container.query_items(
                query=query,
                enable_cross_partition_query=True
            ))[0]
        except IndexError:
            return jsonify({"error": "Do not find user"}), 404

        if existing_user['username'] != new_username:
            dup_query = f"SELECT * FROM c WHERE c.username = '{new_username}'"
            if list(container.query_items(dup_query, enable_cross_partition_query=True)):
                return jsonify({"error": "username duplicate"}), 409

        updated_item = {
            "id": user_id,
            "username": new_username,
            "password": generate_password_hash(new_password),
            **{k: v for k, v in existing_user.items() if k not in ['username', 'password']}
        }

        container.delete_item(item=user_id, partition_key=existing_user['id'])
        container.create_item(body=updated_item)

        return jsonify({
            "id": updated_item['id'],
            "username": updated_item['username']
        }), 200

    except CosmosHttpResponseError as e:
        logging.error(f"Cosmos DB error: {str(e)}")
        return jsonify({"error": "db error"}), 500
    except Exception as e:
        logging.error(f"server error: {str(e)}")
        return jsonify({"error": "server error"}), 500
            

@app.route('/api/users/<user_id>', methods=['DELETE'])
def delete_user(user_id):
    container = get_db_connection(USERINFO_CONTAINER_NAME)
    
    container.delete_item(item=user_id, partition_key=user_id)
    return jsonify({"message": "User deleted"}), 200

#--------------------------------------
@app.before_request
def check_session():
    # æ£€æŸ¥ä¼šè¯æœ‰æ•ˆæœŸ
    if 'user_id' in session:
        last_activity = session.get('last_activity')
        session.modified = True
        if last_activity and (datetime.now() - datetime.fromisoformat(last_activity)) > app.config['PERMANENT_SESSION_LIFETIME']:
            session.clear()
            return jsonify({"status": "error", "message": "Session expired"}), 401
        # æ›´æ–°æœ€åæ´»åŠ¨æ—¶é—´
        session['last_activity'] = datetime.now().isoformat()

@app.route('/api/login', methods=['POST'])
def login():
    data = request.get_json()
    username = data.get('username', '').strip().lower()
    password = data.get('password', '').strip()

    if not username or not password:
        return jsonify({"status": "error", "message": "ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¾ãŸã¯ãƒ‘ã‚¹ãƒ¯ãƒ¼ãƒ‰ãŒé–“é•ã£ã¦ã„ã¾ã™"}), 400

    container = get_db_connection(USERINFO_CONTAINER_NAME)
    
    query = "SELECT * FROM c WHERE c.username = @username"
    params = [dict(name="@username", value=username)]

    items = list(container.query_items(
        query=query,
        parameters=params,
        enable_cross_partition_query=True
    ))

    if not items:
        return jsonify({"success": "false", "message": "User not found"}), 404

    user = items[0]
    if not check_password_hash(user['password'], password):
        return jsonify({"success": "false", "message": "Invalid password"}), 401

    session.clear()
    session['user_id'] = user['id']
    session['username'] = username

    return jsonify({"success": "true", "message": "ãƒ­ã‚°ã‚¤ãƒ³æˆåŠŸï¼"}), 200

@app.route('/api/logout', methods=['POST'])
def logout():
    session.clear()
    return jsonify({"status": "success", "message": "ãƒ­ã‚°ã‚¢ã‚¦ãƒˆ"}), 200

@app.route('/api/protected', methods=['GET'])
def protected():
    if not session.get('session_id'):
        return jsonify({"status": "error", "message": "Unauthorized"}), 401
    
    return jsonify({
        "status": "success",
        "message": "Protected content",
        "secure_session": session.get('session_id')
    }), 200


CHECK_SESSION_COOKIE = "session_cookie"

@app.route('/api/session_cookie', methods=['GET'])
def get_session_cookie():
    try:
        container = get_db_connection(CHECK_SESSION_COOKIE)
        
        query = "SELECT * FROM c"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        for item in items:
            item['id'] = item['id']

        return jsonify(items), 200
        
    except CosmosResourceNotFoundError:
        logging.error("Monitoring status document not found")
        return jsonify({"error": "Status document not found"}), 404
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    

@app.route('/api/session_cookie', methods=['PUT'])
def update_session_cookie():
    try:
        container = get_db_connection(CHECK_SESSION_COOKIE)
        
        # secure_session
        # session_value = request.cookies.get('secure_session', 'none')
        session_value = request.json.get('status', 'off')
        
        status_item = {
            'id': 'session_cookie',
            'type': 'control',
            'session_value': session_value,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        container.upsert_item(body=status_item)
        logging.info(f"Session value updated: {session_value}")
        return jsonify({
            'message': 'Session value updated',
            'session_value': session_value
        }), 200
        
    except CosmosResourceNotFoundError as e:
        logging.error(f"Cosmos DB error: {str(e)}")
        return jsonify({"error": "Database operation failed"}), 500
    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    
#-----------API--------------------
def remove_code_blocks(text):
    text = re.sub(r'```html', '', text)
    text = re.sub(r'```', '', text)
    return text.strip()

def remove_code_blocks_enhance(text):
    text = re.sub(r'```html\n?', '', text)  
    text = re.sub(r'```', '', text)
    text = re.sub(r'\n\n\*\*NG\*\*\n```', '', text)
    return text.strip()


@app.route('/api/dic_search_db', methods=['POST'])
def dic_search_db():
    try:
        data = request.json

        original = data.get('original')
        corrected = data.get('corrected')

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        query = f"SELECT * FROM c WHERE c.original = '{original}' AND c.corrected = '{corrected}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        if items:
            results = [{"original": item["original"], "corrected": item["corrected"]} for item in items]
            return jsonify({"success": True, "data": results}), 200
        else:
            return jsonify({"success": False, "message": "No matching data found in DB."}), 404

    except Exception as e:
        logging.error(f"âŒ Error occurred while searching DB: {e}")
        return jsonify({"success": False, "message": str(e)}), 500

@app.route('/ask_gpt', methods=['POST'])
def ask_gpt():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("âœ… Token Update SUCCESS")
        
        data = request.json
        prompt = data.get("input", "")

        if not prompt:
            return jsonify({"success": False, "error": "No input provided"}), 400

        # db get map
        corrected_map = fetch_and_convert_to_dict()

        # 3. apply_corrections
        corrected = apply_corrections(prompt, corrected_map)


        prompt_result = f"""
        You are a professional Japanese text proofreading assistant. 
        Please carefully proofread the content of a Japanese report following the rules below. 
        This includes not only Japanese text but also English abbreviations (è‹±ç•¥èª), foreign terms (å¤–æ¥èª),
        and specialized terminology (å°‚é–€ç”¨èª). Ensure that all language elements are reviewed according to the guidelines and corrected where necessary.:

        **Report Content to Proofread:**
        {corrected}

        **Proofreading Requirements:**
        1. **Check for typos and missing characters (èª¤å­—è„±å­—ãŒãªã„ã“ã¨):**
        - Ensure there are no **spelling errors** or **missing characters** in the report. 
        - ã‚ãªãŸã®å½¹å‰²ã¯ã€æ—¥æœ¬èªã®èª¤å­—ãƒ»è„±å­—ãƒ»è¡¨è¨˜ãƒŸã‚¹ã‚’ä¿®æ­£ã—ã€ä¸å®Œå…¨ãªå˜èªã‚„æ–‡ç« ã«é©åˆ‡ãªèªã‚’è£œå®Œã™ã‚‹ã“ã¨ã§ã™ã€‚  
        ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã„ã€å…¥åŠ›ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æ ¡æ­£ã—ã¦ãã ã•ã„ã€‚
            **Common Mistakes Examples (èª¤å­—ãƒ»è„±å­—ã®ä¾‹)**:
            Example:
            - `ãƒªãƒ†ãƒ¼ãƒ«æŠ•å®¶` â†’ `ãƒªãƒ†ãƒ¼ãƒ«æŠ•è³‡å®¶` (èª¤å­—: å®¶ â†’ è³‡)
            - `é•·å›½å‚µ` â†’ `é•·æœŸå›½å‚µ` (è„±å­—: æœŸã‚’è¿½åŠ )
            - `è­˜ã•ã‚ŒãŸ` â†’ `æ„è­˜ã•ã‚ŒãŸ` (è¡¨è¨˜ã®çµ±ä¸€)
            - `é‡‘ç·©å’ŒæœŸå¾…` â†’ `é‡‘èç·©å’ŒæœŸå¾…` (èª¤å­—: é‡‘ â†’ é‡‘è)
            - `è¦‹æ–¹ãŒå‹•ã—` â†’ `è¦‹æ–¹ãŒå¤‰å‹•ã—` (èª¤å­—: å‹•ã— â†’ å‹•ã)
            - `è¦–ã™ã‚‹` â†’ `é‡è¦–ã™ã‚‹`  
            - `çµŒæˆé•·` â†’ `çµŒæ¸ˆæˆé•·`  
            - `é€é…é›»å‚™` â†’ `é€é…é›»è¨­å‚™`  
            - `æ¥­è¦‹é€šã—` â†’ `æ¥­ç¸¾è¦‹é€šã—`  
            - `å¸¸å¢—ç›Š` â†’ `çµŒå¸¸å¢—ç›Š`  
            - `è²¡æ”¿ç­–` â†’ `è²¡æ”¿æ”¿ç­–`  
            - `æ–¹` â†’ `æ–¹é‡`  
            - `æ‰‹Eã‚³ãƒãƒ¼ã‚¹` â†’ `å¤§æ‰‹Eã‚³ãƒãƒ¼ã‚¹`  
            - `éŸ¿ã—ã¾ã—ãŸ` â†’ `å½±éŸ¿ã—ã¾ã—ãŸ`  
            - `æ–½ã•ã‚Œ` â†’ `å®Ÿæ–½ã•ã‚ŒãŸ`  
            - `ä¼æ¥­ã®åˆä½µãƒ»å` â†’ `ä¼æ¥­ã®åˆä½µãƒ»å›å`  
            - `æœ¬ã¨ã—ã¾ã™` â†’ `åŸºæœ¬ã¨ã—ã¾ã™`  
            - `å‹™çŠ¶æ³` â†’ `è²¡å‹™çŠ¶æ³`
            - `å†…æŠ•è³‡ä¿¡è¨—` â†’ `å›½å†…æŠ•è³‡ä¿¡è¨—`  
            - `æŒã—ã¾ã—ãŸ` â†’ `ç¶­æŒã—ã¾ã—ãŸ`  
            - `ãƒã‚¤ãƒŠã‚¹å› ` â†’ `ãƒã‚¤ãƒŠã‚¹è¦å› `  
            - `åã•ã‚Œã‚‹` â†’ `åæ˜ ã•ã‚Œã‚‹`  
            - `æ›¿ãƒ˜ãƒƒã‚¸` â†’ `ç‚ºæ›¿ãƒ˜ãƒƒã‚¸`  
            - `æ¯”ã¯` â†’ `æ¯”ç‡ã¯`
            - `è¦ç·©å’Œ` â†’ `è¦åˆ¶ç·©å’Œ`
            - `æ™¯æ¸ˆæŒ‡æ¨™` â†’ `çµŒæ¸ˆæŒ‡æ¨™`
            - `å‰¤` â†’ `çµŒæ¸ˆ`
            - `æ˜‡ã™ã‚‹ãªã©ã¾ã¡ã¾ã¡ã§ã—ãŸã€‚` â†’ `ç•°ãªã‚‹å‹•ãã¨ãªã‚Šã¾ã—ãŸã€‚` (Ensure that the original text is not directly modified but follows this guideline.)
            - `ç©æ¥µå§¿å‹¢ã¨ã—ãŸ` â†’ `é•·ã‚ã¨ã—ãŸ` (Ensure that the original text is not directly modified but follows this guideline.)
            - `æ¶ˆæ¥µå§¿å‹¢ã¨ã—ãŸ` â†’ `é•·ã‚ã¨ã—ãŸ` (Ensure that the original text is not directly modified but follows this guideline.)
            - `ï¼ˆå‰²å®‰ã«ï¼‰æ”¾ç½®` â†’ `å‰²å®‰æ„Ÿã®ã‚ã‚‹`
            - `é™å®šçš„` â†’ `ä»–ã®é©åˆ‡ãªè¡¨ç¾ã«ä¿®æ­£ ï¼ˆä¿®æ­£ç†ç”±: åŠ¹æœã‚„å½±éŸ¿ãŒãƒ—ãƒ©ã‚¹ã‹ãƒã‚¤ãƒŠã‚¹ã‹ä¸æ˜ç­ãªãŸã‚ï¼‰`
            - `åˆ©ç›Šç¢ºå®šã®å£²ã‚Š` â†’ `ï½ãŒå‡ºãŸã¨ã®è¦‹æ–¹ ï¼ˆä¿®æ­£ç†ç”±: æ–­å®šçš„ãªè¡¨ç¾ã§ã¯æ ¹æ‹ ãŒèª¬æ˜ã§ããªã„ãŸã‚ï¼‰`
            - `åˆ©é£Ÿã„å£²ã‚Š` â†’ `ï½ãŒå‡ºãŸã¨ã®è¦‹æ–¹ ï¼ˆä¿®æ­£ç†ç”±: æ–­å®šçš„ãªè¡¨ç¾ã§ã¯æ ¹æ‹ ãŒèª¬æ˜ã§ããªã„ãŸã‚ï¼‰`
            - `å¿…ãšï½` â†’ `æ ¹æ‹ ãŒæ˜ç¤ºã•ã‚Œã¦ã„ãªã„ãŸã‚ä½¿ç”¨ä¸å¯ ï¼ˆä¿®æ­£ç†ç”±: å°†æ¥ã®é‹ç”¨æˆç¸¾ã‚„çµŒæ¸ˆæŒ‡æ¨™ãƒ»ä¼æ¥­æ¥­ç¸¾ç­‰ã«ã¤ã„ã¦æ–­å®šçš„ãªåˆ¤æ–­ã‚’ç¤ºã™è¡¨ç¾ã¯NGï¼‰`
            - `ï½ã«ãªã‚‹` â†’ `æ ¹æ‹ ãŒæ˜ç¤ºã•ã‚Œã¦ã„ãªã„ãŸã‚ä½¿ç”¨ä¸å¯ ï¼ˆä¿®æ­£ç†ç”±: å°†æ¥ã®é‹ç”¨æˆç¸¾ã‚„çµŒæ¸ˆæŒ‡æ¨™ãƒ»ä¼æ¥­æ¥­ç¸¾ç­‰ã«ã¤ã„ã¦æ–­å®šçš„ãªåˆ¤æ–­ã‚’ç¤ºã™è¡¨ç¾ã¯NGï¼‰`
            - `ï½ã§ã‚ã‚‹` â†’ `æ ¹æ‹ ãŒæ˜ç¤ºã•ã‚Œã¦ã„ãªã„ãŸã‚ä½¿ç”¨ä¸å¯ ï¼ˆä¿®æ­£ç†ç”±: å°†æ¥ã®é‹ç”¨æˆç¸¾ã‚„çµŒæ¸ˆæŒ‡æ¨™ãƒ»ä¼æ¥­æ¥­ç¸¾ç­‰ã«ã¤ã„ã¦æ–­å®šçš„ãªåˆ¤æ–­ã‚’ç¤ºã™è¡¨ç¾ã¯NGï¼‰`
  
            **Disambiguation Rule**:
            - ã€Œæ²ˆé™ã€ï¼è‡ªç„¶ã«è½ã¡ç€ã (natural calming down; happens over time)
            - ã€Œé®é™ã€ï¼äººç‚ºçš„ã«ãŠã•ã‚ã‚‹ (intentional suppression; medically or artificially done)

            **Correction Policy**:
            1. Detect whether the context implies a natural or artificial calming.
            2. If the usage does not match the context, correct it using the appropriate word.
            3. Highlight the correction using the format below:
            `<span style="color:red;">Corrected Term</span> (<span>ä¿®æ­£ç†ç”±: æ„å‘³ã®èª¤ç”¨ <s style="background:yellow;color:red">Original Term</s> â†’ Corrected Term</span>)`
            4. Do **not** modify the original sentence structure or paragraph formatting.
            5. Only apply the correction when the term is clearly misused.
            6. If the current usage is correct, do not change or annotate it.

            **Example**:
            - Input: å¸‚å ´ã¯å¾ã€…ã«é®é™ã—ã¦ã„ã£ãŸã€‚
            - Output: å¸‚å ´ã¯å¾ã€…ã« <span style="color:red;">æ²ˆé™</span> (<span>ä¿®æ­£ç†ç”±: æ„å‘³ã®èª¤ç”¨ <s style="background:yellow;color:red">é®é™</s> â†’ æ²ˆé™</span>) ã—ã¦ã„ã£ãŸã€‚


        - è¡¨ç¾ã®ä½¿ç”¨åˆ¶é™:
            Expression Usage Restrictions:
            Restricted Expressions:

            - é­…åŠ›çš„ãª
            - æŠ•è³‡å¦™å‘³
            - å‰²é«˜æ„Ÿ
            - å‰²å®‰æ„Ÿ

            Usage Conditions:
            The above expressions can be used if evidence is provided.

            However, these expressions should not be used in contexts where the word "fund" (ãƒ•ã‚¡ãƒ³ãƒ‰) or any related reference is mentioned. In any sentence or context where "fund" or "ãƒ•ã‚¡ãƒ³ãƒ‰" appears, these expressions should be avoided.

            ä½¿ç”¨ä¾‹:
            é­…åŠ›çš„ãª: æ ¹æ‹ ã«åŸºã¥ã„ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯å¯èƒ½ã§ã™ãŒã€ãƒ•ã‚¡ãƒ³ãƒ‰ã«ã¤ã„ã¦ã¯ä½¿ç”¨ã—ãªã„ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
            æŠ•è³‡å¦™å‘³: æŠ•è³‡å¦™å‘³ãŒã‚ã‚‹ã“ã¨ã‚’ç¤ºã™å ´åˆã§ã‚‚ã€ãƒ•ã‚¡ãƒ³ãƒ‰ã«å¯¾ã™ã‚‹è¨€åŠã¯é¿ã‘ã€ä»–ã®æŠ•è³‡å¯¾è±¡ã«é©ç”¨ã™ã‚‹ã‚ˆã†ã«ã—ã¦ãã ã•ã„ã€‚
            å‰²é«˜æ„Ÿ: å‰²é«˜æ„Ÿã«ã¤ã„ã¦è¿°ã¹ã‚‹å ´åˆã€ãƒ•ã‚¡ãƒ³ãƒ‰ä»¥å¤–ã®æŠ•è³‡å¯¾è±¡ã«å¯¾ã—ã¦é©ç”¨ã—ã¦ãã ã•ã„ã€‚
            å‰²å®‰æ„Ÿ: å‰²å®‰æ„Ÿã«ã¤ã„ã¦è¨€åŠã™ã‚‹å ´åˆã‚‚ã€ãƒ•ã‚¡ãƒ³ãƒ‰ã«å¯¾ã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯ä¸å¯ã§ã™ã€‚

            âœ… å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
            <span style="color:red;">é­…åŠ›çš„ãª</span>
            (<span>ä¿®æ­£ç†ç”±: ãƒ•ã‚¡ãƒ³ãƒ‰ã«å¯¾ã—ã¦ã®ä½¿ç”¨ã¯ä¸å¯ã€‚</span>)
            âœ… Exsample1:
            Input:ãƒ•ã‚¡ãƒ³ãƒ‰ã¯é­…åŠ›çš„ãªæŠ•è³‡å…ˆã¨ã—ã¦ç´¹ä»‹ã•ã‚ŒãŸã€‚

            Output:
            ãƒ•ã‚¡ãƒ³ãƒ‰ã¯
            <span style="color:red;">é­…åŠ›çš„ãª</span>
            (<span>ä¿®æ­£ç†ç”±: ãƒ•ã‚¡ãƒ³ãƒ‰ã«å¯¾ã™ã‚‹ä½¿ç”¨ã¯ä¸å¯</span>)æŠ•è³‡å…ˆã¨ã—ã¦ç´¹ä»‹ã•ã‚ŒãŸã€‚
            âœ… Exsample2:
            Input:
            ã“ã®ãƒ•ã‚¡ãƒ³ãƒ‰éŠ˜æŸ„ã«ã¯æŠ•è³‡å¦™å‘³ãŒã‚ã‚‹ã€‚
            
            Output:
            ã“ã®éŠ˜æŸ„ã«ã¯
            <span style="color:red;">æŠ•è³‡å¦™å‘³</span>
            (<span>ä¿®æ­£ç†ç”±: ãƒ•ã‚¡ãƒ³ãƒ‰ã«å¯¾ã—ã¦ã®ä½¿ç”¨ã¯ä¸å¯ã€‚</span>)ãŒã‚ã‚‹ã€‚


        - æ•°å­—ã‚„ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼ˆï¼…ï¼‰ã‚’å«ã‚€æ–‡ç« ã®èª¤ã‚Šã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹
            æ–‡è„ˆã‚’ç†è§£ã—ã€æ•°å€¤ãƒ»å‰²åˆã®å‰å¾Œã®èªå¥ãŒé©åˆ‡ã‹ç¢ºèªã™ã‚‹ã“ã¨ã€‚
            ä¾‹:
            æœˆæœ«æ™‚ç‚¹ï¼ˆ20æ—¥åˆ¤å®šï¼‰ã§ã®ç‚ºæ›¿ãƒ˜ãƒƒã‚¸ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”ã¯48ï¼…ã§ã™ã€‚
            â†’ æ­£ã—ã„è¡¨è¨˜: æœˆæœ«æ™‚ç‚¹ï¼ˆ20æ—¥åˆ¤å®šï¼‰ã§ã®ç‚ºæ›¿ãƒ˜ãƒƒã‚¸ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ¯”ç‡ã¯48ï¼…ã§ã™ã€‚
            å¸‚å ´ã®æˆé•·ç‡ã¯10%ã®è¦‹è¾¼ã¿ã§ã™ã€‚ âœ… (å•é¡Œãªã—)
            ã‚¤ãƒ³ãƒ•ãƒ¬ç‡ã¯2ä¸Šæ˜‡ã—ã¾ã—ãŸã€‚ âŒ (èª¤ã‚Š: 2%ä¸Šæ˜‡ã—ã¾ã—ãŸã€‚ ã«ä¿®æ­£)
            è²©å£²ã‚·ã‚§ã‚¢ã¯15ã®æ‹¡å¤§ãŒäºˆæƒ³ã•ã‚Œã¾ã™ã€‚ âŒ (èª¤ã‚Š: è²©å£²ã‚·ã‚§ã‚¢ã¯15%ã®æ‹¡å¤§ãŒäºˆæƒ³ã•ã‚Œã¾ã™ã€‚)
        - æ ¡æ­£ãƒ«ãƒ¼ãƒ«
            1. **ã€Œè¡Œã£ã¦æ¥ã„ã€ã®é©åˆ‡ãªç½®ãæ›ãˆ**(Ensure that the original text is not directly modified but follows this guideline.)
            - æ–‡ç« å…¨ä½“ã‚’åˆ†æã—ã€ã€Œè¡Œã£ã¦æ¥ã„ã€ãŒä½•ã‚’æŒ‡ã—ã¦ã„ã‚‹ã®ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„ã€‚
            - **ä¾¡æ ¼ãƒ»æŒ‡æ•°ãƒ»ãƒ¬ãƒ¼ãƒˆãªã©ãŒä¸Šæ˜‡ã—ãŸæ„å‘³ã®å ´åˆ** â†’ ã€Œè¡Œã£ã¦æ¥ã„ã€ã‚’ã€Œä¸Šæ˜‡ã—ãŸã€ã«å¤‰æ›
            - **ä¾¡æ ¼ãƒ»æŒ‡æ•°ãƒ»ãƒ¬ãƒ¼ãƒˆãªã©ãŒä¸‹è½ã—ãŸæ„å‘³ã®å ´åˆ** â†’ ã€Œè¡Œã£ã¦æ¥ã„ã€ã‚’ã€Œä¸‹è½ã—ãŸã€ã«å¤‰æ›

            2. **æ–‡è„ˆã‚’è€ƒæ…®ã—ãŸæ ¡æ­£**
            - ä¿®æ­£ã®éš›ã€å‘¨è¾ºã®æ–‡è„ˆã‚’ç†è§£ã—ã€è‡ªç„¶ãªå½¢ã«èª¿æ•´ã—ã¦ãã ã•ã„ã€‚
            
            3. **ã€Œæ¨ªã°ã„ã€ã®é©åˆ‡ãªç½®ãæ›ãˆ**
            - æ–‡ç« å…¨ä½“ã‚’åˆ†æã—ã€ã€Œæ¨ªã°ã„ã€ã®å‰å¾Œã®æ–‡è„ˆã‚’è€ƒæ…®ã—ã¦ãã ã•ã„ã€‚
            - **æœŸä¸­ã®å¤‰å‹•å¹…ãŒå°ã•ã„å ´åˆ** â†’ ã€Œæ¨ªã°ã„ã€ã‚’ç¶­æŒ
            - **å¤‰å‹•å¹…ãŒå¤§ããã€çµæœçš„ã«åŒç¨‹åº¦ã¨ãªã£ãŸå ´åˆ** â†’ ã€Œã»ã¼å¤‰ã‚ã‚‰ãšã€ã¾ãŸã¯ã€ŒåŒç¨‹åº¦ã¨ãªã‚‹ã€ã«å¤‰æ›´
            - å‘¨å›²ã®æ–‡ç« ã«åˆã‚ã›ã¦ã€ã‚ˆã‚Šé©åˆ‡ãªè¡¨ç¾ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚

            4. **ã€Œå‡ºé…ã‚Œæ„Ÿã€ã®é©åˆ‡ãªä¿®æ­£**
            - å¯¾å¿œæ–¹é‡:

            ã€Œå‡ºé…ã‚Œæ„Ÿã€ ã¯ä¸»è¦³çš„ãªç›¸å ´è¦³ãŒå«ã¾ã‚Œã‚‹ãŸã‚ã€å¿…ãšã€Œâ€¦ã¨è€ƒãˆã¾ã™ã€‚ã€ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

            ä¿®æ­£æ–¹æ³•: æ–‡è„ˆã«å¿œã˜ã¦è‡ªç„¶ã«ã€Œã€œã¨è€ƒãˆã¾ã™ã€å½¢ã«ä¿®æ­£ã—ã¾ã™ã€‚

            ä¿®æ­£å¾Œã®è¡¨ç¾: æ–‡ã®æµã‚Œã«åˆã‚ã›ã¦è‡ªç„¶ã«è¡¨ç¾ã‚’èª¿æ•´ã—ã€èª­ã¿ã‚„ã™ã•ã‚’è€ƒæ…®ã—ã¾ã™ã€‚

            ä¿®æ­£ç†ç”±: ç›¸å ´è¦³ãŒå«ã¾ã‚Œã¦ã„ã‚‹ãŸã‚ã€ä¸»è¦³çš„ãªè¡¨ç¾ã‚’å®¢è¦³çš„ãªè¡¨ç¾ã«å¤‰ãˆã‚‹ã“ã¨ã§æ–‡ç« ã®ä¿¡é ¼æ€§ã‚’å‘ä¸Šã•ã›ã¾ã™ã€‚

            å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ:
            <span style="color:red;">å‡ºé…ã‚Œæ„Ÿ</span>
            (<span>ä¿®æ­£ç†ç”±: ä¸»è¦³çš„è¡¨ç¾ã®ä¿®æ­£ <s style="background:yellow;color:red">å‡ºé…ã‚Œæ„Ÿ</s> â†’ ã€Œã€œã¨è€ƒãˆã¾ã™ã€ã«ä¿®æ­£</span>)

            Exsample:
                Input: ã“ã®éŠ˜æŸ„ã«ã¯å‡ºé…ã‚Œæ„ŸãŒã‚ã‚‹
                Output: ã“ã®éŠ˜æŸ„ã«ã¯
                <span style="color:red;">å‡ºé…ã‚Œæ„Ÿ</span>
                (<span>ä¿®æ­£ç†ç”±: ä¸»è¦³çš„è¡¨ç¾ã®ä¿®æ­£ <s style="background:yellow;color:red">å‡ºé…ã‚Œæ„Ÿ</s> â†’ å‡ºé…ã‚Œã¦ã„ã‚‹ã¨è€ƒãˆã¾ã™</span>)
                ãŒã‚ã‚‹

                Input: ä¸€éƒ¨ã®ã‚»ã‚¯ã‚¿ãƒ¼ã«ã¯å‡ºé…ã‚Œæ„ŸãŒã‚ã‚‹ã¨æ„Ÿã˜ã‚‰ã‚Œã‚‹
                Output: ä¸€éƒ¨ã®ã‚»ã‚¯ã‚¿ãƒ¼ã«ã¯
                <span style="color:red;">å‡ºé…ã‚Œæ„Ÿ</span>
                (<span>ä¿®æ­£ç†ç”±: ä¸»è¦³çš„è¡¨ç¾ã®ä¿®æ­£ <s style="background:yellow;color:red">å‡ºé…ã‚Œæ„Ÿ</s> â†’ å‡ºé…ã‚Œã¦ã„ã‚‹ã¨è€ƒãˆã¾ã™</span>)
                ãŒã‚ã‚‹


            5. **ã€Œä¸Šæ˜‡è¦å› ã€ãƒ»ã€Œä¸‹è½è¦å› ã€ã®é©åˆ‡ãªèª¬æ˜è¿½åŠ **
            - **æ–‡è„ˆã‚’åˆ†æã—ã€å…·ä½“çš„ãªè¦å› ã‚’è¿½åŠ ã—ã¦ãã ã•ã„**ã€‚
            - **ã€Œä¸Šæ˜‡è¦å› ã€ãŒã‚ã‚‹å ´åˆ** â†’ ä¸Šæ˜‡ã®ç†ç”±ï¼ˆä¾‹: ä¼æ¥­æ±ºç®—ã®æ”¹å–„ã€æ”¿ç­–ã®ç™ºè¡¨ã€éœ€çµ¦ãƒãƒ©ãƒ³ã‚¹ã®å¤‰åŒ–ãªã©ï¼‰ã‚’è£œè¶³ã€‚
            - **ã€Œä¸‹è½è¦å› ã€ãŒã‚ã‚‹å ´åˆ** â†’ ä¸‹è½ã®ç†ç”±ï¼ˆä¾‹: æ™¯æ°—å¾Œé€€æ‡¸å¿µã€é‡‘èå¼•ãç· ã‚ã€åœ°æ”¿å­¦ãƒªã‚¹ã‚¯ãªã©ï¼‰ã‚’è£œè¶³ã€‚
            - **ä¿®æ­£å¾Œã‚‚æ–‡ç« ã®æµã‚ŒãŒã‚¹ãƒ ãƒ¼ã‚ºã«ãªã‚‹ã‚ˆã†ã«èª¿æ•´ã—ã¦ãã ã•ã„ã€‚

            6. **ã€Œäºˆæƒ³ã€ã€Œå¿ƒç†ã€ã®é©åˆ‡ãªä¿®æ­£**
            - **ã€Œäºˆæƒ³ã€ ãŒã‚ã‚‹å ´åˆ:**  
                - **èª°ã®äºˆæƒ³ã‹æ˜ç¢ºã§ãªã„å ´åˆ** â†’ ã€Œå¸‚å ´äºˆæƒ³ã€ã«ä¿®æ­£  
            - **ã€Œå¿ƒç†ã€ ãŒã‚ã‚‹å ´åˆ:**  
                - **ä¸»èªãŒæ›–æ˜§ãªå ´åˆ** â†’ ã€Œå¸‚å ´å¿ƒç†ã€ã«ä¿®æ­£

    
        2. **Follow the "Fund Manager Comment Terminology Guide" (ãƒ•ã‚¡ãƒ³ãƒ‰ãƒãƒãƒ¼ã‚¸ãƒ£ã‚³ãƒ¡ãƒ³ãƒˆç”¨èªé›†ã«æ²¿ã£ãŸè¨˜è¼‰ã¨ãªã£ã¦ã„ã‚‹ã“ã¨):**
        - **Consistent Terminology (è¡¨è¨˜ã®çµ±ä¸€):**
            - Ensure the **writing format** of financial terms is **consistent throughout the report**.
            Example:
            - `ç›¸å¯¾ã«ä½ã‹ã£ãŸ` â†’ `ç›¸å¯¾çš„ã«ä½ã‹ã£ãŸ` (æ–‡æ³•ä¿®æ­£)
            - `æ±è¨¼33æ¥­ç¨®åˆ†ã§ã¯` â†’ `æ±è¨¼33æ¥­ç¨®åˆ†é¡ã§ã¯` (è¡¨è¨˜ã®çµ±ä¸€)
        - **Common Mistakes and Corrections (èª¤è¨˜ã¨ä¿®æ­£ä¾‹)**:
            Example:
            - `æ”¿åºœæ”¯ã®` â†’ `æ”¿åºœæ”¯å‡ºã®é…ã‚Œã€1å›ã¯ã®ã‚’å‡ºã€2å›ã¯ã®ã‚’å‰Šé™¤` (èª¤å­—: ã® â†’ å‡º)
            - `æŠ•è³‡æ¯”ç‡ã‚’ç¶­ã™ã‚‹` â†’ `æŠ•è³‡æ¯”ç‡ã‚’ç¶­æŒã™ã‚‹` (ä¸€è‡´æ€§ä¸è¶³ å‹•ç”£ â†’ ä¸å‹•ç”£)
            - `ï¼ˆé…å½“ã“ã¿ï¼‰` â†’ `ï¼ˆé…å½“è¾¼ã¿ï¼‰` (è¡¨è¨˜ã®çµ±ä¸€: ã“ã¿ â†’ è¾¼ã¿)
            - `ã„ã£ã½ã†ã§` â†’ `ä¸€æ–¹ã§` (è¡¨è¨˜ã®çµ±ä¸€: ã„ã£ã½ã† â†’ ä¸€æ–¹)
            - `ã‚¦ã‚¯ãƒ©ã‚¤ãƒŠã¨ãƒ­ã‚·ã‚¢ã‚’ã‚ãã‚‹` â†’ `ã‚¦ã‚¯ãƒ©ã‚¤ãƒŠã¨ãƒ­ã‚·ã‚¢ã‚’å·¡ã‚‹` (è¡¨è¨˜ã®çµ±ä¸€: ã‚ãã‚‹ â†’ å·¡ã‚‹)
            - `æ±è¨¼33æ¥­ç¨®ã§ã¿ã‚‹ã¨` â†’ `æ±è¨¼33æ¥­ç¨®ã§è¦‹ã‚‹ã¨` (æ–‡æ³•ä¿®æ­£: ã¿ã‚‹ â†’ è¦‹ã‚‹)
            - `ã²ãç¶šã` â†’ `å¼•ãç¶šã` (è¡¨è¨˜ã®çµ±ä¸€: ã²ã â†’ å¼•ã)
            - `é›»æ°—æ©Ÿå™¨ã€éŠ€è¡Œæ¥­ã€ä¿é™ºæ¥­ç­‰` â†’ `é›»æ°—æ©Ÿå™¨ã€éŠ€è¡Œæ¥­ã€ä¿é™ºæ¥­ãªã©` (è¡¨è¨˜ã®çµ±ä¸€: ç­‰ â†’ ãªã©)
            - `ã˜ã` â†’ `æ¬¡æœŸ`
            - `åº•ã„ã‚Œå¾Œ` â†’ `åº•å…¥ã‚Œå¾Œ`
            - `ãªã‹ã‹ã‚‰` â†’ `ä¸­ã‹ã‚‰`
            - `ãŠã‚‚ãª` â†’ `ä¸»è¦ãª`
            - `ã¯ã‚„ã` â†’ `æ—©æ€¥ã«`
            - `ã‹ã„ã¤ã‘ã—ãŸ` â†’ `è²·ã„ä»˜ã‘ã—ãŸ`
            - `ãªã©` â†’ `ç­‰`
            - `ã®ãã` â†’ `é™¤ã`
            - `ãã¿ã„ã‚Œ` â†’ `çµ„ã¿å…¥ã‚Œ`
            - `ã˜ã‚‡ã†ã` â†’ `ä¸Šè¨˜`
            - `ã¨ã†ãƒ•ã‚¡ãƒ³ãƒ‰` â†’ `å½“ãƒ•ã‚¡ãƒ³ãƒ‰`

        - **Prohibited Words and Phrases (ç¦æ­¢ï¼ˆNGï¼‰ãƒ¯ãƒ¼ãƒ‰åŠã³æ–‡ç« ã®æ³¨æ„äº‹é …):**
            - Check if any prohibited words or phrases are used in the report and correct them as per the guidelines.
        - **Replaceable and Recommended Terms/Expressions (ç½®ãæ›ãˆãŒå¿…è¦ãªç”¨èª/è¡¨ç¾ã€ç½®ãæ›ãˆã‚’æ¨å¥¨ã™ã‚‹ç”¨èª/è¡¨ç¾):**
            - If you find terms or expressions that need to be replaced, revise them according to the provided rules.
            - ãƒãƒˆæ´¾ï¼ã‚¿ã‚«æ´¾ã®è¡¨è¨˜ï¼ˆé‡‘èæ”¿ç­–ã«é–¢ã™ã‚‹ï¼‰:
                -Exsample:
                - é‡‘èç·©å’Œé‡è¦–  â†’ é‡‘èå¼•ãç· ã‚é‡è¦–
                - é‡‘èç·©å’Œã«å‰å‘ã  â†’ é‡‘èå¼•ãç· ã‚ã«ç©æ¥µçš„
            - Exsample:
             - ç¹”ã‚Šè¾¼ã‚€  â†’ åæ˜ ã•ã‚Œ
             - ç›¸å ´  â†’ å¸‚å ´/ä¾¡æ ¼
             - é€£ã‚Œé«˜  â†’ å½±éŸ¿ã‚’å—ã‘ã¦ä¸Šæ˜‡
             - ä¼æ’­  â†’ åºƒãŒã‚‹
             - ãƒˆãƒ¬ãƒ³ãƒ‰  â†’ å‚¾å‘
             - ãƒ¬ãƒ³ã‚¸  â†’ ç¯„å›²
        - **ã€‡ï¼…ã‚’ä¸Šå›ã‚‹ï¼ˆä¸‹å›ã‚‹ï¼‰ãƒã‚¤ãƒŠã‚¹ã®è¡¨è¨˜:**
                - ã€‡ï¼…ã‚’ä¸Šå›ã‚‹  â†’ ã€‡ï¼…ã‚’è¶…ãˆã‚‹
                - ã€‡ï¼…ã‚’ä¸‹å›ã‚‹  â†’ ç¸®å°
                - ã€‡ï¼…ã‚’ä¸Šå›ã‚‹  â†’ ä¸‹å›ã‚‹ãƒã‚¤ãƒŠã‚¹å¹…
                - ã€‡ï¼…ã‚’ä¸‹å›ã‚‹  â†’ ç¸®å°


        - **Use of Hiragana (ã²ã‚‰ãŒãªã‚’è¡¨è¨˜ã™ã‚‹ã‚‚ã®):**
            - Ensure the report follows the rules for hiragana notation, replacing content that does not conform to commonly used kanji.
        - **Kana Notation for Non-Standard Kanji (ä¸€éƒ¨ã‹ãªæ›¸ãç­‰ã§è¡¨è¨˜ã™ã‚‹ã‚‚ã®):**
            - Ensure non-standard kanji are replaced with kana as the standard writing format.
        - **Correct Usage of Okurigana (ä¸€èˆ¬çš„ãªé€ã‚Šä»®åãªã©):**
            - Ensure the correct usage of okurigana is applied.
        - **English Abbreviations, Loanwords, and Technical Terms (è‹±ç•¥èªã€å¤–æ¥èªã€å°‚é–€ç”¨èªãªã©):**
            - Check if English abbreviations, loanwords, and technical terms are expressed correctly.
        - **Identify and mark any å¸¸ç”¨å¤–æ¼¢å­— (HyÅgai kanji):**
        - **Identify and mark any **å¸¸ç”¨å¤–æ¼¢å­— (HyÅgai kanji)** in the following text**
        - **å¸¸ç”¨å¤–æ¼¢å­—** refers to characters **not included** in the [å¸¸ç”¨æ¼¢å­—è¡¨ (JÅyÅ Kanji List)](https://ja.wikipedia.org/wiki/å¸¸ç”¨æ¼¢å­—), which is Japanâ€™s official list of commonly used kanji.
        - Refer to the [Wikipedia list of HyÅgai kanji](https://ja.wikipedia.org/wiki/å¸¸ç”¨æ¼¢å­—) to determine if a character falls into this category.
        - **For any detected å¸¸ç”¨å¤–æ¼¢å­—**, apply the following formatting:
        - **Highlight the incorrect character in red** (`<span style="color:red;">`).
        - **Strike through the incorrect character and provide the reason in yellow highlight.**

        ---

        ### **ğŸ’¡ Output Format (å‡ºåŠ›ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ)**
        - **Incorrect characters should be displayed in red (`<span style="color:red;">`)**.
        - **Corrected text should be marked with a strikethrough (`<s>`) and highlighted in yellow (`background:yellow`)** to show the correction.
        - Use the following structure:
            ```html
            <span style="color:red;">é´‰</span> (<span>ä¿®æ­£ç†ç”±: å¸¸ç”¨å¤–æ¼¢å­— <s style="background:yellow;color:red">é´‰</s></span>)
            ```
        - **Example Correction**:
            ```plaintext
            é´‰ â†’ <span style="color:red;">é´‰</span> (<span>ä¿®æ­£ç†ç”±: å¸¸ç”¨å¤–æ¼¢å­— <s style="background:yellow;color:red">é´‰</s></span>)
            ```
        - **For multiple HyÅgai kanji**, apply the same structure to each character.

        ---

        ### **âœ… Example Input:**
        ```plaintext
        å½¼ã¯é´‰ãŒç©ºã‚’é£›ã¶ã®ã‚’è¦‹ãŸã€‚

        ### **âœ… Example output:**
        ```plaintext
        å½¼ã¯ <span style="color:red;">é´‰</span> (<span>ä¿®æ­£ç†ç”±: å¸¸ç”¨å¤–æ¼¢å­— <s style="background:yellow;color:red">é´‰</s></span>) ãŒç©ºã‚’é£›ã¶ã®ã‚’è¦‹ãŸã€‚

        - **Foreign Exchange Market Trend Analysis**
            In the foreign exchange market (`ç‚ºæ›¿å¸‚å ´`), determine whether `"å††ã ã‹"` should be revised to `"å††é«˜"` (Yen Appreciation) or `"å††å®‰"` (Yen Depreciation) based on the **context**.

            #### **** Criteria for Yen Appreciation (å††é«˜)**
            - **Yen appreciation (`å††é«˜`) occurs when the value of the yen increases relative to other currencies.**  
            The following situations indicate yen appreciation:
            1. **"å¤šãã®é€šè²¨ãŒå¯¾å††ã§ä¸‹è½ã—ãŸ"** (Many currencies declined against the yen) â†’ Change `å††ã ã‹` to **å††é«˜**.
            2. **"ãƒ‰ãƒ«å††ãŒä¸‹è½ã—ãŸ"** (USD/JPY exchange rate declined) â†’ Change `å††ã ã‹` to **å††é«˜**.
            3. **"å¯¾ç±³ãƒ‰ãƒ«ã§å††ã®ä¾¡å€¤ãŒä¸Šæ˜‡ã—ãŸ"** (The yen appreciated against the US dollar) â†’ Change `å††ã ã‹` to **å††é«˜**.

            #### **** Criteria for Yen Depreciation (å††å®‰)**
            - **Yen depreciation (`å††å®‰`) occurs when the value of the yen declines relative to other currencies.**  
            The following situations indicate yen depreciation:
            1. **"å¤šãã®é€šè²¨ãŒå¯¾å††ã§ä¸Šæ˜‡ã—ãŸ"** (Many currencies rose against the yen) â†’ Change `å††ã ã‹` to **å††å®‰**.
            2. **"ãƒ‰ãƒ«å††ãŒä¸Šæ˜‡ã—ãŸ"** (USD/JPY exchange rate increased) â†’ Change `å††ã ã‹` to **å††å®‰**.
            3. **"å¯¾ç±³ãƒ‰ãƒ«ã§å††ã®ä¾¡å€¤ãŒä¸‹è½ã—ãŸ"** (The yen depreciated against the US dollar) â†’ Change `å††ã ã‹` to **å††å®‰**.


        3. **Replaceable and Recommended Terms/Expressions (æ¨å¥¨ã•ã‚Œã‚‹è¡¨ç¾ã®ä¿®æ­£)**
        - Use the correct **kanji, hiragana, and katakana** combinations based on standard Japanese financial terms.
            Example:
            - `ãŒå¥½ã•ã‚ŒãŸè¼¸é€ç”¨æ©Ÿå™¨ãªã©` â†’ `ãŒå¥½æ„Ÿã•ã‚ŒãŸè¼¸é€ç”¨æ©Ÿå™¨ãªã©` (ä¿®æ­£ç†ç”±: é©åˆ‡ãªè¡¨ç¾)

        - **Task**: Header Date Format Validation & Correction  
        - **Target Area**: Date notation in parentheses following "ä»Šå¾Œé‹ç”¨æ–¹é‡ (Future Policy Decision Basis)"  
        ---
        ### Validation Requirements  
        1. **Full Format Compliance Check**:  
        - Must follow "YYYYå¹´MMæœˆDDæ—¥ç¾åœ¨" (Year-Month-Day as of)  
        - **Year**: 4-digit number (e.g., 2024)  
        - **Month**: 2-digit (01-12, e.g., April â†’ 4)  
        - **Day**: 2-digit (01-31, e.g., 5th â†’ 5)  
        - **Suffix**: Must end with "ç¾åœ¨" (as of)  

        2. **Common Error Pattern Detection**:  
        âŒ "1æœˆ0æ—¥" â†’ Missing month leading zero + invalid day 0  
        âŒ "2024å¹´4æœˆ1æ—¥" â†’ 2024å¹´4æœˆ1æ—¥
        âŒ "2024å¹´12æœˆ" â†’ Missing day value  
        âŒ "2024-04-05ç¾åœ¨" â†’ Incorrect separator usage (hyphen/slash)  
        ---
        ### Correction Protocol  
        1. **Leading Zero Enforcement**  
        - Add leading zeros to single-digit months/days (4æœˆ â†’ 4æœˆ, 5æ—¥ â†’ 5æ—¥)  

        2. **Day 0 Handling**  
        - Replace day 0 with YYYYMMDD Date Format  
        - Example: 2024å¹´4æœˆ0æ—¥ â†’ 2024å¹´4æœˆ00æ—¥

        3. **Separator Standardization**  
        - Convert hyphens/slashes to CJK characters:  
            `2024/04/05` â†’ `2024å¹´4æœˆ5æ—¥`  

        ---
        ### Output Format Specification  
        ```html
        <Correction Example>
        <span style="color:red;">ï¼ˆ2024å¹´4æœˆ0æ—¥ç¾åœ¨ï¼‰</span> 
        â†’ 
        <span style="color:green;">ï¼ˆ2024å¹´04æœˆ00æ—¥ç¾åœ¨ï¼‰</span>
        ä¿®æ­£ç†ç”±:
        â‘ æ—¥ä»˜0ã‚’YYYYMMDDæ—¥ä»˜ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã«ç½®æ›
        ---

        3. **Consistency with Report Data Section (ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿éƒ¨ã¨ã®æ•´åˆæ€§ç¢ºèª):**
        - Ensure the textual description in the report is completely consistent with the data section, without any logical or content-related discrepancies.

        4. **Eliminate language fluency(å˜èªé–“ã®ä¸è¦ãªã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤):**
        - Ensure that there are no extra spaces.
            -Example:
            input:æ™¯æ°—æµ®æšãŒæ„ è­˜ã•ã‚ŒãŸã“ã¨ã§
            output:æ™¯æ°—æµ®æšãŒæ„è­˜ã•ã‚ŒãŸã“ã¨ã§
        
        5.  **Layout and Formatting Rules (ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã«é–¢ã™ã‚‹çµ±ä¸€):**
            - **æ–‡é ­ã®ã€Œâ—‹ã€å°ã¨ä¸€æ–‡å­—ç›®ã®é–“éš”ã‚’çµ±ä¸€:**

                - When a sentence starts with the symbol â—‹, make sure there is no space (half-width or full-width) between it and the first character. That is, use â—‹æ–‡å­— instead of â—‹ æ–‡å­— or â—‹ã€€æ–‡å­—.
                    - Any whitespace (half-width or full-width spaces) after â—‹ must be removed.
                    - This spacing rule must be applied consistently throughout the document.

            åŠè§’æ‹¬å¼§ã‚’å…¨è§’æ‹¬å¼§ã«çµ±ä¸€:
                - Convert all half-width parentheses () to full-width parentheses ï¼ˆï¼‰.
                - Example: (æ³¨) â†’ ï¼ˆæ³¨ï¼‰
                - Example input: 
                    â—‹ ä¸–ç•Œã®é«˜é…å½“æ ªå¼æŒ‡æ•°(æ³¨)ã¯æœˆé–“ã§ã¯ä¸Šæ˜‡ã—ã¾ã—ãŸã€‚
                - Exsample output: 
                    <span style="color:red;">â—‹ä¸–ç•Œ</span> (<span>ä¿®æ­£ç†ç”±: æ–‡é ­ã®ã€Œâ—‹ã€å°ã¨ä¸€æ–‡å­—ç›®ã®é–“éš”ã‚’çµ±ä¸€ <s style="background:yellow;color:red">â—‹ ä¸–ç•Œ</s> â†’ â—‹ä¸–ç•Œ</span>)
                    <span style="color:red;">ï¼ˆæ³¨ï¼‰</span> (<span>ä¿®æ­£ç†ç”±: åŠè§’æ‹¬å¼§ã‚’å…¨è§’æ‹¬å¼§ã«çµ±ä¸€ <s style="background:yellow;color:red">(æ³¨)</s> â†’ ï¼ˆæ³¨ï¼‰</span>)


            - **æ–‡ç« ã®é–“éš”ã®çµ±ä¸€:**
                - If a sentence begins with "â—‹", ensure that the spacing within the frame remains consistent.
            - **ä¸Šä½10éŠ˜æŸ„ ã‚³ãƒ¡ãƒ³ãƒˆæ¬„ã«ã¤ã„ã¦ã€æ å†…ã«é©åˆ‡ã«åã¾ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯:**
                - If the stock commentary contains a large amount of text, confirm whether it fits within the designated frame. 
                - If the ranking changes in the following month, adjust the frame accordingly.
                - **Check point**
                    1. **æ–‡å­—æ•°åˆ¶é™å†…ã«åã¾ã£ã¦ã„ã‚‹ã‹ï¼Ÿ**
                    - 1æ ã‚ãŸã‚Šã®æœ€å¤§æ–‡å­—æ•°ã‚’è¶…ãˆã¦ã„ãªã„ã‹ï¼Ÿ
                    - é©åˆ‡ãªè¡Œæ•°ã§åã¾ã£ã¦ã„ã‚‹ã‹ï¼Ÿ

                    2. **æ¬¡æœˆã®é †ä½å¤‰å‹•ã«ä¼´ã†æ èª¿æ•´ã®å¿…è¦æ€§**
                    - é †ä½ãŒå¤‰æ›´ã•ã‚Œã‚‹ã¨æ èª¿æ•´ãŒå¿…è¦ãªãŸã‚ã€èª¿æ•´ãŒå¿…è¦ãªç®‡æ‰€ã‚’ç‰¹å®š

                    3. **æ å†…ã«åã¾ã‚‰ãªã„å ´åˆã®ä¿®æ­£ææ¡ˆ**
                    - å¿…è¦ã«å¿œã˜ã¦ã€çŸ­ç¸®è¡¨ç¾ã‚„ä¸è¦ãªæƒ…å ±ã®å‰Šé™¤ã‚’ææ¡ˆ
                    - é‡è¦ãªæƒ…å ±ã‚’æãªã‚ãšã«é©åˆ‡ã«ãƒªãƒ©ã‚¤ãƒˆ

                    output Format:
                    - **ã‚³ãƒ¡ãƒ³ãƒˆã®æ è¶…éãƒã‚§ãƒƒã‚¯**
                    - (æ è¶…éã—ã¦ã„ã‚‹ã‹: ã¯ã„ / ã„ã„ãˆ)
                    - (è¶…éã—ã¦ã„ã‚‹å ´åˆã€ã‚ªãƒ¼ãƒãƒ¼ã—ãŸæ–‡å­—æ•°)

                    - **é †ä½å¤‰å‹•ã«ã‚ˆã‚‹æ èª¿æ•´ã®å¿…è¦æ€§**
                    - (èª¿æ•´ãŒå¿…è¦ãªã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ)

                    - **ä¿®æ­£ææ¡ˆ**
                    - (æ å†…ã«åã‚ã‚‹ãŸã‚ã®ä¿®æ­£å¾Œã®ã‚³ãƒ¡ãƒ³ãƒˆ)

            **Standardized Notation (è¡¨è¨˜ã®çµ±ä¸€):**
            - **åŸºæº–ä¾¡é¡ã®é¨°è½ç‡:**
            When there are three decimal places, round off using the round-half-up method to the second decimal place. If there are only two decimal places, keep the value unchanged.
                Make modifications directly in this article and explain the reasons for the modifications.

                exsample:
                0.546ï¼…ï¼ˆÃ—ï¼‰ â†’ 0.55ï¼…ï¼ˆâ—‹ï¼‰
                ä¿®æ­£ç†ç”±: å°æ•°ç‚¹ä»¥ä¸‹ã®æ¡æ•°ã®ä¸¸ã‚ï¼ˆ0.546ï¼… â†’ 0.55ï¼…ï¼‰
                If the value is 0.00ï¼…, replace it with "å‰æœˆæœ«ã‹ã‚‰å¤‰ã‚ã‚‰ãš" or "å‰æœˆæœ«ã¨åŒç¨‹åº¦" instead of stating "é¨°è½ç‡ã¯å¤‰ã‚ã‚‰ãš".
                ä¿®æ­£ç†ç”±: ã€Œé¨°è½ç‡ã¯å¤‰ã‚ã‚‰ãšã€ã¨ã„ã†è¡¨è¨˜ã¯NGã€‚ä»£ã‚ã‚Šã«ã€ŒåŸºæº–ä¾¡é¡ï¼ˆåˆ†é…é‡‘å†æŠ•è³‡ï¼‰ã¯å‰æœˆæœ«ã‹ã‚‰å¤‰ã‚ã‚‰ãšã€ã‚„ã€Œå‰æœˆæœ«ã¨åŒç¨‹åº¦ã€ã¨è¨˜è¼‰ã—ã¾ã™ã€‚

                exsample:
                0.00ï¼…ã¨ãªã‚Šï¼ˆÃ—ï¼‰ â†’ å‰æœˆæœ«ã‹ã‚‰å¤‰ã‚ã‚‰ãšï¼ˆâ—‹ï¼‰

                é¨°è½ç‡ã¯å¤‰ã‚ã‚‰ãšï¼ˆÃ—ï¼‰ â†’ åŸºæº–ä¾¡é¡ï¼ˆåˆ†é…é‡‘å†æŠ•è³‡ï¼‰ã¯å‰æœˆæœ«ã‹ã‚‰å¤‰ã‚ã‚‰ãšï¼ˆâ—‹ï¼‰

                When comparing the performance of the fund with the benchmark (or reference index), the comparison must be made using rounded numbers.

                ä¿®æ­£ç†ç”±: æ¯”è¼ƒã¯ä¸¸ã‚ãŸæ•°å­—ã§è¡Œãªã†ã“ã¨ã€‚
                If the fund and benchmark (or reference index) have the same rate of return, use the phrase "é¨°è½ç‡ã¯åŒç¨‹åº¦ã¨ãªã‚Šã¾ã—ãŸ" instead of saying "é¨°è½ç‡ã¯åŒã˜ã§ã—ãŸ".
                ä¿®æ­£ç†ç”±: åŒã˜ã¨ã„ã†è¡¨ç¾ã¯é¿ã‘ã€ä»£ã‚ã‚Šã«ã€ŒåŒç¨‹åº¦ã€ã¨è¨˜è¼‰ã™ã‚‹ã“ã¨ã€‚

                exsample:
                ã€Œé¨°è½ç‡ã¯åŒã˜ã§ã—ãŸã€ï¼ˆÃ—ï¼‰ â†’ ã€Œé¨°è½ç‡ã¯åŒç¨‹åº¦ã¨ãªã‚Šã¾ã—ãŸã€ï¼ˆâ—‹ï¼‰
                If the fund's rate of return is greater than the benchmark's, use the phrase "ä¸Šå›ã‚Šã¾ã—ãŸ" to indicate the fund outperformed the benchmark.
                ä¿®æ­£ç†ç”±: ä¸Šå›ã£ãŸå ´åˆã€ã€Œä¸Šå›ã‚Šã¾ã—ãŸã€ã¨è¡¨è¨˜ã™ã‚‹ã“ã¨ã€‚

                exsample:
                é¨°è½ç‡ã¯-1.435ï¼…ï¼ˆåŸºé‡‘ï¼‰ã¨-2.221ï¼…ï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰ã®å ´åˆã€å€¤ã®å·®ã¯0.79ï¼…ã¨ãªã‚‹ãŸã‚ã€ã€Œä¸Šå›ã‚Šã¾ã—ãŸã€ã¨è¨˜è¼‰ã—ã¾ã™ã€‚

                If the fund's rate of return is lower than the benchmark's, use the phrase "ä¸‹é™ã—ã¾ã—ãŸ" to indicate the fund underperformed the benchmark.
                ä¿®æ­£ç†ç”±: ä¸‹é™ã—ãŸå ´åˆã€ã€Œä¸‹é™ã—ã¾ã—ãŸã€ã¨è¡¨è¨˜ã™ã‚‹ã“ã¨ã€‚

                exsample:
                é¨°è½ç‡ã¯-1.435ï¼…ï¼ˆåŸºé‡‘ï¼‰ã¨-0.221ï¼…ï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰ã®å ´åˆã€åŸºé‡‘ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã¯ã€Œä¸‹é™ã—ã¾ã—ãŸã€ã¨è¨˜è¼‰ã—ã¾ã™ã€‚

            - **ã€Œä»Šå¾Œã®é‹ç”¨æ–¹é‡ã€ä½œæˆæ—¥ä»˜ã®ãƒ«ãƒ¼ãƒ«:**
                - å‰æœˆæœ«ï¼ˆå–¶æ¥­æ—¥ï¼‰ç¾åœ¨ã§ä½œæˆã€‚
                - ç¿Œæœˆåˆã®æ—¥ä»˜ã«ãªã‚‹å ´åˆã¯ã€ä½œæˆã—ãŸæ—¥ä»˜ã‚’å…¥ã‚Œã‚‹ã€‚
                - ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ»ã‚µãƒ¼ãƒ“ã‚¹éƒ¨ã¸é€ä¿¡ã™ã‚‹ä»¥é™ã®æ—¥ä»˜ï¼ˆå…ˆæ—¥ä»˜ï¼‰ã¯å…¥ã‚Œãªã„ã€‚
                - ã€Œå‚è€ƒæœˆã€ã‚ˆã‚Šå¾Œã§ã‚ã‚Šã€ã€Œãƒã‚§ãƒƒã‚¯æœŸé–“ã€ã‚ˆã‚Šå‰ã®æ—¥ä»˜ã®ã¿ä½¿ç”¨å¯ã€‚
                    - Example:
                    - OK: å‚è€ƒæœˆï¼2024å¹´2æœˆ â†’ ä½œæˆæ—¥ãŒ2024å¹´2æœˆ28æ—¥ï¼ˆå–¶æ¥­æ—¥ï¼‰ or 3æœˆ1æ—¥ï¼ˆç¿Œæœˆåˆï¼‰
                    - NG: å‚è€ƒæœˆï¼2024å¹´2æœˆ â†’ ä½œæˆæ—¥ãŒ3æœˆ5æ—¥ï¼ˆã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆãƒ»ã‚µãƒ¼ãƒ“ã‚¹éƒ¨é€ä¿¡å¾Œã®å…ˆæ—¥ä»˜ï¼‰

            - **ï¼…ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼‰ã€ã‚«ã‚¿ã‚«ãƒŠ:**
                - **åŠè§’ã‚«ã‚¿ã‚«ãƒŠ â†’ å…¨è§’ã‚«ã‚¿ã‚«ãƒŠ**ï¼ˆä¾‹:ã€Œï½¶ï¾€ï½¶ï¾…ã€â†’ã€Œã‚«ã‚¿ã‚«ãƒŠã€ï¼‰
                - **åŠè§’è¨˜å· â†’ å…¨è§’è¨˜å·**ï¼ˆä¾‹:ã€Œ%ã€â†’ã€Œï¼…ã€ã€ã€Œ@ã€â†’ã€Œï¼ ã€ï¼‰
                    Example:
                        input: ï¾ï¾ï¾ï¾ï¾ï½°ï½¸ (ä¿®æ­£ç†ç”±: åŠè§’ã‚«ã‚¿ã‚«ãƒŠã‚’å…¨è§’ã‚«ã‚¿ã‚«ãƒŠã«çµ±ä¸€ ï¾ï¾ï¾ï¾ï¾ï½°ï½¸ â†’ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯)ã«å¯¾ã—ã¦ 
                        output: ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ (ä¿®æ­£ç†ç”±: åŠè§’ã‚«ã‚¿ã‚«ãƒŠã‚’å…¨è§’ã‚«ã‚¿ã‚«ãƒŠã«çµ±ä¸€ ï¾ï¾ï¾ï¾ï¾ï½°ï½¸ â†’ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯)ã«å¯¾ã—ã¦
                    Example:
                        input: ï½¶ï¾€ï½¶ï¾… 
                        output: ã‚«ã‚¿ã‚«ãƒŠ
                    Example:
                        input: %
                        output: ï¼… 
                    Example:
                        input: @
                        output: ï¼  

            - **æ•°å­—ã€ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã€ã€Œï¼‹ã€ãƒ»ã€Œï¼ã€:**
                - **å…¨è§’æ•°å­—ãƒ»ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ â†’ åŠè§’æ•°å­—ãƒ»ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ**ï¼ˆä¾‹:ã€Œï¼‘ï¼’ï¼“ã€â†’ã€Œ123ã€ã€ã€Œï¼¡ï¼¢ï¼£ã€â†’ã€ŒABCã€ï¼‰
                - **å…¨è§’ã€Œï¼‹ã€ã€Œï¼ã€ â†’ åŠè§’ã€Œ+ã€ã€Œ-ã€**ï¼ˆä¾‹:ã€Œï¼‹ï¼ã€â†’ã€Œ+-ã€
                    Example:
                        input: ï¼‘ï¼’ï¼“ ï¼¡ï¼¢ï¼£ ï½±ï½²ï½³ ï¼‹ï¼
                        output: 123 ABC ã‚¢ã‚¤ã‚¦ +-

            - **ã‚¹ãƒšãƒ¼ã‚¹ã¯å¤‰æ›´ãªã—**  

            - **ã€Œâ€»ã€ã®ä½¿ç”¨:**
                - ã€Œâ€»ã€ã¯å¯èƒ½ã§ã‚ã‚Œã° **ä¸Šä»˜ãæ–‡å­—ï¼ˆsuperscriptï¼‰â€»** ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
                - å‡ºåŠ›å½¢å¼ã®ä¾‹:
                - ã€Œé‡è¦äº‹é …â€»ã€ â†’ ã€Œé‡è¦äº‹é …<sup>â€»</sup>ã€

            - **ï¼ˆã‚«ãƒƒã‚³æ›¸ãï¼‰:**
                - Parenthetical notes should only be included in their first occurrence in a comment.
                    For the following Japanese text, check if parentheses ("ï¼ˆ ï¼‰") are used appropriately.
                    If a parenthetical note appears more than once, remove the parentheses for subsequent occurrences.
                    The first occurrence should retain the parentheses, but any further appearances should have the parentheses removed.
                    Modification reason: Parentheses are redundant after the first mention, so the text is cleaned up for consistency and readability.

                **Check point**
                    1. **ã‚«ãƒƒã‚³æ›¸ãã¯ã€ã‚³ãƒ¡ãƒ³ãƒˆã®åˆå‡ºã®ã¿ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ**
                    - åŒã˜ã‚«ãƒƒã‚³æ›¸ããŒ2å›ä»¥ä¸Šç™»å ´ã—ã¦ã„ãªã„ã‹ï¼Ÿ
                    - åˆå‡ºãƒšãƒ¼ã‚¸ä»¥é™ã®ã‚³ãƒ¡ãƒ³ãƒˆã«ã‚«ãƒƒã‚³æ›¸ããŒé‡è¤‡ã—ã¦è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„ã‹ï¼Ÿ

                    2. **ãƒ‡ã‚£ã‚¹ã‚¯ãƒ­ã®ãƒšãƒ¼ã‚¸ç•ªå·é †ã«å¾“ã£ã¦ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨**
                    - ã‚·ãƒ¼ãƒˆã®é †ç•ªã§ã¯ãªãã€å®Ÿéš›ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’åŸºæº–ã«ã™ã‚‹ã€‚

                    3. **ä¾‹å¤–å‡¦ç†**
                    - ã€Œä¸€éƒ¨ä¾‹å¤–ãƒ•ã‚¡ãƒ³ãƒ‰ã‚ã‚Šã€ã¨ã‚ã‚‹ãŸã‚ã€ä¾‹å¤–çš„ã«ã‚«ãƒƒã‚³æ›¸ããŒè¤‡æ•°å›ç™»å ´ã™ã‚‹ã‚±ãƒ¼ã‚¹ã‚’è€ƒæ…®ã™ã‚‹ã€‚
                    - ä¾‹å¤–ã¨ã—ã¦èªã‚ã‚‰ã‚Œã‚‹ã‚±ãƒ¼ã‚¹ã‚’åˆ¤æ–­ã—ã€é©åˆ‡ã«æŒ‡æ‘˜ã€‚

                    output Format:
                    - **ã‚«ãƒƒã‚³æ›¸ãã®åˆå‡ºãƒªã‚¹ãƒˆ**ï¼ˆã©ã®ãƒšãƒ¼ã‚¸ã«æœ€åˆã«ç™»å ´ã—ãŸã‹ï¼‰
                    - **é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ**ï¼ˆã©ã®ãƒšãƒ¼ã‚¸ã§äºŒé‡è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
                    - **ä¿®æ­£ææ¡ˆ**ï¼ˆã©ã®ãƒšãƒ¼ã‚¸ã®ã‚«ãƒƒã‚³æ›¸ãã‚’å‰Šé™¤ã™ã¹ãã‹ï¼‰
                    - **ä¾‹å¤–ãƒ•ã‚¡ãƒ³ãƒ‰ãŒé©ç”¨ã•ã‚Œã‚‹å ´åˆã€è£œè¶³æƒ…å ±**

            - **ä¼šè¨ˆæœŸé–“ã®è¡¨è¨˜:**
                - The use of "ï½" is prohibited; always use "-".
                - Make modifications directly in this article and explain the reasons for the modifications.
                    - Example: 6ï½8æœˆæœŸï¼ˆÃ—ï¼‰ â†’ 6-8æœˆæœŸï¼ˆâ—‹ï¼‰

                - æš¦å¹´ã‚’æ¡ç”¨ã—ã¦ã„ã‚‹å›½ã®å¹´åº¦è¡¨è¨˜:
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                ã‚«ãƒƒã‚³æ›¸ãã§æš¦å¹´ã®æœŸé–“ã‚’æ˜è¨˜ã™ã‚‹ã€‚
                - Example:
                    ãƒ–ãƒ©ã‚¸ãƒ«ã®2021å¹´åº¦äºˆç®—ï¼ˆÃ—ï¼‰ â†’ ãƒ–ãƒ©ã‚¸ãƒ«ã®2021å¹´åº¦ï¼ˆ2021å¹´1æœˆ-12æœˆï¼‰äºˆç®—ï¼ˆâ—‹ï¼‰

                - æ±ºç®—æœŸé–“ã¯ã€Œâ—-â—æœˆæœŸã€ã«çµ±ä¸€ã—ã€æ—¥ä»˜ã¯çœç•¥ã™ã‚‹ã€‚
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                    - Example: ç¬¬1å››åŠæœŸï¼ˆ5æœˆ21æ—¥ï½8æœˆ20æ—¥ï¼‰ï¼ˆÃ—ï¼‰ â†’ 5-8æœˆæœŸï¼ˆâ—‹ï¼‰
                    ã‚¤ãƒ¬ã‚®ãƒ¥ãƒ©ãƒ¼ãªã‚±ãƒ¼ã‚¹ã‚‚å«ã‚ã€åŸå‰‡ã€Œâ—-â—æœˆæœŸã€ã¨è¡¨è¨˜ã€‚

            - **ã€ŒTOPIXã€ã¾ãŸã¯ã€Œæ±è¨¼æ ªä¾¡æŒ‡æ•°ã€ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨:**
                æ–‡ä¸­ã§ä½¿ç”¨ã™ã‚‹å ´åˆ: ã€ŒTOPIXï¼ˆæ±è¨¼æ ªä¾¡æŒ‡æ•°ï¼‰ã€ã¨è¡¨è¨˜ã™ã‚‹ã“ã¨ã‚’æŒ‡ç¤ºã€‚
                ã€Œæ–‡ä¸­ã§ã¯ã€TOPIXï¼ˆæ±è¨¼æ ªä¾¡æŒ‡æ•°ï¼‰ã€ã¨è¡¨è¨˜ã—ã¦ãã ã•ã„ã€‚(Ensure that the original text is not directly modified but follows this guideline.)ã€
                ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼ˆBMï¼‰ã‚„å‚è€ƒæŒ‡æ•°ã¨ã—ã¦ä½¿ç”¨ã™ã‚‹å ´åˆ: ã€Œæ±è¨¼æ ªä¾¡æŒ‡æ•°ï¼ˆTOPIXï¼‰ï¼ˆé…å½“è¾¼ã¿ï¼‰ã€ã¨è¡¨è¨˜ã™ã‚‹ã“ã¨ã‚’æŒ‡ç¤ºã€‚
                ã€ŒBMã‚„å‚è€ƒæŒ‡æ•°ã§ä½¿ç”¨ã™ã‚‹å ´åˆã¯ã€ã€æ±è¨¼æ ªä¾¡æŒ‡æ•°ï¼ˆTOPIXï¼‰ï¼ˆé…å½“è¾¼ã¿ï¼‰ã€ã¨è¡¨è¨˜ã—ã¦ãã ã•ã„ã€‚(Ensure that the original text is not directly modified but follows this guideline.)ã€

            - **å¹´ã‚’ã¾ãŸããƒ‡ã‚£ã‚¹ã‚¯ãƒ­ã‚³ãƒ¡ãƒ³ãƒˆã®å¹´åº¦è¡¨è¨˜:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - When specifying the fiscal year in disclosure comments that span multiple years, always use "yyyyå¹´åº¦".
                - Similarly, for disclosures based on the January-March period, specify the corresponding year.
                - Example:
                    - For a disclosure with a December-end reference, released in January:
                    - ä»Šå¹´åº¦ï¼ˆÃ—ï¼‰ â†’ 2021å¹´åº¦ï¼ˆâ—‹ï¼‰
                    - æ¥å¹´åº¦ï¼ˆÃ—ï¼‰ â†’ 2022å¹´åº¦ï¼ˆâ—‹ï¼‰
            - **Benchmark, Index, and Reference Index Name Formatting:(ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ»å‚è€ƒæŒ‡æ•°ã®åç§°ã®è¡¨è¨˜)
                - Ensure Consistency in Index Terminology:
                    Read the context and identify terms related to "index" (æŒ‡æ•°) within the text. Ensure that these terms are unified and consistently referred to using the correct and standardized terminology.
                    It is important to carefully analyze each mention of "index" to make sure the terminology is consistent throughout the text.
                    Do not modify the original text directly. Instead, provide comments that explain the reasoning behind the proposed changes, especially when identifying inconsistencies or clarifications needed.
                Example Formatting Guidelines:

                    Incorrect format (Ã—): "ISMéè£½é€ æ¥­æ™¯æ³"
                    Correct format (â—‹): "ISMéè£½é€ æ¥­æ™¯æ³æŒ‡æ•°"
                    Incorrect format (Ã—): "MSCIã‚¤ãƒ³ãƒ‰"
                    Correct format (â—‹): "MSCIã‚¤ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"
                    If multiple terms are used to refer to the same index, they should be unified under the correct term. For example, if "MSCIã‚¤ãƒ³ãƒ‰æŒ‡æ•°" and "MSCIã‚¤ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹" are used in different places, they should be unified as "MSCIã‚¤ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹" in the final report to maintain consistency.
                Handling Multiple Terms Referring to the Same Index:

                    If it can be clearly determined that different terms refer to the same index (e.g., "MSCIã‚¤ãƒ³ãƒ‰æŒ‡æ•°" and "MSCIã‚¤ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"), do not modify them but mark them accordingly. These terms should be noted as referring to the same index.
                    Example:
                    Original: "MSCIã‚¤ãƒ³ãƒ‰æŒ‡æ•°" and "MSCIã‚¤ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹"
                    Comment: These are different ways of referring to the same index, so no change is needed.
                Handling Uncertainty in Index Terminology:
                    
                    If there is uncertainty about whether multiple terms refer to the same index (e.g., it is unclear whether "ISMéè£½é€ æ¥­æ™¯æ³" and "ISMéè£½é€ æ¥­æ™¯æ³æŒ‡æ•°" refer to the same index), mark them without modification. Additionally, note that these terms might refer to the same index, but the exact nature of the index should be verified.
                    Example:
                    Original: "ISMéè£½é€ æ¥­æ™¯æ³" and "ISMéè£½é€ æ¥­æ™¯æ³æŒ‡æ•°"
                    Comment: These terms are potentially referring to the same index but require further clarification. Therefore, no changes are made in this case.
                Key Notes:

                Always ensure that consistency is maintained across the report. Even if different names are used for the same index, it is essential to mark them properly and explain that they are different terms for the same entity.
                Consistency applies not only to the formatting of the terms but also to how the terms are presented across the entire document. All references to a given index must follow the same format from the first mention to the last.


            - **ä¸Šæ˜‡ or ä¸‹è½ã«é–¢ã™ã‚‹è¦å› ã‚’æ˜è¨˜:**
                æ–‡ç« å†…ã« ã€Œä¸Šæ˜‡ã€ ã¾ãŸã¯ ã€Œä¸‹è½ã€ ã¨ã„ã†å˜èªãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ãã®è¦å› ã‚’ç‰¹å®šã—ã€æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚
                output:
                ã€Œâ—â—ã¯ä¸Šæ˜‡ï¼ˆã¾ãŸã¯ä¸‹è½ï¼‰ã—ã¾ã—ãŸã€‚(ç†ç”±: â—‹â—‹)ã€
            - **æŒ‡å®šç”¨èªã®è¡¨è¨˜ãƒ«ãƒ¼ãƒ«ã‚’æç¤º:**
                ç‹¬Ifoä¼æ¥­æ™¯æ³æ„ŸæŒ‡æ•° ã¾ãŸã¯ ç‹¬Ifoæ™¯æ³æ„ŸæŒ‡æ•° ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
                ç‹¬Ifoä¼æ¥­æ™¯æ³æ„ŸæŒ‡æ•° ã¾ãŸã¯ç‹¬Ifoæ™¯æ³æ„ŸæŒ‡æ•° ã®è¡¨è¨˜ã«ã¤ã„ã¦ã€æœˆå ±å†…ã§ã®çµ±ä¸€ãƒ«ãƒ¼ãƒ«ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚(Ensure that the original text is not directly modified but follows this guideline.)ã€
                
                ã€Œç‹¬ZEWæ™¯æ°—æœŸå¾…æŒ‡æ•°ã€ã¾ãŸã¯ã€Œç‹¬ZEWæ™¯æ³æ„ŸæŒ‡æ•°ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã™ã‚‹ã€‚
                ã€Œã€ç‹¬ZEWæ™¯æ°—æœŸå¾…æŒ‡æ•°ã€ã¾ãŸã¯ã€ç‹¬ZEWæ™¯æ³æ„ŸæŒ‡æ•°ã€ã®è¡¨è¨˜ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚ZEWå˜ç‹¬ä½¿ç”¨ã®å ´åˆã¯æ‹¬å¼§æ›¸ãä»˜ãã€ã¾ãŸã¯ã€æ¬§å·çµŒæ¸ˆç ”ç©¶ã‚»ãƒ³ã‚¿ãƒ¼ã€ã®ã¿ã¨ã—ã€ã€ZEWã€å˜ç‹¬ä½¿ç”¨ã‚’é¿ã‘ã¦ãã ã•ã„ã€‚(Ensure that the original text is not directly modified but follows this guideline.)ã€
            - **ç‰¹å®šã®é‡‘èç”¨èªã«å¯¾ã™ã‚‹è¡¨è¨˜ãƒ«ãƒ¼ãƒ«ã‚’ç¢ºèªãƒ»é©ç”¨:**

                ã€Œãƒªãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã€‚
                ã€Œãƒªãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯ã€ãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰æŠœã‘ã¦ã€ã¾ã ã‚¤ãƒ³ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ãªã£ã¦ã„ãªã„çŠ¶æ³ã‚’æŒ‡ã—ã¾ã™ã€‚ã€

                ã€Œãƒªã‚¹ã‚¯ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã€‚
                ã€Œãƒªã‚¹ã‚¯ãƒ—ãƒ¬ãƒŸã‚¢ãƒ ã¨ã¯ã€ã‚ã‚‹ãƒªã‚¹ã‚¯è³‡ç”£ã®æœŸå¾…åç›Šç‡ãŒã€åŒæœŸé–“ã®ç„¡ãƒªã‚¹ã‚¯è³‡ç”£ï¼ˆå›½å‚µãªã©ï¼‰ã®åç›Šç‡ã‚’ä¸Šå›ã‚‹å¹…ã‚’æŒ‡ã—ã¾ã™ã€‚ã€

                ã€Œãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã€‚
                ã€Œã€ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ ã€ã®ä½¿ç”¨ã¯é¿ã‘ã€ç›¸å ´ã®ã€å‹¢ã„ã€ã‚„ã€æ–¹å‘æ€§ã€ãªã©ã®è¨€è‘‰ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚ã€

                ã€Œãƒ™ãƒ¼ã‚¸ãƒ¥ãƒ–ãƒƒã‚¯ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã€‚
                ã€Œã€ãƒ™ãƒ¼ã‚¸ãƒ¥ãƒ–ãƒƒã‚¯ã€ã¯ã€FRBï¼ˆç±³é€£é‚¦æº–å‚™åˆ¶åº¦ç†äº‹ä¼šï¼‰ãŒç™ºè¡¨ã—ãŸãƒ™ãƒ¼ã‚¸ãƒ¥ãƒ–ãƒƒã‚¯ï¼ˆåœ°åŒºé€£éŠ€çµŒæ¸ˆå ±å‘Šï¼‰ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚ã‚¹ãƒšãƒ¼ã‚¹ãŒé™ã‚‰ã‚Œã‚‹å ´åˆã¯ã€ã€ãƒ™ãƒ¼ã‚¸ãƒ¥ãƒ–ãƒƒã‚¯ï¼ˆç±³åœ°åŒºé€£éŠ€çµŒæ¸ˆå ±å‘Šï¼‰ã€ã¨ã—ã¦ãã ã•ã„ã€‚ã€

                ã€Œãƒ•ãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã€‚
                ã€Œãƒ•ãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ã¨ã¯ã€ç¨å¼•å¾Œå–¶æ¥­åˆ©ç›Šã«æ¸›ä¾¡å„Ÿå´è²»ã‚’åŠ ãˆã€è¨­å‚™æŠ•è³‡é¡ã¨é‹è»¢è³‡æœ¬ã®å¢—åŠ ã‚’å·®ã—å¼•ã„ãŸã‚‚ã®ã§ã™ã€‚ã€
                
                ã€Œã‚·ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ»ãƒªã‚¹ã‚¯ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã€‚
                ã€Œã‚·ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ»ãƒªã‚¹ã‚¯ã¨ã¯ã€å€‹åˆ¥ã®é‡‘èæ©Ÿé–¢ã®æ”¯æ‰•ä¸èƒ½ã‚„ç‰¹å®šã®å¸‚å ´ãƒ»æ±ºæ¸ˆã‚·ã‚¹ãƒ†ãƒ ç­‰ã®æ©Ÿèƒ½ä¸å…¨ãŒã€ä»–ã®é‡‘èæ©Ÿé–¢ã€å¸‚å ´ã€ã¾ãŸã¯é‡‘èã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã«æ³¢åŠã™ã‚‹ãƒªã‚¹ã‚¯ã‚’æŒ‡ã—ã¾ã™ã€‚ã€

                ã€Œã‚¯ãƒ¬ã‚¸ãƒƒãƒˆï¼ˆä¿¡ç”¨ï¼‰å¸‚å ´ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’è¡¨ç¤ºã€‚
                ã€Œã‚¯ãƒ¬ã‚¸ãƒƒãƒˆï¼ˆä¿¡ç”¨ï¼‰å¸‚å ´ã¨ã¯ã€ä¿¡ç”¨ãƒªã‚¹ã‚¯ï¼ˆè³‡é‡‘ã®å€Ÿã‚Šæ‰‹ã®ä¿¡ç”¨åº¦ãŒå¤‰åŒ–ã™ã‚‹ãƒªã‚¹ã‚¯ï¼‰ã‚’å†…åŒ…ã™ã‚‹å•†å“ï¼ˆã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå•†å“ï¼‰ã‚’å–å¼•ã™ã‚‹å¸‚å ´ã®ç·ç§°ã§ã‚ã‚Šã€ä¼æ¥­ã®ä¿¡ç”¨ãƒªã‚¹ã‚¯ã‚’å–å¼•ã™ã‚‹å¸‚å ´ã§ã™ã€‚ã€
            - **ç‰¹å®šã®é‡‘èç”¨èªã«å¯¾ã—ã€æ¬„å¤–ã«æ³¨è¨˜ã‚’åŠ ãˆã‚‹æŒ‡ç¤ºã‚’è¡¨ç¤º:**
                ã€Œæ ¼ä»˜åˆ¥ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                    æ ¼ä»˜åˆ¥ -> æ ¼ä»˜åˆ¥
                ã€Œæ ¼ä»˜æ©Ÿé–¢ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                    æ ¼ä»˜æ©Ÿé–¢ -> æ ¼ä»˜æ©Ÿé–¢
                ã€Œçµ„å…¥æ¯”ç‡ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                    çµ„å…¥æ¯”ç‡ -> çµ„å…¥æ¯”ç‡
                ã€Œå¼•ç· ç­–ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                    å¼•ç· ç­– -> å¼•ç· ç­–
                ã€Œå›½å‚µè²·å…¥ã‚Œã‚ªãƒšã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                    å›½å‚µè²·å…¥ã‚Œã‚ªãƒš -> å›½å‚µè²·å…¥ã‚ªãƒš

                ã€ŒæŠ•è³‡é©æ ¼å‚µã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                ã€Œâ€»æ¬„å¤–ã«æ³¨è¨˜: æŠ•è³‡é©æ ¼å‚µã¨ã¯ã€æ ¼ä»˜æ©Ÿé–¢ã«ã‚ˆã£ã¦æ ¼ä»˜ã‘ã•ã‚ŒãŸå…¬ç¤¾å‚µã®ã†ã¡ã€å‚µå‹™ã‚’å±¥è¡Œã™ã‚‹èƒ½åŠ›ãŒååˆ†ã«ã‚ã‚‹ã¨è©•ä¾¡ã•ã‚ŒãŸå…¬ç¤¾å‚µã‚’æŒ‡ã—ã¾ã™ã€‚ã€

                ã€Œãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                ã€Œâ€»æ¬„å¤–ã«æ³¨è¨˜: ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã¨ã¯ã€é‡‘åˆ©ãŒä¸€å®šã®å‰²åˆã§å¤‰å‹•ã—ãŸå ´åˆã€å‚µåˆ¸ã®ä¾¡æ ¼ãŒã©ã®ç¨‹åº¦å¤‰åŒ–ã™ã‚‹ã‹ã‚’ç¤ºã™æŒ‡æ¨™ã§ã™ã€‚ã“ã®å€¤ãŒå¤§ãã„ã»ã©ã€é‡‘åˆ©å¤‰å‹•ã«å¯¾ã™ã‚‹å‚µåˆ¸ä¾¡æ ¼ã®å¤‰å‹•ç‡ãŒå¤§ãããªã‚Šã¾ã™ã€‚ã€

                ã€Œãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‚µã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                ã€Œâ€»æ¬„å¤–ã«æ³¨è¨˜: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã¯ã€ä¸€èˆ¬çš„ã«å‚µåˆ¸ã®åˆ©æ‰•ã„ãŠã‚ˆã³å…ƒæœ¬è¿”æ¸ˆã®ä¸å±¥è¡Œã€ã¾ãŸã¯é…å»¶ãªã©ã‚’æŒ‡ã—ã€ã“ã®ã‚ˆã†ãªçŠ¶æ…‹ã«ã‚ã‚‹å‚µåˆ¸ã‚’ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‚µã€ã¨ã„ã„ã¾ã™ã€‚ã€

                ã€Œãƒ‡ã‚£ã‚¹ãƒˆãƒ¬ã‚¹å‚µåˆ¸ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                ã€Œâ€»æ¬„å¤–ã«æ³¨è¨˜: ãƒ‡ã‚£ã‚¹ãƒˆãƒ¬ã‚¹å‚µåˆ¸ã¨ã¯ã€ä¿¡ç”¨äº‹ç”±ãªã©ã«ã‚ˆã‚Šä¾¡æ ¼ãŒè‘—ã—ãä¸‹è½ã—ãŸå‚µåˆ¸ã‚’æŒ‡ã—ã¾ã™ã€‚ã€
                
                ã€Œã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ã€ ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆ:
                ã€Œâ€»æ¬„å¤–ã«æ³¨è¨˜: ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ï¼ˆåˆ©å›ã‚Šæ›²ç·šï¼‰ã¨ã¯ã€æ¨ªè»¸ã«æ®‹å­˜å¹´æ•°ã€ç¸¦è»¸ã«åˆ©å›ã‚Šã‚’ã¨ã£ãŸåº§æ¨™ã«ã€å‚µåˆ¸åˆ©å›ã‚Šã‚’ç‚¹æã—ã¦çµã‚“ã æ›²ç·šã®ã“ã¨ã‚’æŒ‡ã—ã¾ã™ã€‚ã€

            - **çµ„å…¥ä¸Šä½10éŠ˜æŸ„ã€ã«ã¤ã„ã¦è¨˜è¿°ãŒã‚ã‚‹å ´åˆã€ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨:**
                ã€Œçµ„å…¥ä¸Šä½10éŠ˜æŸ„ã‚’è¶…ãˆã‚‹ä¿æœ‰éŠ˜æŸ„ï¼ˆå€‹åˆ¥éŠ˜æŸ„ã®ç‰¹å®šãŒå¯èƒ½ãªå­ä¼šç¤¾åç­‰ã‚’å«ã‚€ï¼‰ã¯åŸå‰‡ã¨ã—ã¦é–‹ç¤ºç¦æ­¢ã§ã‚ã‚‹ã€ã“ã¨ã‚’æ˜ç¤ºã€‚
                ãŸã ã—ã€ç¤¾å†…è¦ç¨‹ã«åŸºã¥ãé–‹ç¤ºãŒèªã‚ã‚‰ã‚Œã¦ã„ã‚‹ãƒ•ã‚¡ãƒ³ãƒ‰ã¯ä¾‹å¤–ã¨ã™ã‚‹ã“ã¨ã‚’ä¼ãˆã‚‹ã€‚

            - **å¹´åº¦è¡¨è¨˜:**
                - Use four-digit notation for years.(Ensure that the original text is not directly modified but follows this guideline.)
                - Example: 22å¹´ï¼ˆÃ—ï¼‰ â†’ 2022å¹´ï¼ˆâ—‹ï¼‰

            - **å‰å¹´æ¯” or å‰å¹´åŒæœˆï¼ˆåŒæœŸï¼‰æ¯”ã®çµ±ä¸€:**
                - ã€Œå‰å¹´åŒæœˆï¼ˆåŒæœŸï¼‰æ¯”ã€ã«çµ±ä¸€ã€‚
                - é€šå¹´ã®æ¯”è¼ƒã«ã¯ã€Œå‰å¹´æ¯”ã€ã®ä½¿ç”¨å¯ã€‚
                - Ensure that the original text is not directly modified but follows this guideline.
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                    - Example:
                        å‰å¹´æ¯”+3.0%ï¼ˆÃ—ï¼‰ â†’ å‰å¹´åŒæœˆæ¯”+3.0%ï¼ˆâ—‹ï¼‰
                        2023å¹´ã®GDPã¯å‰å¹´æ¯”+3.0ï¼…ï¼ˆâ—‹ï¼‰

            - **å¹´ã‚’ã¾ãŸã„ã çµŒæ¸ˆæŒ‡æ¨™ã®è¨˜è¼‰:**
                - ã‚³ãƒ¡ãƒ³ãƒˆå†…ã®åˆå‡ºã®ã¿ã«è¨˜è¼‰ã™ã‚‹ã€‚(In the case where there is a description of the economic indicator over the year, it is described only in the first comment.)
                    - Example:
                        - 2023å¹´12æœˆã®CPIã¯ï½ï¼ˆâ—‹ï¼‰
                        - ä¸€æ–¹2024å¹´1æœˆã®ãƒ¦ãƒ¼ãƒ­åœPMIã¯ï½ ï¼ˆâ—‹ï¼‰
                        - 10-12æœˆæœŸã®GDPã¯ï½ï¼ˆâ—‹ï¼‰
            - **çµŒæ¸ˆæŒ‡æ¨™ã«ã¤ã„ã¦:**
                    -"åŠ é€Ÿ" ã®å¯¾è±¡ã‚’æ˜ç¢ºã«è¨˜è¼‰ã™ã‚‹ã“ã¨ã€‚æ–‡è„ˆã‚’è€ƒæ…®ã—ã€"åŠ é€Ÿ" ã®å¯¾è±¡ã‚’é©åˆ‡ã«è£œã†ã€‚
                        æ–‡è„ˆã«å¿œã˜ã¦ã€"ä½•ãŒåŠ é€Ÿã—ãŸã®ã‹" ã‚’åˆ¤æ–­ã—ã€é©åˆ‡ãªå˜èªã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚
                        ä¿®æ­£ãƒ«ãƒ¼ãƒ«:

                        - å‰æœˆã‹ã‚‰åŠ é€Ÿã—ã¾ã—ãŸï¼ˆÃ—ï¼‰ â†’ ä½•ãŒåŠ é€Ÿã—ãŸã®ã‹ã‚’æ˜è¨˜ï¼ˆâ—‹ï¼‰
                        Exsample: ã€Œå‰æœˆã‹ã‚‰ä¸Šæ˜‡ãŒåŠ é€Ÿã—ã¾ã—ãŸã€ â†’ ã€Œæ™¯æ°—å›å¾©ã®åŠ é€Ÿï¼ˆâ—‹ï¼‰ã€
                        - å‰æœˆã‹ã‚‰ä¸Šæ˜‡ãŒåŠ é€Ÿã—ã¾ã—ãŸï¼ˆÃ—ï¼‰ â†’ å…·ä½“çš„ãªçµŒæ¸ˆæ´»å‹•ã‚’æ˜è¨˜ï¼ˆâ—‹ï¼‰
                        Exsample: ã€Œæ™¯æ°—åŠ é€Ÿï¼ˆâ—‹ï¼‰ã€ã€Œæ¶ˆè²»ã®å›å¾©ãŒåŠ é€Ÿï¼ˆâ—‹ï¼‰ã€ã€ŒæŠ•è³‡ã®æ‹¡å¤§ãŒåŠ é€Ÿï¼ˆâ—‹ï¼‰ã€
                        - çµŒæ¸ˆï¼ˆÃ—ï¼‰ â†’ æ™¯æ°—ï¼ˆâ—‹ï¼‰ï¼ˆ"çµŒæ¸ˆ" ã§ã¯ãªã "æ™¯æ°—" ã‚’ç”¨ã„ã‚‹ï¼‰
                        Exsample: 
                        å‰æœˆã‹ã‚‰åŠ é€Ÿã—ã¾ã—ãŸã€‚ï¼ˆÃ—ï¼‰-> ä¼æ¥­ã®è¨­å‚™æŠ•è³‡ãŒåŠ é€Ÿã—ã¾ã—ãŸã€‚ï¼ˆâ—‹ï¼‰
                        çµŒæ¸ˆåŠ é€ŸãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ï¼ˆÃ—ï¼‰-> æ™¯æ°—åŠ é€ŸãŒè¦‹ã‚‰ã‚Œã¾ã™ã€‚ï¼ˆâ—‹ï¼‰
                        æ¶ˆè²»ãŒå‰æœˆã‹ã‚‰åŠ é€Ÿã—ã¾ã—ãŸã€‚ï¼ˆÃ—ï¼‰-> å€‹äººæ¶ˆè²»ã®æ‹¡å¤§ãŒåŠ é€Ÿã—ã¾ã—ãŸã€‚ï¼ˆâ—‹ï¼‰
                        ã‚¤ãƒ³ãƒ•ãƒ¬ãŒå‰æœˆã‹ã‚‰åŠ é€Ÿã—ã¾ã—ãŸã€‚ï¼ˆÃ—ï¼‰-> ç‰©ä¾¡ä¸Šæ˜‡ã®ã‚¹ãƒ”ãƒ¼ãƒ‰ãŒåŠ é€Ÿã—ã¾ã—ãŸã€‚ï¼ˆâ—‹ï¼‰


                    - æ—¥ä»˜ãƒ»å›½åã®æ˜è¨˜:
                        ã„ã¤ã®ã‚‚ã®ã‹ç‰¹å®šã§ãã‚‹å ´åˆã€â—‹æœˆã®ã‚‚ã®ã‹ã‚’æ˜è¨˜ã™ã‚‹ã€‚ä¾‹: ã€Œ10æœˆã®è£½é€ æ¥­PMIï¼ˆè³¼è²·æ‹…å½“è€…æ™¯æ°—æŒ‡æ•°ï¼‰ã¯ï½ã€
                        å¿…è¦ã«å¿œã˜ã¦å›½åã‚‚è¨˜è¼‰ã™ã‚‹ã€‚ï¼ˆæ–‡è„ˆã«å¿œã˜ã¦è¨˜è¼‰ã—ã¦ã„ã‚Œã°ã€ä½ç½®ã¯å•ã‚ãªã„ï¼‰
                    - æ—¥ä»˜ãƒ»å›½åã®å¤‰æ›ãƒ«ãƒ¼ãƒ«:

                        **ã€Œä¸‹æ—¬ã€ã€Œä¸Šæ—¬ã€**ã¨è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã€æ–‡è„ˆã‹ã‚‰é©åˆ‡ãªæ—¥ä»˜ã«å¤‰æ›´ã™ã‚‹ã€‚
                        **ã€Œãƒ¦ãƒ¼ãƒ­åœã€**ã¨è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã€æ–‡è„ˆã«å¿œã˜ã¦é©åˆ‡ãªå›½åã«ç½®ãæ›ãˆã‚‹ã€‚
                        Exsample:

                        ä¿®æ­£å‰: ã€Œä¸‹æ—¬ã¯ã€ãƒ¦ãƒ¼ãƒ­åœç·åˆPMIï¼ˆè³¼è²·æ‹…å½“è€…æ™¯æ°—æŒ‡æ•°ï¼‰ãŒâ€¦ã€

                        ä¿®æ­£å¾Œ: ã€Œ10æœˆä¸‹æ—¬ã®ãƒ‰ã‚¤ãƒ„ç·åˆPMIï¼ˆè³¼è²·æ‹…å½“è€…æ™¯æ°—æŒ‡æ•°ï¼‰ãŒâ€¦ã€

                        ä¿®æ­£å‰: ã€Œä¸Šæ—¬ã¯ã€ãƒ¦ãƒ¼ãƒ­åœç·åˆPMIï¼ˆè³¼è²·æ‹…å½“è€…æ™¯æ°—æŒ‡æ•°ï¼‰ãŒâ€¦ã€

                        ä¿®æ­£å¾Œ: ã€Œ10æœˆä¸Šæ—¬ã®ãƒ•ãƒ©ãƒ³ã‚¹ç·åˆPMIï¼ˆè³¼è²·æ‹…å½“è€…æ™¯æ°—æŒ‡æ•°ï¼‰ãŒâ€¦ã€

            - **æ¥­ç•ŒæŒ‡æ•°ã®è¡¨è¨˜:**
                - å¿…ãšå¯¾è±¡ã¨ãªã‚‹ã€Œæœˆã€ã‚’æ˜è¨˜ã™ã‚‹ã€‚
                - æœˆãŒãªã„å ´åˆã€æœ€è¿‘3ãƒ¶æœˆä»¥å†…ã‹ã€3ãƒ¶æœˆä»¥ä¸Šå‰ã®ã‚‚ã®ã‹ã‚’ç¢ºèªã™ã‚‹ã€‚
                - å¿…è¦ã«å¿œã˜ã¦å›½åã‚‚è¨˜è¼‰ã™ã‚‹ã€‚ï¼ˆæ–‡è„ˆã«å¿œã˜ã¦ä½ç½®ã¯è‡ªç”±ï¼‰
                    - Example:
                        - è£½é€ æ¥­PMIï¼ˆè³¼è²·æ‹…å½“è€…æ™¯æ°—æŒ‡æ•°ï¼‰ï¼ˆÃ—ï¼‰ â†’ 10æœˆã®è£½é€ æ¥­PMIï¼ˆâ—‹ï¼‰
                        - ç›´è¿‘3ãƒ¶æœˆä»¥å†…ã®æŒ‡æ•°ã¯æ˜ç¤ºçš„ã«ã€Œâ—‹æœˆã€ã¨è¨˜è¼‰ã™ã‚‹ã€‚ï¼ˆä¾‹:12æœˆã®CPIï¼‰
                        - 3ãƒ¶æœˆä»¥ä¸Šå‰ã®æŒ‡æ•°ã¯ã€æ¯”è¼ƒã®æ–‡è„ˆã‚’æ˜ç¢ºã«ã™ã‚‹ã€‚ï¼ˆä¾‹:2023å¹´10æœˆã®GDPæˆé•·ç‡ï¼‰
                        - ãƒ¦ãƒ¼ãƒ­åœã®10æœˆPMIã¯ï½ï¼ˆâ—‹ï¼‰
            - **ã‚«ã‚¿ã‚«ãƒŠè¡¨è¨˜ã®çµ±ä¸€:**
                Katakana representation of foreign words should be unified within the document.
                    Ensure that the Katakana form is consistent throughout the text, and choose one version for the entire document.
                    Modification reason: To maintain consistency in the usage of Katakana for foreign words.
                    Example of text modifications:

                    ã‚µã‚¹ãƒ†ãƒŠãƒ–ãƒ« (Ã—) â†’ ã‚µã‚¹ãƒ†ã‚£ãƒŠãƒ–ãƒ« (â—‹)

                    ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ (Ã—) â†’ ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ (â—‹)
            
            - **ãƒ¬ãƒ³ã‚¸ã®è¡¨è¨˜ã«ã¤ã„ã¦è¡¨è¨˜:**
                - Always append "%" when indicating a range.(Ensure that the original text is not directly modified but follows this guideline.)
                - Make modifications directly in this article and explain the reasons for the modifications.
                - Example: -1ï½0.5%ï¼ˆÃ—ï¼‰ â†’ -1%ï½0.5%ï¼ˆâ—‹ï¼‰
            - **å„Ÿé‚„ã«é–¢ã™ã‚‹è¨˜è¼‰:**
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                - æœ€çµ‚ãƒªãƒªãƒ¼ã‚¹ã®1ãƒµæœˆç¨‹å‰ã‚ˆã‚Šã€å„Ÿé‚„ã«é–¢ã™ã‚‹å†…å®¹ã‚’å…¥ã‚Œã‚‹ã“ã¨ã€‚
                - ä¾‹ï¼‰å½“ãƒ•ã‚¡ãƒ³ãƒ‰ã¯ã€â—â—æœˆâ—â—æ—¥ã«ä¿¡è¨—ã®çµ‚äº†æ—¥ï¼ˆå„Ÿé‚„æ—¥orç¹°ä¸Šå„Ÿé‚„æ—¥ï¼‰ã‚’è¿ãˆã‚‹äºˆå®šã§ã™ã€‚
                - â€»ï¼ˆï¼‰å†…ã¯ã€å®šæ™‚å„Ÿé‚„ã®å ´åˆã«ã¯å„Ÿé‚„æ—¥ã€ç¹°ä¸Šå„Ÿé‚„ã®å ´åˆã«ã¯ç¹°ä¸Šå„Ÿé‚„æ—¥ã¨ã™ã‚‹ã€‚
            - **å€‹åˆ¥ä¼æ¥­åã®è¡¨è¨˜:**
                - æŠ•è³‡ç’°å¢ƒç­‰ã«ãŠã„ã¦ã¯ã€å€‹åˆ¥ä¼æ¥­ã®åç§°ã‚’ä½¿ã‚ãªã„è¡¨ç¾ã‚’å¿ƒæ›ã‘ã‚‹ã€‚
                - ä¾‹:ã‚¹ã‚¤ã‚¹é‡‘èå¤§æ‰‹ã‚¯ãƒ¬ãƒ‡ã‚£ãƒ»ã‚¹ã‚¤ã‚¹ï¼ˆÃ—ï¼‰ â†’ ã‚¹ã‚¤ã‚¹ã®å¤§æ‰‹é‡‘èã‚°ãƒ«ãƒ¼ãƒ—ï¼ˆâ—‹ï¼‰
            - **ãƒ—ãƒ©ã‚¹ã«å¯„ä¸/å½±éŸ¿ã®è¡¨è¨˜:**
                - Do not Make modifications directly in this article and explain the reasons for the modifications.
                - ã€Œãƒ—ãƒ©ã‚¹ã«å¯„ä¸ã€ã¾ãŸã¯ã€Œãƒ—ãƒ©ã‚¹ã«å½±éŸ¿ã€ã©ã¡ã‚‰ã‚‚å¯ã€‚
                - ã¾ãŸã¯ã€ã€Œï½ãƒ—ãƒ©ã‚¹è¦å› ã¨ãªã‚‹ã€ã¨è¡¨è¨˜ã€‚
            - **ãƒã‚¤ãƒŠã‚¹ã«å½±éŸ¿ã®è¡¨è¨˜:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - ã€Œãƒã‚¤ãƒŠã‚¹ã«å¯„ä¸ã€ï¼ˆÃ—ï¼‰â†’ã€Œãƒã‚¤ãƒŠã‚¹ã«å½±éŸ¿ã€ï¼ˆâ—‹ï¼‰
                - ãƒã‚¤ãƒŠã‚¹ã®éš›ã¯ã€Œå¯„ä¸ã€ã¯ä½¿ç”¨ã—ãªã„ã€‚
                - ã¾ãŸã¯ã€ã€Œï½ãƒã‚¤ãƒŠã‚¹è¦å› ã¨ãªã‚‹ã€ã¨è¡¨è¨˜ã€‚
            - **åˆ©å›ã‚Šã®è¡¨è¨˜:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - åˆ©å›ã‚Šã¯ã€Œä¸Šæ˜‡ï¼ˆä¾¡æ ¼ã¯ä¸‹è½ï¼‰ã€ã¾ãŸã¯ã€Œä½ä¸‹ï¼ˆä¾¡æ ¼ã¯ä¸Šæ˜‡ï¼‰ã€ã¨è¡¨è¨˜ã€‚
            - **ä½ä¸‹ã¨ä¸‹è½ã®è¡¨è¨˜:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - å‚µåˆ¸åˆ©å›ã‚Šã¯ã€Œä½ä¸‹ï¼ˆâ—‹ï¼‰ã€ã¨è¡¨è¨˜ã—ã€ã€Œä¸‹è½ï¼ˆÃ—ï¼‰ã€ã¯ä½¿ç”¨ã—ãªã„ã€‚
                - ä¾¡æ ¼ã¯ã€Œä¸‹è½ï¼ˆâ—‹ï¼‰ã€ã¨è¡¨è¨˜ã—ã€ã€Œä½ä¸‹ï¼ˆÃ—ï¼‰ã€ã¯ä½¿ç”¨ã—ãªã„ã€‚
                - é‡‘åˆ©ã®ã€Œä½ä¸‹ï¼ˆã€‡ï¼‰ã€ã¨è¡¨è¨˜ã—ã€ã€Œä¸‹è½ï¼ˆÃ—ï¼‰ã€ã¯ä½¿ç”¨ã—ãªã„ã€‚

            - **è³‡é‡‘æµå‡ºå…¥ã®è¡¨è¨˜:**
                - Do notMake modifications directly in this article and explain the reasons for the modifications.
                - ã€Œå¤–å›½äººæŠ•è³‡å®¶ã®è³‡é‡‘æµå‡ºã€ã‚’ã€Œå¤–å›½äººæŠ•è³‡å®¶ã‹ã‚‰ã®è³‡é‡‘æµå…¥ã€ã¨è¨˜è¼‰ã€‚

            - **ï¼ˆé‡‘åˆ©ã®ï¼‰å…ˆé«˜æ„Ÿ/å…ˆé«˜è¦³ ã®è¡¨è¨˜çµ±ä¸€:**
                - æ–‡ä¸­ã«ã€Œå…ˆé«˜è¦³ã€ã¨ã„ã†è¡¨è¨˜ãŒã‚ã‚‹å ´åˆã§ã‚‚ã€åŸæ–‡ã¯ä¿®æ­£ã—ãªã„ã§ãã ã•ã„ã€‚
                - ãã®ä»£ã‚ã‚Šã€ã€Œå…ˆé«˜è¦³ã€ã®ç›´å¾Œã«ã€Œä¿®æ­£ææ¡ˆã€ã¨ã—ã¦ã€ã€Œå…ˆé«˜æ„Ÿã€ã¸ã®çµ±ä¸€ç†ç”±ã‚’æç¤ºã—ã¦ãã ã•ã„ã€‚
                - è¡¨è¨˜ãŒã™ã§ã«ã€Œå…ˆé«˜æ„Ÿã€ã§ã‚ã‚‹å ´åˆã¯ã€ä½•ã‚‚è¿½è¨˜ã›ãšãã®ã¾ã¾ã«ã—ã¦ãã ã•ã„ã€‚

                - è¡¨ç¤ºãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆï¼ˆä¾‹ï¼‰:
                    å…ˆé«˜è¦³<span style="color:red;">ï¼ˆä¿®æ­£ç†ç”±: ç”¨èªã®çµ±ä¸€ <s style="background:yellow;color:red">å…ˆé«˜è¦³</s> â†’ å…ˆé«˜æ„Ÿï¼‰</span>

                - å¿…ãšåŸæ–‡ã®æ§‹æˆã¨æ–‡è„ˆã‚’ä¿æŒã—ã€æ§‹æ–‡ã‚’å£Šã•ãšã€ä¿®æ­£ç†ç”±ã¯è£œè¶³çš„ã«å¾Œã‚ã«è¿½è¨˜ã—ã¦ãã ã•ã„ã€‚
                
            - **ãƒãƒ¼ãƒˆãƒ•ã‚©ãƒªã‚ªã®è¡¨è¨˜:**
                - Make modifications directly in this article and explain the reasons for the modifications.
                - ã€Œâ—â—ã¸ã®çµ„ã¿å…¥ã‚Œï¼ˆÃ—ï¼‰ã€ã§ã¯ãªãã€ã€Œâ—â—ã®çµ„ã¿å…¥ã‚Œï¼ˆâ—‹ï¼‰ã€ã¨è¡¨è¨˜ã€‚
                - ã€Œã¸ã®æŠ•è³‡æ¯”ç‡ã€ã¯ä½¿ç”¨å¯èƒ½ã€‚
                
            - **æ§‹æˆæ¯”ã®0ï¼…ã®è¡¨è¨˜:
                - ã€Œ0ï¼…ç¨‹åº¦ã€orã€Œã‚¼ãƒ­ï¼…ç¨‹åº¦ã€ã®è¡¨è¨˜ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨
                - å¤‰æ›´å‰è¡¨è¨˜: æ§‹æˆæ¯”ã¯0ï¼…ã§ã‚ã‚‹
                - çµ±ä¸€å¾Œè¡¨è¨˜: æ§‹æˆæ¯”ã¯0ï¼…ç¨‹åº¦
                - Append a correction reason in the following format:
                        `<span style="color:red;">å¤‰æ›´å‰è¡¨è¨˜</span> (<span>ä¿®æ­£ç†ç”±: æ§‹æˆæ¯”è¡¨è¨˜ <s style="background:yellow;color:red">å¤‰æ›´å‰è¡¨è¨˜</s> â†’ çµ±ä¸€å¾Œè¡¨è¨˜</span>)`
                    
                    Example:
                    Input: æ§‹æˆæ¯”ã¯0ï¼…ã§ã‚ã‚‹
                    Output: 
                    <span style="color:red;">æ§‹æˆæ¯”ã¯0ï¼…ã§ã‚ã‚‹</span> 
                    (<span>ä¿®æ­£ç†ç”±: æ§‹æˆæ¯”è¡¨è¨˜ <s style="background:yellow;color:red">æ§‹æˆæ¯”ã¯0ï¼…ã§ã‚ã‚‹</s> â†’ æ§‹æˆæ¯”ã¯0ï¼…ç¨‹åº¦ã§ã‚ã‚‹ã€‚</span>)ã§å£²ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚

                
            - **'æŠ•è³‡ç’°å¢ƒã®è¨˜è¿°:** 
                - Make modifications directly in this article and explain the reasons for the modifications.
                **ã€Œå…ˆæœˆã®æŠ•è³‡ç’°å¢ƒã€**ã®éƒ¨åˆ†ã§ã€Œå…ˆæœˆæœ«ã€ã®è¨˜è¿°ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ã€Œå‰æœˆæœ«ã€ã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚
                - Ensure that the original text is directly modified and follows this guideline.
                Example:
                ä¿®æ­£å‰: å…ˆæœˆæœ«ã®å¸‚å ´å‹•å‘ã‚’åˆ†æã™ã‚‹ã¨â€¦
                ä¿®æ­£å¾Œ: å‰æœˆæœ«ã®å¸‚å ´å‹•å‘ã‚’åˆ†æã™ã‚‹ã¨â€¦

            - **é€šè²¨è¡¨è¨˜ã®çµ±ä¸€:**
                - Standardize currency notation across the document.
                    - The first appearance of any currency symbol (e.g., ãƒ‰ãƒ«, $, å††, JPY) will be the standard.
                    - All following occurrences of that currency must match this format.

                    - For example, if "100ãƒ‰ãƒ«" appears first, then all future "$100" will be rewritten to "100ãƒ‰ãƒ«" for consistency.
                    - If "$100" appears first, then "100ãƒ‰ãƒ«" should be rewritten as "$100".

                    - Always apply this rule in the direction of "first-appeared" format.
                    - Append a correction reason in the following format:
                        `<span style="color:red;">çµ±ä¸€å¾Œè¡¨è¨˜</span> (<span>ä¿®æ­£ç†ç”±: é€šè²¨è¡¨è¨˜ã®çµ±ä¸€ <s style="background:yellow;color:red">å¤‰æ›´å‰è¡¨è¨˜</s> â†’ çµ±ä¸€å¾Œè¡¨è¨˜</span>)`
                    
                    Example:
                    Input: ã“ã®ãƒãƒƒã‚°ã¯100ãƒ‰ãƒ«ã§ã™ãŒã€ã‚¢ãƒ¡ãƒªã‚«ã§ã¯$100ã§å£²ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚
                    Output:
                    ã“ã®ãƒãƒƒã‚°ã¯100ãƒ‰ãƒ«ã§ã™ãŒã€ã‚¢ãƒ¡ãƒªã‚«ã§ã¯
                    <span style="color:red;">100ãƒ‰ãƒ«</span>
                    (<span>ä¿®æ­£ç†ç”±: é€šè²¨è¡¨è¨˜ã®çµ±ä¸€ <s style="background:yellow;color:red">$100</s> â†’ 100ãƒ‰ãƒ«</span>)ã§å£²ã‚‰ã‚Œã¦ã„ã¾ã™ã€‚


            **Preferred and Recommended Terminology (ç½®ãæ›ãˆãŒå¿…è¦ãªç”¨èª/è¡¨ç¾):**
            - **ç¬¬1å››åŠæœŸ:**
                - Ensure the period is clearly stated.
                - Example: 18å¹´ç¬¬4å››åŠæœŸï¼ˆÃ—ï¼‰ â†’ 2018å¹´10-12æœˆæœŸï¼ˆâ—‹ï¼‰
            - **ç´„â—‹ï¼…ç¨‹åº¦:**
                - Do not use "ç´„" (approximately) and "ç¨‹åº¦" (extent) together. Choose either one.
                - Example: ç´„â—‹ï¼…ç¨‹åº¦ï¼ˆÃ—ï¼‰ â†’ ç´„â—‹ï¼… or â—‹ï¼…ç¨‹åº¦ï¼ˆâ—‹ï¼‰
            - **å¤§æ‰‹ä¼æ¥­è¡¨è¨˜ã®æ˜ç¢ºåŒ–**  
                **Correction Rule:**
                - If a sentence contains vague expressions likeã€Œå¤§æ‰‹â—‹â—‹ã€, analyze the context to determine what type of company is being referred to.
                - Rewrite it in the format:ã€Œå¤§æ‰‹â—‹â—‹ä¼šç¤¾ã€/ã€Œå¤§æ‰‹â—‹â—‹ä¼æ¥­ã€/ã€Œå¤§æ‰‹â—‹â—‹ãƒ¡ãƒ¼ã‚«ãƒ¼ã€depending on the companyâ€™s nature.
                - Use context clues (e.g., product type, industry references) to guess the appropriate company category (e.g., ä¸å‹•ç”£, è‡ªå‹•è»Š, é›»æ©Ÿ, é‡‘è).
                - Append a correction reason in this format:
                `<span style="color:red;">Changed Expression</span> (<span>ä¿®æ­£ç†ç”±: è¡¨ç¾ã®æ˜ç¢ºåŒ– <s style="background:yellow;color:red">Original Expression</s> â†’ Changed Expression</span>)`

                **Example Input:**
                - å¤§æ‰‹ã¯æ¥­ç•Œå…¨ä½“ã«å½±éŸ¿åŠ›ã‚’æŒã¤ã€‚
                - å¤§æ‰‹ãŒæ–°ã—ã„åŠå°ä½“ã‚’ç™ºè¡¨ã—ãŸã€‚

                **Example Output:**
                - <span style="color:red;">å¤§æ‰‹ä¸å‹•ç”£ä¼šç¤¾</span> (<span>ä¿®æ­£ç†ç”±: è¡¨ç¾ã®æ˜ç¢ºåŒ– <s style="background:yellow;color:red">å¤§æ‰‹</s> â†’ å¤§æ‰‹ä¸å‹•ç”£ä¼šç¤¾</span>) ã¯æ¥­ç•Œå…¨ä½“ã«å½±éŸ¿åŠ›ã‚’æŒã¤ã€‚
                - <span style="color:red;">å¤§æ‰‹åŠå°ä½“ãƒ¡ãƒ¼ã‚«ãƒ¼</span> (<span>ä¿®æ­£ç†ç”±: è¡¨ç¾ã®æ˜ç¢ºåŒ– <s style="background:yellow;color:red">å¤§æ‰‹</s> â†’ å¤§æ‰‹åŠå°ä½“ãƒ¡ãƒ¼ã‚«ãƒ¼</span>) ãŒæ–°ã—ã„åŠå°ä½“ã‚’ç™ºè¡¨ã—ãŸã€‚

                **Important Notes:**
                - Always preserve the original sentence structure and paragraph formatting.
                - Only make corrections whenã€Œâ—‹â—‹å¤§æ‰‹ã€is ambiguous and can be clarified using contextual information.
                - Do not modify proper nouns or known company names (e.g., ãƒˆãƒ¨ã‚¿, ã‚½ãƒ‹ãƒ¼).

            - **å…¥åŠ›ä¾‹:**  
                - ã€Œå¤§æ‰‹ãƒ¡ãƒ¼ã‚«ãƒ¼/ä¼šç¤¾/ä¼æ¥­ã€  
                - **å‡ºåŠ›:** ã€Œå¤§æ‰‹ä¸å‹•ç”£ä¼šç¤¾ã€å¤§æ‰‹åŠå°ä½“ãƒ¡ãƒ¼ã‚«ãƒ¼ã€  
            - **The actual company name must be found and converted in the article
            - **å…ˆæœˆ/å‰æœˆã®è¡¨è¨˜:
                - 1ãƒµæœˆå‰ã«ã¤ã„ã¦è¨€åŠã™ã‚‹å ´åˆã¯ã€ã€Œå‰æœˆã€ã‚’ä½¿ç”¨ã€‚
            å‰æœŸæ¯”â—‹ï¼…ã®è¡¨è¨˜:

            - **å‰æœŸæ¯”å¹´ç‡â—‹ï¼…:**
                - åŸºæœ¬çš„ã«ã€æœŸé–“æ¯”è¼ƒã®ä¼¸ç‡ã¯ã€Œå¹´ç‡ã€ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
                - ä¸»ã«çµŒæ¸ˆçµ±è¨ˆç­‰ã§ä¸€èˆ¬çš„ã«å‰æœŸæ¯”ã§å¹´ç‡æ›ç®—ã•ã‚Œã¦ã„ã‚‹ã‚‚ã®ã«ã¤ã„ã¦ã¯ã€ã€Œå‰æœŸæ¯”å¹´ç‡â—‹ï¼…ã€ã¨è¡¨è¨˜ã€‚
            - **ç¬¬â—‹å››åŠæœŸã®è¡¨è¨˜:**
                **ãƒ«ãƒ¼ãƒ«:
                - If the input contains a format like "18å¹´ç¬¬4å››åŠæœŸ", infer it as:
                    - "18å¹´" â†’ "2018å¹´"
                    - "ç¬¬1å››åŠæœŸ" â†’ "1-3æœˆæœŸ"
                    - "ç¬¬2å››åŠæœŸ" â†’ "4-6æœˆæœŸ"
                    - "ç¬¬3å››åŠæœŸ" â†’ "7-9æœˆæœŸ"
                    - "ç¬¬4å››åŠæœŸ" â†’ "10-12æœˆæœŸ"
                - Modify the expression accordingly, converting the year to a 4-digit format and specifying the exact month range.
                - Add a correction reason in this format:
                `<span style="color:red;">ä¿®æ­£å¾Œ</span> (<span>ä¿®æ­£ç†ç”±: å››åŠæœŸè¡¨è¨˜ã®æ˜ç¢ºåŒ– <s style="background:yellow;color:red">ä¿®æ­£å‰</s> â†’ ä¿®æ­£å¾Œ</span>)`

                ---

                **Example:**
                - Input: 18å¹´ç¬¬4å››åŠæœŸã®å£²ä¸ŠãŒå¥½èª¿ã ã£ãŸã€‚
                - Output: 
                <span style="color:red;">2018å¹´10-12æœˆæœŸ</span> (<span>ä¿®æ­£ç†ç”±: å››åŠæœŸè¡¨è¨˜ã®æ˜ç¢ºåŒ– <s style="background:yellow;color:red">18å¹´ç¬¬4å››åŠæœŸ</s> â†’ 2018å¹´10-12æœˆæœŸ</span>) ã®å£²ä¸ŠãŒå¥½èª¿ã ã£ãŸã€‚

                ---

                **Additional Notes:**
                - Do not modify any proper names, organizations, or if the date range is already correct.
                - Apply to all similar shorthand expressions like "20å¹´ç¬¬2å››åŠæœŸ", "21å¹´ç¬¬1å››åŠæœŸ" etc.
                - Keep the structure and formatting of the original document.


        **Special Rules:**
        1. **Do not modify proper nouns (e.g., names of people, places, or organizations) unless they are clearly misspelled.**
            -Exsample:
            ãƒ™ãƒƒã‚»ãƒ³ãƒˆæ°: Since this is correct and not a misspelling, it will not be modified.
        2. **Remove unnecessary or redundant text instead of replacing it with other characters.**
            -Exsample:
            ãƒ¦ãƒ¼ãƒ­åœåŸŸå†…ã®æ™¯æ°—å§”: Only the redundant character å§” will be removed, and no additional characters like ã® will be added. The corrected text will be: ãƒ¦ãƒ¼ãƒ­åœåŸŸå†…ã®æ™¯æ°—.
        3. **Preserve spaces between words in the original text unless they are at the beginning or end of the text.**
            -Example:
            input: æœˆã® å‰åŠã¯ç±³å›½ã® å‚µåˆ¸åˆ©å›ã‚Šã®ä¸Šæ˜‡ ã«ã¤ã‚Œã¦
            Output: æœˆã® å‰åŠã¯ç±³å›½ã® å‚µåˆ¸åˆ©å›ã‚Šã®ä¸Šæ˜‡ ã«ã¤ã‚Œã¦ (spaces between words are preserved).

        **Output Requirements:**
        1. **Highlight the original incorrect text in red and include additional details:**
        - For corrected parts:
            - Highlight the original incorrect text in red using `<span style="color:red;">`.
            - Append the corrected text in parentheses, marked with a strikethrough using `<s>` tags.
            - Provide the reason for the correction and indicate the change using the format `123 â†’ 456`.
            - Example:
            `<span style="color:red;">123</span> (<span>ä¿®æ­£ç†ç”±: ä¸€è‡´æ€§ä¸è¶³ <s style="background:yellow;color:red">123</s> â†’ 456</span>)`
        
        2. **Preserve the original structure and formatting of the document:**
        - Maintain paragraph breaks, headings, and any existing structure in the content.

        3. **Use the uploaded correction rules for reference:**
        - {corrected}

        4. **Do not provide any explanations or descriptions in the output. Only return the corrected HTML content.**

         **Corrected Terminology Map (ä¿®æ­£ã•ã‚ŒãŸç”¨èªãƒªã‚¹ãƒˆ):
            {corrected_map}
        - Replace only when the **original** term in `corrected_map` appears in the input text.
        - Do **not** replace anything if the input already contains the `corrected` term (it is already correct).
        - Do **not** perform any reverse replacements (`corrected â†’ original` ã¯ç¦æ­¢).
        - Modify the original text only when the `original` term is found.

        - If the `corrected` term appears in the input, **do not modify it** (it is already correct).
        - Do **not** reverse substitutions (i.e., never convert corrected â†’ `original`).
        
        - After replacing, add the reason in this format:
        Original Term (ä¿®æ­£ç†ç”±: ç”¨èªã®çµ±ä¸€ Original Term â†’ Corrected Term)
        Example:
            `<span style="color:red;">Corrected Term</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®çµ±ä¸€ <s style="background:yellow;color:red">Original Term</s> â†’ Corrected Term</span>)`
        
        Example:
        Input: ä¸­éŠ€
        Output: 
        `<span style="color:red;">ä¸­å¤®éŠ€è¡Œ</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®çµ±ä¸€ <s style="background:yellow;color:red">ä¸­éŠ€</s> â†’ ä¸­å¤®éŠ€è¡Œ</span>)`
        â€» Note: Do **not** convert ä¸­å¤®éŠ€è¡Œ â†’ ä¸­éŠ€. All replacements must follow the direction from `original` to `corrected` only.

        Input: ä¸­å¤®éŠ€è¡Œ  
        Output:  
        ä¸­å¤®éŠ€è¡Œ â† (No correction shown because it is already the correct term)

        If the input already contains the corrected term, it should remain unchanged.
        For English abbreviations or foreign terms, the rule is the same: replace the original term with the corrected term and format as follows:
        Example:
        Input: BOE
        Output: <span style="color:red;">BOEï¼ˆè‹±ä¸­å¤®éŠ€è¡Œã€ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰éŠ€è¡Œï¼‰</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">BOE</s> â†’ BOEï¼ˆè‹±ä¸­å¤®éŠ€è¡Œã€ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰éŠ€è¡Œï¼‰</span>)
        Input: AAA
        Output: <span style="color:red;">AAAï¼ˆå…¨ç±³è‡ªå‹•è»Šå”ä¼šï¼‰</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">AAA</s> â†’ AAAï¼ˆå…¨ç±³è‡ªå‹•è»Šå”ä¼šï¼‰</span>)

        Input: ã‚¤ãƒ³ãƒã‚¦ãƒ³ãƒ‰
        Output: <span style="color:red;">ã‚¤ãƒ³ãƒã‚¦ãƒ³ãƒ‰ï¼ˆè¦³å…‰å®¢ã®å—ã‘å…¥ã‚Œï¼‰</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ã‚¤ãƒ³ãƒã‚¦ãƒ³ãƒ‰</s> â†’ ã‚¤ãƒ³ãƒã‚¦ãƒ³ãƒ‰ï¼ˆè¦³å…‰å®¢ã®å—ã‘å…¥ã‚Œï¼‰</span>)

        
        **Except Original Term
        Input: ç­‰
        Output: 
        `<span style="color:red;">ç­‰</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èª/è¡¨ç¾ <s style="background:yellow;color:red">ç­‰</s> â†’ ãªã©</span>)`

        Input: ãƒ­ãƒ¼ãƒ³
        Output: 
        `<span style="color:red;">ãƒ­ãƒ¼ãƒ³</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èª/è¡¨ç¾ <s style="background:yellow;color:red">ãƒ­ãƒ¼ãƒ³</s> â†’ è²¸ã—ä»˜ã‘</span>)`
                            
        Input: ï¼…ã‚’ä¸Šå›ã‚‹
        Output: 
        `<span style="color:red;">ï¼…ã‚’ä¸Šå›ã‚‹</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èª/è¡¨ç¾ <s style="background:yellow;color:red">ï¼…ã‚’ä¸Šå›ã‚‹</s> â†’ ï¼…ã‚’è¶…ãˆã‚‹</span>)`
                            
        Input: ï¼…ã‚’ä¸‹å›ã‚‹
        Output: 
        `<span style="color:red;">ï¼…ã‚’ä¸‹å›ã‚‹</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èª/è¡¨ç¾ <s style="background:yellow;color:red">ï¼…ã‚’ä¸‹å›ã‚‹</s> â†’ ï¼…ã‚’ä¸‹å›ã‚‹ãƒã‚¤ãƒŠã‚¹å¹…</span>)`
        
        Input: ä¼æ’­ï¼ˆã§ã‚“ã±ï¼‰
        Output: 
        `<span style="color:red;">ä¼æ’­ï¼ˆã§ã‚“ã±ï¼‰</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®ç½®ãæ›ãˆ <s style="background:yellow;color:red">ä¼æ’­ï¼ˆã§ã‚“ã±ï¼‰</s> â†’ åºƒã‚Šã¾ã™</span>)`
        
        Input: ä¼æ’­ã—ã¦ã„ã¾ã™
        Output:
        `<span style="color:red;">ä¼æ’­ã—ã¦ã„ã¾ã™</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®ç½®ãæ›ãˆ <s style="background:yellow;color:red">ä¼æ’­ã—ã¦ã„ã¾ã™</s> â†’ åºƒãŒã‚‹ã—ã¦ã„ã¾ã™</span>)`
        
        Input: é€£ã‚Œé«˜
        Output:
        `<span style="color:red;">é€£ã‚Œé«˜</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®ç½®ãæ›ãˆ <s style="background:yellow;color:red">é€£ã‚Œé«˜</s> â†’ å½±éŸ¿ã‚’å—ã‘ã¦ä¸Šæ˜‡</span>)`
        
        Input: ç›¸å ´
        Output:
        `<span style="color:red;">ç›¸å ´</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®ç½®ãæ›ãˆ <s style="background:yellow;color:red">ç›¸å ´</s> â†’ å¸‚å ´/ä¾¡æ ¼</span>)`
        
        Input: ãƒãƒˆæ´¾
        Output:
        `<span style="color:red;">ãƒãƒˆæ´¾</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®ç½®ãæ›ãˆ <s style="background:yellow;color:red">ãƒãƒˆæ´¾</s> â†’ é‡‘èç·©å’Œé‡è¦–ã€é‡‘èç·©å’Œã«å‰å‘ã</span>)`
        
        Input: ã‚¿ã‚«æ´¾
        Output:
        `<span style="color:red;">ã‚¿ã‚«æ´¾</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®ç½®ãæ›ãˆ <s style="background:yellow;color:red">ã‚¿ã‚«æ´¾</s> â†’ é‡‘èå¼•ãç· ã‚é‡è¦–ã€é‡‘èå¼•ãç· ã‚ã«ç©æ¥µçš„</span>)`
        
        Input: ç¹”ã‚Šè¾¼ã‚€
        Output: 
        `<span style="color:red;">ç¹”ã‚Šè¾¼ã‚€</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®ç½®ãæ›ãˆ <s style="background:yellow;color:red">ç¹”ã‚Šè¾¼ã‚€</s> â†’ åæ˜ ã•ã‚Œ</span>)`
        
        Input: ç©æ¥µå§¿å‹¢ã¨ã—ãŸ
        Output: 
        `<span style="color:red;">ç©æ¥µå§¿å‹¢ã¨ã—ãŸ</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èª/è¡¨ç¾ <s style="background:yellow;color:red">ç©æ¥µå§¿å‹¢ã¨ã—ãŸ</s> â†’ é•·ã‚ã¨ã—ãŸ</span>)`
        
        Input: é™å®šçš„
        Output: 
        `<span style="color:red;">é™å®šçš„</span> (<span>ä¿®æ­£ç†ç”±: åŠ¹æœã‚„å½±éŸ¿ãŒãƒ—ãƒ©ã‚¹ã‹ãƒã‚¤ãƒŠã‚¹ã‹ä¸æ˜ç­ãªãŸã‚ <s style="background:yellow;color:red">é™å®šçš„</s> â†’ ä»–ã®é©åˆ‡ãªè¡¨ç¾ã«ä¿®æ­£</span>)`
        
        Input: åˆ©ç›Šç¢ºå®šã®å£²ã‚Š
        Output: 
        `<span style="color:red;">åˆ©ç›Šç¢ºå®šã®å£²ã‚Š</span> (<span>ä¿®æ­£ç†ç”±: æ–­å®šçš„ãªè¡¨ç¾ã§ã¯æ ¹æ‹ ãŒèª¬æ˜ã§ããªã„ãŸã‚ <s style="background:yellow;color:red">åˆ©ç›Šç¢ºå®šã®å£²ã‚Š</s> â†’ ãŒå‡ºãŸã¨ã®è¦‹æ–¹</span>)`
        
        Input: åˆ©é£Ÿã„å£²ã‚Š
        Output: 
        `<span style="color:red;">åˆ©é£Ÿã„å£²ã‚Š</span> (<span>ä¿®æ­£ç†ç”±: æ–­å®šçš„ãªè¡¨ç¾ã§ã¯æ ¹æ‹ ãŒèª¬æ˜ã§ããªã„ãŸã‚ <s style="background:yellow;color:red">åˆ©é£Ÿã„å£²ã‚Š</s> â†’ ãŒå‡ºãŸã¨ã®è¦‹æ–¹</span>)`
        
        Input: ABS
        Output: 
        `<span style="color:red;">ABS</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">ABS</s> â†’ ABSï¼ˆè³‡ç”£æ‹…ä¿è¨¼åˆ¸ã€å„ç¨®è³‡ç”£æ‹…ä¿è¨¼åˆ¸ï¼‰</span>)`
        
        Input: AI
        Output: 
        `<span style="color:red;">AI</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">AI</s> â†’ AIï¼ˆäººå·¥çŸ¥èƒ½</span>)`
        
        Input: BRICSï¼ˆ5ãƒµå›½ï¼‰
        Output: 
        `<span style="color:red;">BRICSï¼ˆ5ãƒµå›½ï¼‰</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">BRICSï¼ˆ5ãƒµå›½ï¼‰</s> â†’ BRICSï¼ˆãƒ–ãƒ©ã‚¸ãƒ«ã€ãƒ­ã‚·ã‚¢ã€ã‚¤ãƒ³ãƒ‰ã€ä¸­å›½ã€å—ã‚¢ãƒ•ãƒªã‚«ï¼‰</span>)`
        
        Input: CMBS
        Output: 
        `<span style="color:red;">CMBS</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">CMBS</s> â†’ CMBSï¼ˆå•†æ¥­ç”¨ä¸å‹•ç”£ãƒ­ãƒ¼ãƒ³æ‹…ä¿è¨¼åˆ¸ï¼‰</span>)`
        
        Input: ISM
        Output: 
        `<span style="color:red;">ISM</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">ISM</s> â†’ ISMï¼ˆå…¨ç±³ä¾›çµ¦ç®¡ç†å”ä¼šï¼‰</span>)`
        
        Input: IT
        Output: 
        `<span style="color:red;">IT</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">IT</s> â†’ ITï¼ˆæƒ…å ±æŠ€è¡“ï¼‰</span>)`
        
        Input: MBS
        Output: 
        `<span style="color:red;">MBS</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">MBS</s> â†’ MBSï¼ˆä½å®…ãƒ­ãƒ¼ãƒ³æ‹…ä¿è¨¼åˆ¸ï¼‰</span>)`
        
        Input: PMI
        Output: 
        `<span style="color:red;">PMI</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">PMI</s> â†’ PMIï¼ˆè³¼è²·æ‹…å½“è€…æ™¯æ°—æŒ‡æ•°ï¼‰</span>)`
        
        Input: S&P
        Output: 
        `<span style="color:red;">S&P</span> (<span>ä¿®æ­£ç†ç”±: è‹±ç•¥èª <s style="background:yellow;color:red">S&P</s> â†’ S&Pï¼ˆã‚¹ã‚¿ãƒ³ãƒ€ãƒ¼ãƒ‰ãƒ»ã‚¢ãƒ³ãƒ‰ãƒ»ãƒ—ã‚¢ãƒ¼ã‚ºï¼‰ç¤¾</span>)`
        
        Input: ã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³
        Output: 
        `<span style="color:red;">ã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³</s> â†’ ã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆè³‡ç”£é…åˆ†ï¼‰</span>)`
        
        Input: E-ã‚³ãƒãƒ¼ã‚¹
        Output: 
        `<span style="color:red;">Eã‚³ãƒãƒ¼ã‚¹</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">E-ã‚³ãƒãƒ¼ã‚¹</s> â†’ Eã‚³ãƒãƒ¼ã‚¹ï¼ˆé›»å­å•†å–å¼•ï¼‰</span>)`
             
        Input: e-ã‚³ãƒãƒ¼ã‚¹
        Output: 
        `<span style="color:red;">eã‚³ãƒãƒ¼ã‚¹</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">eã‚³ãƒãƒ¼ã‚¹</s> â†’ eã‚³ãƒãƒ¼ã‚¹ï¼ˆé›»å­å•†å–å¼•ï¼‰</span>)`
           
        Input: EC
        Output: 
        `<span style="color:red;">ECï¼ˆé›»å­å•†å–å¼•ï¼‰</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">EC</s> â†’ ECï¼ˆé›»å­å•†å–å¼•ï¼‰</span>)`
        
        Input: ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–
        Output: 
        `<span style="color:red;">ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–</s> â†’ ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ï¼ˆåˆ©å›ã‚Šæ›²ç·šï¼‰</span>)`
        
        Input: ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¸ãƒ£ãƒ¼
        Output: 
        `<span style="color:red;">ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¸ãƒ£ãƒ¼</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ã‚¨ã‚¯ã‚¹ãƒãƒ¼ã‚¸ãƒ£ãƒ¼</s> â†’ ï¼Šç©æ¥µçš„ã«ä½¿ç”¨ã—ãªã„ã€‚ã€€ï¼ˆä¾¡æ ¼å¤‰å‹•ãƒªã‚¹ã‚¯è³‡ç”£ã®é…åˆ†æ¯”ç‡ã€å‰²åˆï¼‰</span>)`
        
        Input: ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆï¼ˆä¿¡ç”¨ï¼‰å¸‚å ´
        Output: 
        `<span style="color:red;">ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆï¼ˆä¿¡ç”¨ï¼‰å¸‚å ´</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆï¼ˆä¿¡ç”¨ï¼‰å¸‚å ´</s> â†’ ä¿¡ç”¨ãƒªã‚¹ã‚¯ï¼ˆè³‡é‡‘ã®å€Ÿã‚Šæ‰‹ã®ä¿¡ç”¨åº¦ãŒå¤‰åŒ–ã™ã‚‹ãƒªã‚¹ã‚¯ï¼‰ã‚’å†…åŒ…ã™ã‚‹å•†å“ï¼ˆã‚¯ãƒ¬ã‚¸ãƒƒãƒˆå•†å“ï¼‰ã‚’å–å¼•ã™ã‚‹å¸‚å ´ã®ç·ç§°ã€‚ã€€ä¼æ¥­ã®ä¿¡ç”¨ãƒªã‚¹ã‚¯ã‚’å–å¼•ã™ã‚‹å¸‚å ´ã€‚</span>)`
        
        Input: ã‚·ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ»ãƒªã‚¹ã‚¯
        Output: 
        `<span style="color:red;">ã‚·ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ»ãƒªã‚¹ã‚¯</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ã‚·ã‚¹ãƒ†ãƒŸãƒƒã‚¯ãƒ»ãƒªã‚¹ã‚¯</s> â†’ å€‹åˆ¥ã®é‡‘èæ©Ÿé–¢ã®æ”¯æ‰•ä¸èƒ½ç­‰ã‚„ã€ç‰¹å®šã®å¸‚å ´ã¾ãŸã¯æ±ºæ¸ˆã‚·ã‚¹ãƒ†ãƒ ç­‰ã®æ©Ÿèƒ½ä¸å…¨ãŒã€ä»–ã®é‡‘èæ©Ÿé–¢ã€ä»–ã®å¸‚å ´ã€ã¾ãŸã¯é‡‘èã‚·ã‚¹ãƒ†ãƒ å…¨ä½“ã«æ³¢åŠã™ã‚‹ãƒªã‚¹ã‚¯</span>)`
        
        Input: ãƒ‡ã‚£ã‚¹ãƒˆãƒ¬ã‚¹å‚µåˆ¸
        Output: 
        `<span style="color:red;">ãƒ‡ã‚£ã‚¹ãƒˆãƒ¬ã‚¹å‚µåˆ¸</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ‡ã‚£ã‚¹ãƒˆãƒ¬ã‚¹å‚µåˆ¸</s> â†’ ä¿¡ç”¨äº‹ç”±ãªã©ã«ã‚ˆã‚Šã€ä¾¡æ ¼ãŒè‘—ã—ãä¸‹è½ã—ãŸå‚µåˆ¸</span>)`
        
        Input: ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚·ãƒ–
        Output: 
        `<span style="color:red;">ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚·ãƒ–</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚·ãƒ–</s> â†’ ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚·ãƒ–ï¼ˆæ™¯æ°—ã«å·¦å³ã•ã‚Œã«ãã„ï¼‰</span>)`
        
        Input: ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«
        Output: 
        `<span style="color:red;">ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«</s> â†’ ãƒ†ã‚¯ãƒ‹ã‚«ãƒ«ï¼ˆéå»ã®æ ªä¾¡ã®å‹•ãã‹ã‚‰åˆ¤æ–­ã™ã‚‹ã“ã¨</span>)`
        
        Input: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
        Output: 
        `<span style="color:red;">ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ</s> â†’ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆå‚µå‹™ä¸å±¥è¡Œï¼‰</span>)`
        
        Input: ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‚µ
        Output: 
        `<span style="color:red;">ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‚µ</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‚µ</s> â†’ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã¯ä¸€èˆ¬çš„ã«ã¯å‚µåˆ¸ã®åˆ©æ‰•ã„ãŠã‚ˆã³å…ƒæœ¬è¿”æ¸ˆã®ä¸å±¥è¡Œã€ã‚‚ã—ãã¯é…å»¶ãªã©ã‚’ã„ã„ã€ã“ã®ã‚ˆã†ãªçŠ¶æ…‹ã«ã‚ã‚‹å‚µåˆ¸ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå‚µã¨ã„ã„ã¾ã™ã€‚</span>)`
        
        Input: ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        Output: 
        `<span style="color:red;">ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</s> â†’ ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé‡‘åˆ©æ„Ÿå¿œåº¦ï¼‰</span>)`
        
        Input: æŠ•è³‡é©æ ¼å‚µ
        Output: 
        `<span style="color:red;">æŠ•è³‡é©æ ¼å‚µ</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">æŠ•è³‡é©æ ¼å‚µ</s> â†’ æ ¼ä»˜æ©Ÿé–¢ã«ã‚ˆã£ã¦æ ¼ä»˜ã‘ã•ã‚ŒãŸå…¬ç¤¾å‚µã®ã†ã¡ã€å‚µå‹™ã‚’å±¥è¡Œã™ã‚‹èƒ½åŠ›ãŒååˆ†ã«ã‚ã‚‹ã¨è©•ä¾¡ã•ã‚ŒãŸå…¬ç¤¾å‚µ</span>)`
        
        Input: ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ã‚º
        Output: 
        `<span style="color:red;">ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ã‚º</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ã‚º</s> â†’ ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ã‚ºï¼ˆè³ƒæ–™ã‚„ç©ºå®¤ç‡ã€éœ€çµ¦é–¢ä¿‚ãªã©ã®åŸºç¤çš„æ¡ä»¶ï¼‰â€»REITãƒ•ã‚¡ãƒ³ãƒ‰ã§ä½¿ç”¨ã™ã‚‹</span>)`
        
        Input: ãƒ•ãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼
        Output: 
        `<span style="color:red;">ãƒ•ãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ•ãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼</s> â†’ ç¨å¼•å¾Œå–¶æ¥­åˆ©ç›Šã«æ¸›ä¾¡å„Ÿå´è²»ã‚’åŠ ãˆã€è¨­å‚™æŠ•è³‡é¡ã¨é‹è»¢è³‡æœ¬ã®å¢—åŠ ã‚’å·®ã—å¼•ã„ãŸã‚‚ã®</span>)`
        
        Input: ãƒ™ãƒ¼ã‚¸ãƒ¥ãƒ–ãƒƒã‚¯
        Output: 
        `<span style="color:red;">ãƒ™ãƒ¼ã‚¸ãƒ¥ãƒ–ãƒƒã‚¯</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ™ãƒ¼ã‚¸ãƒ¥ãƒ–ãƒƒã‚¯</s> â†’ ã‚¹ãƒšãƒ¼ã‚¹ãŒãªã„å ´åˆã¯ã€ãƒ™ãƒ¼ã‚¸ãƒ¥ãƒ–ãƒƒã‚¯ï¼ˆç±³åœ°åŒºé€£éŠ€çµŒæ¸ˆå ±å‘Šï¼‰</span>)`
        
        Input: ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ 
        Output: 
        `<span style="color:red;">ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ </span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒ¢ãƒ¡ãƒ³ã‚¿ãƒ </s> â†’  ç›¸å ´ã®å‹¢ã„)ãŒå¼·ãã€æŠ•è³‡å®¶ãŸã¡ã¯çŸ­æœŸçš„ãªåˆ©ç›Šã‚’ç‹™ã£ã¦ã„ã¾ã™ã€‚</span>)`
        
        Input: ãƒªã‚ªãƒ¼ãƒ—ãƒ³
        Output: 
        `<span style="color:red;">ãƒªã‚ªãƒ¼ãƒ—ãƒ³</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒªã‚ªãƒ¼ãƒ—ãƒ³</s> â†’ ãƒªã‚ªãƒ¼ãƒ—ãƒ³/ãƒªã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ï¼ˆçµŒæ¸ˆæ´»å‹•å†é–‹ï¼‰</span>)`
        
        Input: ãƒªã‚¹ã‚¯ãƒ—ãƒ¬ãƒŸã‚¢ãƒ 
        Output: 
        `<span style="color:red;">ãƒªã‚¹ã‚¯ãƒ—ãƒ¬ãƒŸã‚¢ãƒ </span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒªã‚¹ã‚¯ãƒ—ãƒ¬ãƒŸã‚¢ãƒ </s> â†’ åŒã˜æŠ•è³‡æœŸé–“å†…ã«ãŠã„ã¦ã€ã‚ã‚‹ãƒªã‚¹ã‚¯è³‡ç”£ã®æœŸå¾…åç›Šç‡ãŒã€ç„¡ãƒªã‚¹ã‚¯è³‡ç”£ï¼ˆå›½å‚µãªã©ï¼‰ã®åç›Šç‡ã‚’ä¸Šå›ã‚‹å¹…ã®ã“ã¨ã€‚</span>)`
        
        Input: ãƒªãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        Output: 
        `<span style="color:red;">ãƒªãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</span> (<span>ä¿®æ­£ç†ç”±: å¤–æ¥èªãƒ»å°‚é–€ç”¨èª <s style="background:yellow;color:red">ãƒªãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³</s> â†’ ãƒªãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³**ãƒ‡ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã‹ã‚‰æŠœã‘ã¦ã€ã¾ã ã€ã‚¤ãƒ³ãƒ•ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã«ã¯ãªã£ã¦ã„ãªã„çŠ¶æ³ã®ã“ã¨ã€‚</span>)`
        
        **ã€ä¾‹å¤–ç”¨èª â€“ ä¿®æ­£ã—ãªã„ã“ã¨ã€‘**
        - ã‚³ãƒ­ãƒŠç¦
        - ã‚³ãƒ­ãƒŠã‚·ãƒ§ãƒƒã‚¯
        - æ–°å‹ã‚³ãƒ­ãƒŠç¦
        - ä½å®…ãƒ­ãƒ¼ãƒ³
        - å¼•ãç· ã‚ç­–
        - å¼•ãç· ã‚æ”¿ç­–
        - çµ„å…¥æ¯”ç‡
        - æ ¼ä»˜æ©Ÿé–¢
        - æ ¼ä»˜åˆ¥
        - å›½å‚µè²·å…¥ã‚ªãƒš

        **ç‰¹å®šè¡¨ç¾ã®è¨€ã„æ›ãˆãƒ«ãƒ¼ãƒ«ï¼ˆæ–‡è„ˆåˆ¤æ–­ã‚’ä¼´ã†ä¿®æ­£ï¼‰:
        æ–‡è„ˆã«å¿œã˜ã¦ã€å…·ä½“çš„ãªè¡¨ç¾ã«è¨€ã„æ›ãˆã¦ãã ã•ã„ã€‚
        ã€Œã¾ã¡ã¾ã¡ã€ã®ä½¿ç”¨
        ã€Œã¾ã¡ã¾ã¡ã€ã¨ã„ã†æ›–æ˜§ãªè¡¨ç¾ãŒå‡ºç¾ã—ãŸå ´åˆã¯ã€ãã®èªã‚’ãã®ã¾ã¾ä¿æŒã—ãŸä¸Šã§ã€å¾Œã«ã€Œä¿®æ­£ç†ç”±: æ›–æ˜§è¡¨ç¾ã®æ˜ç¢ºåŒ–ã€ã‚’è£œè¶³ã—ã¦ãã ã•ã„ã€‚

        å¤‰æ›èªã€Œç•°ãªã‚‹å‹•ãã€ã‚‚è¡¨ç¤ºã—ã¾ã™ãŒã€åŸæ–‡ã¯å¤‰æ›´ã›ãšã€è£…é£¾ã§ç¤ºã™ã®ã¿ã§ã™ã€‚

        Output Format (Original term preserved, only correction reason shown):
        <span style="color:red;">ã¾ã¡ã¾ã¡</span>
        (<span>ä¿®æ­£ç†ç”±: æ›–æ˜§è¡¨ç¾ã®æ˜ç¢ºåŒ– <s style="background:yellow;color:red">ã¾ã¡ã¾ã¡</s> â†’ ç•°ãªã‚‹å‹•ã</span>)

        -ã€Œè¡Œã£ã¦æ¥ã„ã€ã®è¡¨ç¾

        æ–‡è„ˆã«å¿œã˜ã¦ã€ã€Œä¸Šæ˜‡ï¼ˆä¸‹è½ï¼‰ã—ãŸã®ã¡ä¸‹è½ï¼ˆä¸Šæ˜‡ï¼‰ã€ã®ã‚ˆã†ã«æ˜ç¢ºã«ã—ã¦ãã ã•ã„ã€‚
        Exsample:

        Input: ç›¸å ´ã¯è¡Œã£ã¦æ¥ã„ã®å±•é–‹ã¨ãªã£ãŸ
        Output: ç›¸å ´ã¯ä¸Šæ˜‡ã—ãŸã®ã¡ä¸‹è½ã™ã‚‹å±•é–‹ã¨ãªã£ãŸ
        
        å¤‰æ›èªã€Œè¡Œã£ã¦æ¥ã„ã€ã‚‚è¡¨ç¤ºã—ã¾ã™ãŒã€åŸæ–‡ã¯å¤‰æ›´ã›ãšã€è£…é£¾ã§ç¤ºã™ã®ã¿ã§ã™ã€‚

        Output Format (Original term preserved, only correction reason shown):
        
        ä¿®æ­£ç†ç”±: è¡¨ç¾ã®æ˜ç¢ºåŒ–
        <span style="color:red;">è¡Œã£ã¦æ¥ã„</span> (<span>ä¿®æ­£ç†ç”±: è¡¨ç¾ã®æ˜ç¢ºåŒ– <s style="background:yellow;color:red">Original Term</s> â†’ è¡Œã£ã¦æ¥ã„</span>)

        -ã€Œæ¨ªã°ã„ã€è¡¨ç¾ã®é©æ­£ä½¿ç”¨

        å°å¹…ãªå¤‰å‹•ã§ã‚ã‚Œã°ã€Œæ¨ªã°ã„ã€ã‚’ä½¿ç”¨å¯èƒ½ã€‚
        å¤§ããªå¤‰å‹•ã®æœ«ã«åŒæ°´æº–ã§çµ‚äº†ã—ãŸå ´åˆã¯ã€ã€Œã»ã¼å¤‰ã‚ã‚‰ãšã€ã€ŒåŒç¨‹åº¦ã¨ãªã‚‹ã€ãªã©ã«ä¿®æ­£ã€‚
        
        å¤‰æ›èªã€Œæ¨ªã°ã„ã€ã‚‚è¡¨ç¤ºã—ã¾ã™ãŒã€åŸæ–‡ã¯å¤‰æ›´ã›ãšã€è£…é£¾ã§ç¤ºã™ã®ã¿ã§ã™ã€‚
        Output Format (Original term preserved, only correction reason shown):

        ä¿®æ­£ç†ç”±: ç”¨èªã®é©æ­£ä½¿ç”¨
        <span style="color:red;">æ¨ªã°ã„</span> (<span>ä¿®æ­£ç†ç”±: ç”¨èªã®é©æ­£ä½¿ç”¨ <s style="background:yellow;color:red">æ¨ªã°ã„</s> â†’ ã»ã¼å¤‰ã‚ã‚‰ãš</span>)

        -ã€Œï¼ˆå‰²å®‰ã«ï¼‰æ”¾ç½®ã€è¡¨ç¾ã®ä¿®æ­£

        ã€Œå‰²å®‰æ„Ÿã®ã‚ã‚‹ã€ãªã©ã€ã‚ˆã‚Šé©åˆ‡ãªè¡¨ç¾ã«ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚

        Exsample:
        Input: æ ªä¾¡ã¯å‰²å®‰ã«æ”¾ç½®ã•ã‚ŒãŸ
        Output: æ ªä¾¡ã«ã¯å‰²å®‰æ„ŸãŒã‚ã‚‹çŠ¶æ…‹ãŒç¶šã„ãŸ

        å¤‰æ›èªã€Œï¼ˆå‰²å®‰ã«ï¼‰æ”¾ç½®ã€ã‚‚è¡¨ç¤ºã—ã¾ã™ãŒã€åŸæ–‡ã¯å¤‰æ›´ã›ãšã€è£…é£¾ã§ç¤ºã™ã®ã¿ã§ã™ã€‚
        Output Format (Original term preserved, only correction reason shown):

        ä¿®æ­£ç†ç”±: è¡¨ç¾ã®æ˜ç¢ºåŒ–ã¨å®¢è¦³æ€§ã®å‘ä¸Š
        <span style="color:red;">ï¼ˆå‰²å®‰ã«ï¼‰æ”¾ç½®</span> (<span>ä¿®æ­£ç†ç”±: è¡¨ç¾ã®æ˜ç¢ºåŒ–ã¨å®¢è¦³æ€§ã®å‘ä¸Š <s style="background:yellow;color:red"ï¼ˆå‰²å®‰ã«ï¼‰æ”¾ç½®</s> â†’ å‰²å®‰æ„Ÿã®ã‚ã‚‹</span>)

        """  
        # ChatCompletion Call
        response = openai.ChatCompletion.create(
        # OpenAI API è°ƒç”¨ asyncio
        # loop = asyncio.get_event_loop()
        # response = await loop.run_in_executor(None, lambda: openai.ChatCompletion.create(
            deployment_id=deployment_id,  # Deploy Name
            messages=[
                {"role": "system", "content": "You are a professional Japanese text proofreading assistant."
                "This includes not only Japanese text but also English abbreviations (è‹±ç•¥èª), "
                "foreign terms (å¤–æ¥èª),and specialized terminology (å°‚é–€ç”¨èª)."},
                {"role": "user", "content": prompt_result}
            ],
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE,
            seed=SEED
        )
        answer = response['choices'][0]['message']['content'].strip()
        re_answer = remove_code_blocks(answer)
        
        return jsonify({"success": True, "corrected_text": re_answer})
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

def convert_logs(items):
    converted_data = {
        "code": 200,
        "message": "æˆåŠŸ",
        "data": []
    }
    
    for idx, item in enumerate(items):
        log_entries = item.get("logEntries", [])
        
        for log_idx, log_entry in enumerate(log_entries):
            log_parts = log_entry.split(" - ")
            timestamp_str = log_parts[0] if len(log_parts) > 1 else ""
            message = log_parts[1] if len(log_parts) > 1 else ""
            
            log_data = {
                "id": idx * len(log_entries) + log_idx + 1,  # ID
                "name": message,  # message[:30] message split pre 30 'name'
                "status": "å®Œäº†(ä¿®æ­£ã‚ã‚Š)" if "âœ… SUCCESS" in message else "ã‚¨ãƒ©ãƒ¼",
                "timeclock": timestamp_str,
                "progress": "æˆåŠŸ" if "âœ… SUCCESS" in message else "ã‚¨ãƒ©ãƒ¼",
                "timestamp": timestamp_str,
                "selected": False
            }
            
            converted_data["data"].append(log_data)
    
    return converted_data

# appLog
APPLOG_CONTAINER_NAME='appLog'
@app.route('/api/applog', methods=['GET'])
def get_applog():
    # Cosmos DB è¿æ¥
    container = get_db_connection(APPLOG_CONTAINER_NAME)

    query = "SELECT * FROM c"
    items = list(container.query_items(query=query, enable_cross_partition_query=True))

    for item in items:
        item['id'] = item['id']

    converted_logs = convert_logs(items)
    return jsonify(converted_logs)

# azure Cosmos DB
@app.route('/api/faqs', methods=['GET'])
def get_faq():
    # Cosmos DB é“¾æ¥å®¢æˆ·ç«¯,ENV
    container=get_db_connection()

    query = "SELECT * FROM c"
    items = list(container.query_items(query=query, enable_cross_partition_query=True))

    for item in items:
        item['id'] = item['id']

    return jsonify(items)

@app.route('/api/tenbrend', methods=['POST'])
def tenbrend():
    data = request.get_json() or {}

    raw_fcode = data.get('fcode', '').strip()
    months = data.get('month', '').strip()
    stocks = data.get('stock', '').strip()
    fund_type = data.get('fundType', 'public').strip()  # é»˜è®¤ä¸ºå…¬å‹Ÿ

    # æ ¹æ® fundType é€‰æ‹©å®¹å™¨ï¼ˆå³ Cosmos DB çš„è¡¨ï¼‰

    if fund_type == 'private':
        TENBREND_CONTAINER_NAME = 'tenbrend_private'
    else:
        TENBREND_CONTAINER_NAME = 'tenbrend'

    # Cosmos DB é“¾æ¥å®¢æˆ·ç«¯
    container = get_db_connection(TENBREND_CONTAINER_NAME)
    parameters = []
    if not raw_fcode:
        query = "SELECT * FROM c"

    else:
        # æ„å»º SQL æŸ¥è¯¢
        if '-' in raw_fcode:
            # å¸¦ `-` çš„ç›´æ¥ç”¨å­—ç¬¦ä¸²æ¨¡ç³ŠåŒ¹é…
            query = "SELECT * FROM c WHERE CONTAINS(c.fcode, @fcode)"
            parameters.append({"name": "@fcode", "value": raw_fcode})
        else:
            try:
                fcode_num = raw_fcode
                query = "SELECT * FROM c WHERE c.fcode = @fcode"
                parameters.append({"name": "@fcode", "value": fcode_num})
            except ValueError:
                # fallback åˆ°å­—ç¬¦ä¸²æŸ¥è¯¢
                query = "SELECT * FROM c WHERE CONTAINS(c.fcode, @fcode)"
                parameters.append({"name": "@fcode", "value": raw_fcode})

        if months:
            query += " AND c.months = @months"
            parameters.append({"name": "@months", "value": months})

        if stocks:
            query += " AND CONTAINS(c.stocks, @stocks)"
            parameters.append({"name": "@stocks", "value": stocks})

    items = list(container.query_items(
        query=query,
        parameters=parameters,
        enable_cross_partition_query=True
    ))

    filtered_items = [item for item in items if item.get('id')]
    return jsonify({"code": 200, "data": filtered_items})


@app.route('/api/tenbrend/months', methods=['POST'])
def tenbrend_months():
    data = request.get_json() or {}

    fcode = data.get('fcode', '').strip()
    stocks = data.get('stock', '').strip() if data.get('stock') else ''
    fund_type = data.get('fundType', 'public').strip()

    if not fcode:
        return jsonify({"code": 400, "message": "fcode is required"}), 400

    # âœ… æ ¹æ® fundType åˆ‡æ¢å®¹å™¨ï¼ˆè¡¨ï¼‰
    if fund_type == 'private':
        TENBREND_CONTAINER_NAME ='tenbrend_private'
    else:
        TENBREND_CONTAINER_NAME='tenbrend'

    # Cosmos DB é“¾æ¥å®¢æˆ·ç«¯
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    query = "SELECT c.months FROM c WHERE CONTAINS(c.fcode, @fcode)"
    parameters = [{"name": "@fcode", "value": fcode}]

    if stocks:
        query += " AND CONTAINS(c.stocks, @stocks)"
        parameters.append({"name": "@stocks", "value": stocks})

    try:
        items = list(container.query_items(
            query=query,
            parameters=parameters,
            enable_cross_partition_query=True
        ))

        months = sorted({item.get('months') for item in items if item.get('months')})
        return jsonify({"code": 200, "data": months})
    except Exception as e:
        print("âŒ Cosmos DB query failed:", e)
        return jsonify({"code": 500, "message": "internal error"}), 500


@app.route('/api/tenbrend/stocks', methods=['POST'])
def tenbrend_stocks():
    data = request.get_json() or {}

    fcode = data.get('fcode', '').strip()
    months = data.get('month', '').strip() if data.get('month') else ''
    fund_type = data.get('fundType', 'public').strip()

    if not fcode:
        return jsonify({"code": 400, "message": "fcode is required"}), 400

    if fund_type == 'private':
        TENBREND_CONTAINER_NAME ='tenbrend_private'
    else:
        TENBREND_CONTAINER_NAME='tenbrend'

    # Cosmos DB é“¾æ¥å®¢æˆ·ç«¯
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    query = "SELECT c.stocks FROM c WHERE CONTAINS(c.fcode, @fcode)"
    parameters = [{"name": "@fcode", "value": fcode}]

    if months:
        query += " AND c.months = @months"
        parameters.append({"name": "@months", "value": months})

    try:
        items = list(container.query_items(
            query=query,
            parameters=parameters,
            enable_cross_partition_query=True
        ))

        stocks = sorted({item.get('stocks') for item in items if item.get('stocks')})
        return jsonify({"code": 200, "data": stocks})
    except Exception as e:
        print("âŒ Cosmos DB query failed:", e)
        return jsonify({"code": 500, "message": "internal error"}), 500




@app.route('/api/tenbrend/template', methods=['GET'])
def download_excel_template():
    data = request.get_json() or {}
    # é»˜è®¤æ˜¯â€œå…¬å‹Ÿâ€
    fund_type = data.get('fundType', 'public').strip()

    # æ ¹æ®ç±»å‹æ‹¼æ¥è·¯å¾„
    if fund_type == 'ç§å‹Ÿ':
        file_url = ACCOUNT_URL + STORAGE_CONTAINER_NAME +"/10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_ç§å‹Ÿ.xlsx"
    else:
        file_url = ACCOUNT_URL + STORAGE_CONTAINER_NAME +"/10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_å…¬å‹Ÿ.xlsx"

    try:
        # æ³¨æ„:send_file ä¸èƒ½ç›´æ¥ä¸‹è½½è¿œç¨‹é“¾æ¥ï¼Œæ”¹ä¸ºé‡å®šå‘
        return redirect(file_url)
    except Exception as e:
        return jsonify({"code": 500, "message": str(e)}), 500



# Data transfer
def transform_data(items,fund_type):
    menu_data = {
        "å…¬å‹Ÿ": [],
        "ç§å‹Ÿ": []
    }

    for item in items:
        if fund_type == 'public':
            fund_category = menu_data["å…¬å‹Ÿ"]
        elif fund_type == 'private':
            fund_category = menu_data["ç§å‹Ÿ"]
        else:
            continue  # ì˜ëª»ëœ fund_typeì€ ë¬´ì‹œ

        # ë°ì´í„° êµ¬ì¡°ì— ë§ê²Œ ë³€í™˜
        reference = {
            "id": "reference",
            "name": "ğŸ“ å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«",
            "children": [
                {
                    "id": "report_data",
                    "name": "ğŸ“‚ ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿",
                    "children": []
                },
                {
                    "id": "mingbing_data",
                    "name": "ğŸ“‚ 10éŠ˜æŸ„è§£èª¬ä¸€è¦§è¡¨",
                    "children": []
                }
            ]
        }

        # report_dataé‡Œæ·»åŠ item
        reference["children"][0]["children"].append({
            "id": item.get('id'),
            "name": item.get('fileName'),
            "icon": "ğŸ“„",
            "file": item.get('fileName'),
            "pdfPath": extract_pdf_path(item.get('link')),
        })

        # mingbing_dataé‡Œæ·»åŠ item
        reference["children"][1]["children"].append({
            "id": item.get('id'),
            "name": item.get('fileName'),
            "icon": "ğŸ“„",
            "file": item.get('fileName'),
            "pdfPath": extract_pdf_path(item.get('link'))
        })

        fund_category.append(reference)

        # checked_files è¿½åŠ session
        checked_files = {
            "id": "checked_files",
            "name": "ğŸ“ ãƒã‚§ãƒƒã‚¯å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«",
            "children": [
                {
                    "id": "individual_comments",
                    "name": "ğŸ“‚ å…±é€šã‚³ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«",
                    "children": []
                },
                {
                    "id": "kobetsucomment",
                    "name": "ğŸ“‚ å€‹åˆ¥ã‚³ãƒ¡ãƒ³ãƒˆãƒ•ã‚¡ã‚¤ãƒ«",
                    "children": []
                }
            ]
        }

        # individual_commentsé‡Œæ·»åŠ item
        checked_files["children"][0]["children"].append({
            "id": item.get('id'),  
            "name": item.get('fileName'),  
            "icon": "âš ï¸",
            "file": item.get('fileName'),  
            "status": item.get('comment_status'),  
            "readStatus": item.get('comment_readStatus'),  
            "pdfPath": extract_pdf_path(item.get('link'))  
        })

        # kobetsucommenté‡Œæ·»åŠ item
        checked_files["children"][1]["children"].append({
            "id": item.get('id'),  
            "name": item.get('fileName'),
            "icon": "âŒ",
            "file": item.get('fileName'),
            "status": item.get('individual_status'),
            "readStatus": item.get('individual_readStatus'),
            "pdfPath": extract_pdf_path(item.get('link'))  
        })

        fund_category.append(checked_files)

    return menu_data

def extract_pdf_path(link):
    match = re.search(r'href="([^"]+)"', link)
    return match.group(1) if match else ""

def extract_base_name(file_path):
    file_name = os.path.basename(file_path)
    base_name, _ = os.path.splitext(file_name)
    return base_name

# public_Fund and private_Fund
@app.route('/api/fund', methods=['POST'])
def handle_fund():
    fund_type = request.json.get('type')
    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400

    container_name = f"{fund_type}_Fund"
    
    try:
        # Cosmos DB è¿æ¥
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        query = "SELECT * FROM c"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        
        # filtered_items = [item for item in items if item and item.get('id')]
        
        # return jsonify(filtered_items)
        formatted_data = transform_data(items,fund_type)

        return jsonify(formatted_data)
        
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500

# 625 tenbrend
def convert_to_tenbrend(items):
    corrections = []

    for item in items:
        old_text = item.get("å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬", "").strip()
        new_text = item.get("æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬", "").strip()

        if old_text != new_text:
            corrections.append({
                "check_point": "çµ„å…¥éŠ˜æŸ„è§£èª¬",
                "comment": f"{old_text} â†’ {new_text}",
                "intgr": False,
                "locations": [],
                "original_text": new_text,
                "page": '',
                "reason_type": item.get("stocks", "")
            })

    return corrections


# 509 debug
def convert_format(filtered_items):
    checkResults = {}

    for correction in filtered_items.get("result", {}).get("corrections", []):
        page = correction["page"] + 1
        position = {}
        colorSet = "rgb(172 228 230)"

        change = {
            "before": correction["original_text"],
            "after": correction["comment"].split("â†’")[-1].strip(),
        }
        if correction["intgr"]:
            name = "ä¸ä¸€è‡´"
            colorSet = "rgba(172, 228, 230, 0.5)"
        else:
            name = ""
            colorSet= "rgba(255, 255, 0, 0.5)"

        if correction["locations"]:
            # for idx, loc in enumerate(correction["locations"]): 
            # checkResults
            if page not in checkResults:
                checkResults[page] = [{"title": filtered_items["fileName"], "items": []}]

            # loc = correction["locations"][0]
            for loc in correction["locations"]:
                pdf_height = loc.get("pdf_height", 792)  # PDF height (Default: A4 , 792pt)

                # x = loc["x0"] - 22 if idx == 0 else loc["x0"]
                position = {
                    "x": loc["x0"],
                    "y": pdf_height - loc["y1"] + 50,
                    "width": loc["x1"] - loc["x0"],
                    "height": loc["y1"] - loc["y0"],
                }

                if correction["intgr"]:
                    checkResults[page][0]["items"].append({
                        "name": name,
                        "color": colorSet, #"rgba(255, 255, 0, 0.5)", # green background rgba(0, 255, 0, 0.5)
                        "page": page,
                        "position": position,
                        "changes": [change],
                        "reason_type":correction["reason_type"],
                        "check_point":correction["check_point"],
                        "original_text":correction["original_text"],
                        })
                else:
                        existing_item = any(
                                item["name"] == name and
                                item["changes"] == [change] and
                                item["position"] == position
                                for item in checkResults[page][0]["items"]
                            )
                        if not existing_item:
                            checkResults[page][0]["items"].append({
                                "name": name,
                                "color": colorSet, #"rgba(255, 255, 0, 0.5)", # green background rgba(0, 255, 0, 0.5)
                                "page": page,
                                "position": position,
                                "changes": [change],
                                "reason_type":correction["reason_type"],
                                "check_point":correction["check_point"],
                                "original_text":correction["original_text"],
                                })


    return {'data': checkResults, 'code': 200}

# public_Fund and check-results
@app.route('/api/check_results', methods=['POST'])
def handle_check_results():
    fund_type = request.json.get('type')
    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400
    
    selected_id = request.json.get('selectedId')
    if not selected_id:
        return jsonify({"error": "selectedId is required"}), 400
    
    pageNumber = request.json.get('pageNumber')
    if not pageNumber:
        return jsonify({"error": "pageNumber is required"}), 400

    container_name = f"{fund_type}_Fund"
    
    try:
        # Cosmos DB è¿æ¥
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        query = "SELECT * FROM c WHERE c.id = @id"
        parameters = [{"name": "@id", "value": selected_id}]
        items = list(container.query_items(query=query, parameters=parameters, enable_cross_partition_query=True))
        
        if not items:
            return jsonify({"error": "Item not found"}), 404

        converted_data = convert_format(items[0])

        return jsonify(converted_data)
        
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500

# get side bar
@app.route('/api/menu', methods=['POST'])
def handle_menu():
    fund_type = request.json.get('type')
    page = int(request.json.get('page', 1))
    page_size = int(request.json.get('page_size', 10))
    # user_name = request.json.get('user_name')

    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400

    # container name Setting
    container_name = f"{fund_type}_Fund"

    
    try:
        # Cosmos DB è¿æ¥
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        # Query exe
        query = "SELECT * FROM c WHERE CONTAINS(c.id, '.pdf') OR c.upload_type='å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        
        # filter result
        filtered_items = [item for item in items if item and item.get('id')]

        # pagenations
        total = len(filtered_items)
        start = (page - 1) * page_size
        end = start + page_size
        paged_items = filtered_items[start:end]

        response = {
            "code": 200,
            "data": paged_items,
            "total": total
        }

        return jsonify(response)
        
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    
@app.route('/api/menu_all', methods=['POST'])
def handle_menu_all():
    # param check
    fund_type = request.json.get('type')

    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400

    # container name Setting
    container_name = f"{fund_type}_Fund"
    
    try:
        # Cosmos DB è¿æ¥
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        # Query exe
        query = "SELECT * FROM c WHERE CONTAINS(c.id, '.pdf') OR c.upload_type='å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        
        # filter result
        filtered_items = [item for item in items if item and item.get('id')]
        response = {
        "code": 200,
        "data": filtered_items
        }

        return jsonify(response)
        
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    
# Cosmos DB çŠ¶æ€ç¡®è®¤ endpoint
MONITORING_CONTAINER_NAME = "monitoring-status"

# Cosmos DB çŠ¶æ€ç¡®è®¤ endpoint
@app.route('/api/monitoring-status', methods=['GET'])
def get_monitoring_status():
    try:
        # Cosmos DB è¿æ¥
        container = get_db_connection(MONITORING_CONTAINER_NAME)
        
        # Cosmos DBé‡Œå–æ•°æ®
        query = "SELECT * FROM c"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        for item in items:
            item['id'] = item['id']

        return jsonify(items), 200
        
    except CosmosResourceNotFoundError:
        logging.error("Monitoring status document not found")
        return jsonify({"error": "Status document not found"}), 404
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500

# çŠ¶æ€æ›´æ–°
@app.route('/api/monitoring-status', methods=['PUT'])
def update_monitoring_status():
    try:
        # Cosmos DB è¿æ¥
        container = get_db_connection(MONITORING_CONTAINER_NAME)
        
        new_status = request.json.get('status', 'off')
        
        status_item = {
            'id': 'monitoring_status',
            'type': 'control',
            'status': new_status,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        container.upsert_item(body=status_item)
        logging.info(f"Monitoring status updated to {new_status}")
        return jsonify({'message': 'Status updated', 'new_status': new_status, 'code': 200}), 200
        
    except CosmosResourceNotFoundError as e:
        logging.error(f"Cosmos DB error: {str(e)}")
        return jsonify({"error": "Database operation failed"}), 500
    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500

# read/unread status change
# Cosmos DB çŠ¶æ€ç¡®è®¤ endpoint
@app.route('/api/update_read_status', methods=['POST'])
def get_read_status():
    fund_type = request.json.get('type')
    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400
    
    selected_id = request.json.get('selectedId')
    if not selected_id:
        return jsonify({"error": "selectedId is required"}), 400

    # container name Setting
    container_name = f"{fund_type}_Fund"
    try:
        # Cosmos DB è¿æ¥
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        query = "SELECT * FROM c WHERE c.id = @id"
        parameters = [{"name": "@id", "value": selected_id}]
        items = list(container.query_items(query=query, parameters=parameters, enable_cross_partition_query=True))
        
        if not items:
            return jsonify({"error": "Item not found"}), 404

        return jsonify(items[0]), 200
        
    except CosmosResourceNotFoundError:
        logging.error("read status document not found")
        return jsonify({"error": "read Status document not found"}), 404
    except Exception as e:
        logging.error(f"Database error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500
    
@app.route('/api/update_read_status', methods=['PUT'])
def update_read_status():
    selected_id = request.json.get('selectedId')
    if not selected_id:
        return jsonify({"error": "selectedId is required"}), 400

    mark = request.json.get('mark')
    if mark not in ['read', 'unread']:
        return jsonify({"error": "Invalid mark value"}), 400

    fund_type = request.json.get('type')
    if fund_type not in ['public', 'private']:
        return jsonify({"error": "Invalid fund type"}), 400

    # container name Setting
    container_name = f"{fund_type}_Fund"
    
    try:
        # Cosmos DB è¿æ¥
        container = get_db_connection(container_name)
        logging.info(f"Connected to {container_name} container")
        
        query = "SELECT * FROM c WHERE c.id = @id"
        parameters = [{"name": "@id", "value": selected_id}]
        items = list(container.query_items(query=query, parameters=parameters, enable_cross_partition_query=True))

        if not items:
            return jsonify({"error": "Item not found"}), 404

        status_item = items[0]

        # readStatus å’Œ timestamp
        status_item['readStatus'] = mark
        status_item['timestamp'] = datetime.utcnow().isoformat()
        
        container.upsert_item(body=status_item)
        logging.info(f"readStatus updated to {mark} for item {selected_id}")
        return jsonify({'message': 'Status updated', 'new_status': mark, 'code': 200}), 200
        
    except CosmosResourceNotFoundError as e:
        logging.error(f"Cosmos DB error: {str(e)}")
        return jsonify({"error": "Item not found"}), 404
    except Exception as e:
        logging.error(f"Unexpected error: {str(e)}")
        return jsonify({"error": "Internal server error"}), 500


@app.route("/api/health")
def health_check():
    return "OK", 200

logging.basicConfig(level=logging.INFO)

def get_storage_container():
    """
    Azure AD RBAC æ–¹å¼ Azure Blob Storageì— è¿æ¥, è¿”å›ContainerClient .
    :return: ContainerClient
    """
    try:
        # BlobServiceClient 
        blob_service_client = BlobServiceClient(account_url=ACCOUNT_URL, credential=credential)
        
        container_client = blob_service_client.get_container_client(STORAGE_CONTAINER_NAME)
        
        print("Connected to Azure Blob Storage via Azure AD RBAC")
        logging.info("Connected to Azure Blob Storage via Azure AD RBAC")
        
        return container_client
    except Exception as e:
        logging.error(f"Azure Blob Storage Connection Error: {e}")
        print(f"Azure Blob Storage Connection Error: {e}")
        raise e
    
def allowed_file(filename):
    """    
    :param filename:
    :return: bool
    """
    ALLOWED_EXTENSIONS = {'pdf', 'xlsx','txt','xls','XLSX','xlm','xlsm','xltx','xltm','xlsb','doc','docx'}   # PDF å’Œ Excel  
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

@app.route('/api/test_token', methods=['GET'])
def test_token():
    try:
        token_cache._refresh_token()
        token = token_cache.get_token()
        return jsonify({"access_token": token}), 200
    except Exception as e:
        logging.exception("Token Get Error")
        return jsonify({"message": f"Token Get Error: {str(e)}"}), 500

# uploadpdf,api/brand
def parse_escaped_json(raw_text: str):
    text = raw_text.strip()
    if text.startswith('"') and text.endswith('"'):
        text = text[1:-1]
    
    text = text.replace('```json', '')
    text = text.replace('```', '')

    text = text.replace('""', '"')

    parsed = json.loads(text)
    return parsed

def parse_gpt_response(answer):
    try:
        json_str = re.search(r'\{[\s\S]*?\}', answer).group()
        return json.loads(json_str)
    except (AttributeError, json.JSONDecodeError):
        dict_str = re.search(r'corrected_map\s*=\s*\{[\s\S]*?\}', answer, re.DOTALL)
        if dict_str:
            dict_str = dict_str.group().split('=', 1)[1].strip()
            return ast.literal_eval(dict_str)
        return {}

def detect_corrections(original, corrected):
    matcher = SequenceMatcher(None, original, corrected)
    corrections = {}
    for tag, i1, i2, j1, j2 in matcher.get_opcodes():
        if tag == 'replace':
            orig_part = original[i1:i2].strip()
            corr_part = corrected[j1:j2].strip()
            if orig_part and corr_part:
                corrections[orig_part] = corr_part
    return corrections

def filter_corrected_map(corrected_map):
    keys_to_remove = [" ", "  "]
    for key in keys_to_remove:
        if key in corrected_map:
            del corrected_map[key]
    return corrected_map

# 512 debug
def apply_corrections(input_text, corrected_map):
    result = input_text


    for original, corrected in corrected_map.items():

        if result == corrected:
            continue

        if re.search(re.escape(corrected), result):
            continue

        pattern_already_corrected = re.compile(
            rf"<span style=\"color:red;\">{re.escape(corrected)}</span>\s*"
            rf"\(<span>ä¿®æ­£ç†ç”±: ç”¨èªã®çµ±ä¸€\s*<s style=\"background:yellow;color:red\">{re.escape(original)}</s>\s*â†’\s*{re.escape(corrected)}</span>\)"
        )
        if pattern_already_corrected.search(result):
            continue

        # original
        if re.search(original, result):
            replacement = (
                f'<span style="color:red;">{corrected}</span> '
                f'(<span>ä¿®æ­£ç†ç”±: ç”¨èªã®çµ±ä¸€ '
                f'<s style="background:yellow;color:red">{original}</s> â†’ {corrected}</span>)'
            )
            result = result.replace(original, replacement)

    return result


DICTIONARY_CONTAINER_NAME = "dictionary"
def fetch_and_convert_to_dict():
    try:
        container = get_db_connection(DICTIONARY_CONTAINER_NAME)
        query = "SELECT c.original, c.corrected FROM c"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        corrected_dict = {item["original"]: item["corrected"] for item in items if "original" in item and "corrected" in item}

        return corrected_dict

    except CosmosHttpResponseError as e:
        print(f"âŒ DB error: {e}")
        return {}
    
@app.route('/api/check_upload', methods=['POST'])
def check_upload():
    if 'files' not in request.files:
        return jsonify({"success": False, "message": "No files part"}), 400

    files = request.files.getlist('files')
    file_type = request.form.get("fileType")
    fund_type = request.form.get("fundType")

    for file in files:
        if file.filename == '':
            return jsonify({"success": False, "error": "No selected file"}), 400

        file_bytes = file.read()

        if file and allowed_file(file.filename):
            try:
                if file.filename.endswith('.pdf'):  
                    tenbrend_data = check_tenbrend(file.filename,fund_type)
                    reader = PdfReader(io.BytesIO(file_bytes))
                    text = ""
                    for page in reader.pages:
                        text += page.extract_text()

                    # Encode the PDF bytes to Base64
                    file_base64 = base64.b64encode(file_bytes).decode('utf-8')

                    return jsonify({
                        "success": True,
                        "original_text": extract_text_from_base64_pdf(file_bytes),  # file_bytesPDF text  , input = extract_text_from_base64_pdf(pdf_base64)
                        "pdf_bytes": file_base64,  # PDF Base64 
                        "file_name": file.filename,
                        "tenbrend_data":tenbrend_data,
                        # "fund_type": fund_type
                    })
                
                elif file.filename.endswith('.txt'):
                    text = file_bytes.decode('utf-8')  # UTF-8 

                    return jsonify({
                        "success": True,
                        "prompt_text": text
                    })

                elif file.filename.endswith(('.doc', '.docx')):
                    # Just Only DOCX format
                    # docx = Document(io.BytesIO(file_bytes))
                    # text = "\n".join([para.text for para in docx.paragraphs])

                    file_base64 = base64.b64encode(file_bytes).decode('utf-8')

                    return jsonify({
                        "success": True,
                        "original_text": "",
                        "docx_bytes": file_base64,
                        "file_name": file.filename
                    })

                elif regcheck.search(r'\.(xls|xlsx|XLSX|xlsm|xlm|xltx|xltm|xlsb)$',file.filename):
                    """
                    :param file_bytes: ä¸Šä¼ çš„base64æ–‡ä»¶
                    :return: ä¿®æ”¹å®Œçš„base64 encoding
                    """
                    #--------------excel start------------------------------------------
                    # ğŸ”¹ 1ï¸âƒ£ corrected_map init
                    # corrected_map = fetch_and_convert_to_dict()
                    # all_text=[]

                    # # ğŸ”¹ 2ï¸âƒ£ ä¸´æ—¶ä¿å­˜å†…å­˜é‡Œ in-memory zip)
                    # in_memory_zip = zipfile.ZipFile(io.BytesIO(file_bytes), 'r')

                    # # new ZIP çš„ BytesIO
                    # output_buffer = io.BytesIO()
                    # new_zip = zipfile.ZipFile(output_buffer, 'w', zipfile.ZIP_DEFLATED, allowZip64=True)

                    # # ğŸ”¹ 3ï¸âƒ£ å¾ªç¯æ–‡ä»¶             
                    # for item in in_memory_zip.infolist():
                    #     file_data = in_memory_zip.read(item.filename)
                    #     # ğŸ”¹ 4ï¸âƒ£ æ˜¯å¦drawingN.xml æ£€æŸ¥ (å¤„ç†æ–‡æœ¬æ¡†)
                    #     if item.filename.startswith("xl/drawings/drawing") and item.filename.endswith(".xml"):
                    #         try:
                    #             tree = ET.fromstring(file_data)
                    #             ns = {'a': 'http://schemas.openxmlformats.org/drawingml/2006/main'}
                    #             # æ‰€æœ‰çš„ <a:t> 
                    #             text_elements = []
                    #             for t_element in tree.findall(".//a:t", ns):
                    #                 original_text = t_element.text
                    #                 if original_text:
                    #                     parent = t_element.getparent()
                    #                     if parent is not None:
                    #                         x = parent.attrib.get('x', 0)
                    #                         y = parent.attrib.get('y', 0)
                    #                         text_elements.append((float(y), float(x), original_text.strip()))
                    #             text_elements.sort(key=lambda item: (item[0], item[1]))
                    #             for _, _, text in text_elements:
                    #                 all_text.append(text)
                    #             file_data = ET.tostring(tree, encoding='utf-8', standalone=False)
                    #         except Exception as e:
                    #             print(f"Warning: Parsing {item.filename} failed - {e}")

                    #         try:
                    #             tree = ET.fromstring(file_data)
                    #             ns = {'ss': 'http://schemas.openxmlformats.org/spreadsheetml/2006/main'}

                    #             for row in tree.findall(".//ss:Row", ns):
                    #                 for cell in row.findall("ss:Cell", ns):
                    #                     value_element = cell.find("ss:Data", ns)
                    #                     if value_element is not None and value_element.text:
                    #                         all_text.append(value_element.text.strip())

                    #                     if cell.attrib.get('ss:MergeAcross') is not None:
                    #                         merged_value = value_element.text.strip() if value_element is not None else ""
                    #                         for _ in range(int(cell.attrib['ss:MergeAcross'])):
                    #                             all_text.append(merged_value)

                    #         except Exception as e:
                    #             print(f"Warning: Parsing {item.filename} failed - {e}")
                                
                    #     new_zip.writestr(item, file_data)

                    # # merge all text one string
                    # combined_text = ''.join(all_text)
                    
                    # # 612 debug
                    # # if file_type != "å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«":
                    # #     result_map = gpt_correct_text(combined_text)
                    # #     corrected_map.update(result_map)  # ê²°ê³¼ ë§µ ë³‘í•©
                    # # else:
                    # #     corrected_map = ""


                    # in_memory_zip.close()
                    # new_zip.close()

                    # output_buffer.seek(0)
                    #--------------excel end------------------------------------------
                    # excel_base64 = base64.b64encode(output_buffer.getvalue()).decode('utf-8')
                    excel_base64 = base64.b64encode(file_bytes).decode('utf-8')

                    return jsonify({
                        "success": True,
                        "original_text": "",# combined_text,
                        "excel_bytes": excel_base64,
                        "combined_text": "",# combined_text,
                        "file_name": file.filename
                    })

            except Exception as e:
                logging.error(f"Error processing file {file.filename}: {str(e)}")
                return jsonify({"success": False, "error": str(e)}), 500

    return jsonify({"success": False, "error": "Invalid file type"}), 400


# 5007 debug
def remove_correction_blocks(html_text):
    pattern = re.compile(
        r'<span[^>]*?>.*?<\/span>\s*\(<span>æç¤º:<s[^>]*?>.*?<\/s><\/span>\)',
        re.DOTALL
    )
    return pattern.sub('', html_text)

half_to_full_dict = {
    "ï½¦": "ãƒ²", "ï½§": "ã‚¡", "ï½¨": "ã‚£", "ï½©": "ã‚¥", "ï½ª": "ã‚§", "ï½«": "ã‚©",
    "ï½¬": "ãƒ£", "ï½­": "ãƒ¥", "ï½®": "ãƒ§", "ï½¯": "ãƒƒ", "ï½°": "ãƒ¼",
    "ï½±": "ã‚¢", "ï½²": "ã‚¤", "ï½³": "ã‚¦", "ï½´": "ã‚¨", "ï½µ": "ã‚ª",
    "ï½¶": "ã‚«", "ï½·": "ã‚­", "ï½¸": "ã‚¯", "ï½¹": "ã‚±", "ï½º": "ã‚³",
    "ï½»": "ã‚µ", "ï½¼": "ã‚·", "ï½½": "ã‚¹", "ï½¾": "ã‚»", "ï½¿": "ã‚½",
    "ï¾€": "ã‚¿", "ï¾": "ãƒ", "ï¾‚": "ãƒ„", "ï¾ƒ": "ãƒ†", "ï¾„": "ãƒˆ",
    "ï¾…": "ãƒŠ", "ï¾†": "ãƒ‹", "ï¾‡": "ãƒŒ", "ï¾ˆ": "ãƒ", "ï¾‰": "ãƒ",
    "ï¾Š": "ãƒ", "ï¾‹": "ãƒ’", "ï¾Œ": "ãƒ•", "ï¾": "ãƒ˜", "ï¾": "ãƒ›",
    "ï¾": "ãƒ", "ï¾": "ãƒŸ", "ï¾‘": "ãƒ ", "ï¾’": "ãƒ¡", "ï¾“": "ãƒ¢",
    "ï¾”": "ãƒ¤", "ï¾•": "ãƒ¦", "ï¾–": "ãƒ¨",
    "ï¾—": "ãƒ©", "ï¾˜": "ãƒª", "ï¾™": "ãƒ«", "ï¾š": "ãƒ¬", "ï¾›": "ãƒ­",
    "ï¾œ": "ãƒ¯", "ï¾": "ãƒ³",
    "%": "ï¼…", "@": "ï¼ "
}

full_to_half_dict = {
    'ï¼': '0', 'ï¼‘': '1', 'ï¼’': '2', 'ï¼“': '3', 'ï¼”': '4',
    'ï¼•': '5', 'ï¼–': '6', 'ï¼—': '7', 'ï¼˜': '8', 'ï¼™': '9',
    'ï¼¡': 'A', 'ï¼¢': 'B', 'ï¼£': 'C', 'ï¼¤': 'D', 'ï¼¥': 'E',
    'ï¼¦': 'F', 'ï¼§': 'G', 'ï¼¨': 'H', 'ï¼©': 'I', 'ï¼ª': 'J',
    'ï¼«': 'K', 'ï¼¬': 'L', 'ï¼­': 'M', 'ï¼®': 'N', 'ï¼¯': 'O',
    'ï¼°': 'P', 'ï¼±': 'Q', 'ï¼²': 'R', 'ï¼³': 'S', 'ï¼´': 'T',
    'ï¼µ': 'U', 'ï¼¶': 'V', 'ï¼·': 'W', 'ï¼¸': 'X', 'ï¼¹': 'Y', 
    'ï¼º': 'Z','ï¼‹':'+','ï¼':'-'
}

# åŠè§’â†’,-å…¨è§’
def half_and_full_process(text, mapping):
    return ''.join(mapping.get(c, c) for c in text)

replace_rules = {
    # 'AAA': 'AAAï¼ˆå…¨ç±³è‡ªå‹•è»Šå”ä¼šï¼‰', # 729 fix bug
    'ABS': 'ABSï¼ˆè³‡ç”£æ‹…ä¿è¨¼åˆ¸ã€å„ç¨®è³‡ç”£æ‹…ä¿è¨¼åˆ¸ï¼‰',
    'ADB': 'ADBï¼ˆã‚¢ã‚¸ã‚¢é–‹ç™ºéŠ€è¡Œï¼‰',
    'ADR': 'ADRï¼ˆç±³å›½é è¨—è¨¼åˆ¸ï¼‰',
    'AI': 'AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰',
    'AIIB': 'AIIBï¼ˆã‚¢ã‚¸ã‚¢ã‚¤ãƒ³ãƒ•ãƒ©æŠ•è³‡éŠ€è¡Œï¼‰',
    'APEC': 'APECï¼ˆã‚¢ã‚¸ã‚¢å¤ªå¹³æ´‹çµŒæ¸ˆå”åŠ›ä¼šè­°ï¼‰',
    'API': 'APIï¼ˆå…¨ç±³çŸ³æ²¹å”ä¼šï¼‰',
    'BIS': 'BISï¼ˆå›½éš›æ±ºæ¸ˆéŠ€è¡Œï¼‰',
    'BOE': 'BOEï¼ˆè‹±ä¸­å¤®éŠ€è¡Œã€ã‚¤ãƒ³ã‚°ãƒ©ãƒ³ãƒ‰éŠ€è¡Œï¼‰',
    'BRICSï¼ˆ5ãƒµå›½ï¼‰': 'BRICSï¼ˆãƒ–ãƒ©ã‚¸ãƒ«ã€ãƒ­ã‚·ã‚¢ã€ã‚¤ãƒ³ãƒ‰ã€ä¸­å›½ã€å—ã‚¢ãƒ•ãƒªã‚«ï¼‰',
    'CDSå¸‚å ´': 'CDSï¼ˆã‚¯ãƒ¬ã‚¸ãƒƒãƒˆãƒ»ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ»ã‚¹ãƒ¯ãƒƒãƒ—ï¼‰å¸‚å ´',
    'CFROIC': 'CFROICï¼ˆæŠ•ä¸‹è³‡æœ¬ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼åˆ©ç›Šç‡ï¼‰',
    'Chat GPT': 'Chat GPTï¼ˆAIã‚’ä½¿ã£ãŸå¯¾è©±å‹ã‚µãƒ¼ãƒ“ã‚¹ï¼‰',
    'CMBS': 'CMBSï¼ˆå•†æ¥­ç”¨ä¸å‹•ç”£ãƒ­ãƒ¼ãƒ³æ‹…ä¿è¨¼åˆ¸ï¼‰',
    'COP26': 'COP26ï¼ˆå›½é€£æ°—å€™å¤‰å‹•æ çµ„ã¿æ¡ç´„ç¬¬26å›ç· ç´„å›½ä¼šè­°ï¼‰',
    'CPI': 'CPIï¼ˆæ¶ˆè²»è€…ç‰©ä¾¡æŒ‡æ•°ï¼‰',
    'CSR': 'CSRï¼ˆä¼æ¥­ã®ç¤¾ä¼šçš„è²¬ä»»ï¼‰',
    'DR': 'DRï¼ˆé è¨—è¨¼æ›¸ï¼‰',
    'DRAM': 'DRAMï¼ˆåŠå°ä½“ç´ å­ã‚’åˆ©ç”¨ã—ãŸè¨˜æ†¶è£…ç½®ã®ã²ã¨ã¤ï¼‰',
    'DX': 'DXï¼ˆãƒ‡ã‚¸ã‚¿ãƒ«ãƒˆãƒ©ãƒ³ã‚¹ãƒ•ã‚©ãƒ¼ãƒ¡ãƒ¼ã‚·ãƒ§ãƒ³ï¼‰',
    'EC': 'ECï¼ˆé›»å­å•†å–å¼•ï¼‰',
    'ECB': 'ECBï¼ˆæ¬§å·ä¸­å¤®éŠ€è¡Œï¼‰',
    'EIA': 'EIAï¼ˆç±³ã‚¨ãƒãƒ«ã‚®ãƒ¼çœã‚¨ãƒãƒ«ã‚®ãƒ¼æƒ…å ±å±€ï¼‰',
    'EMEA': 'EMEAï¼ˆæ¬§å·ãƒ»ä¸­æ±ãƒ»ã‚¢ãƒ•ãƒªã‚«ï¼‰',
    'EPA': 'EPAï¼ˆç±³ç’°å¢ƒä¿è­·å±€ï¼‰',
    'EPS': 'EPSï¼ˆä¸€æ ªå½“ãŸã‚Šåˆ©ç›Šï¼‰',
    'ESM': 'ESMï¼ˆæ¬§å·å®‰å®šãƒ¡ã‚«ãƒ‹ã‚ºãƒ ï¼‰',
    'ESG': 'ESGï¼ˆç’°å¢ƒãƒ»ç¤¾ä¼šãƒ»ä¼æ¥­çµ±æ²»ï¼‰',
    'EU': 'EUï¼ˆæ¬§å·é€£åˆï¼‰',
    'EV': 'EVï¼ˆé›»æ°—è‡ªå‹•è»Šï¼‰',
    'EVA': 'EVAï¼ˆçµŒæ¸ˆçš„ä»˜åŠ ä¾¡å€¤ï¼‰',
    'FASB': 'FASBï¼ˆç±³è²¡å‹™ä¼šè¨ˆåŸºæº–å¯©è­°ä¼šï¼‰',
    'FDA': 'FDAï¼ˆç±³å›½é£Ÿå“åŒ»è–¬å“å±€ï¼‰',
    'FFãƒ¬ãƒ¼ãƒˆï¼ˆç±³å›½ã®å ´åˆï¼‰': 'æ”¿ç­–é‡‘åˆ©ï¼ˆFFãƒ¬ãƒ¼ãƒˆï¼‰',
    'FOMC': 'FOMCï¼ˆç±³é€£é‚¦å…¬é–‹å¸‚å ´å§”å“¡ä¼šï¼‰',
    'FRB': 'FRBï¼ˆç±³é€£é‚¦æº–å‚™åˆ¶åº¦ç†äº‹ä¼šï¼‰',
    'FTA': 'FTAï¼ˆè‡ªç”±è²¿æ˜“å”å®šï¼‰',
    'G7': 'G7ï¼ˆä¸»è¦7ãƒµå›½ä¼šè­°ï¼‰',
    'G8': 'G8ï¼ˆä¸»è¦8ãƒµå›½é¦–è„³ä¼šè­°ï¼‰',
    'G20': 'G20ï¼ˆ20ãƒµå›½ãƒ»åœ°åŸŸï¼‰è²¡å‹™ç›¸ãƒ»ä¸­å¤®éŠ€è¡Œç·è£ä¼šè­°ã€é¦–è„³ä¼šè­°',
    'GDP': 'GDPï¼ˆå›½å†…ç·ç”Ÿç”£ï¼‰',
    'GPIF': 'å¹´é‡‘ç©ç«‹é‡‘ç®¡ç†é‹ç”¨ç‹¬ç«‹è¡Œæ”¿æ³•äººï¼ˆGPIFï¼‰',
    'GNP': 'GNPï¼ˆå›½æ°‘ç·ç”Ÿç”£ï¼‰',
    'GSTã€€â€»ã‚¤ãƒ³ãƒ‰ã®å ´åˆ': 'GSTï¼ˆç‰©å“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ç¨ï¼‰',
    'IEA': 'IEAï¼ˆå›½éš›ã‚¨ãƒãƒ«ã‚®ãƒ¼æ©Ÿé–¢ï¼‰',
    'IMF': 'IMFï¼ˆå›½éš›é€šè²¨åŸºé‡‘ï¼‰',
    'IoT': 'IoTï¼ˆãƒ¢ãƒã®ã‚¤ãƒ³ã‚¿ãƒ¼ãƒãƒƒãƒˆï¼‰',
    'IPEF': 'IPEFï¼ˆã‚¤ãƒ³ãƒ‰å¤ªå¹³æ´‹çµŒæ¸ˆæ çµ„ã¿ï¼‰',
    'IPO': 'IPOï¼ˆæ–°è¦æ ªå¼å…¬é–‹ï¼‰',
    'ISMéè£½é€ æ¥­æ™¯æ³': 'ISMéè£½é€ æ¥­æ™¯æ³æŒ‡æ•°',
    'IT': 'ITï¼ˆæƒ…å ±æŠ€è¡“ï¼‰',
    'LBO': 'LBOï¼ˆãƒ¬ãƒãƒ¬ãƒƒã‚¸ãƒ‰ãƒ»ãƒã‚¤ã‚¢ã‚¦ãƒˆï¼šå¯¾è±¡ä¼æ¥­ã®è³‡ç”£ã‚’æ‹…ä¿ã«è³‡é‡‘èª¿é”ã™ã‚‹è²·åï¼‰',
    'LED': 'LEDï¼ˆç™ºå…‰ãƒ€ã‚¤ã‚ªãƒ¼ãƒ‰ï¼‰',
    'LME': 'LMEï¼ˆãƒ­ãƒ³ãƒ‰ãƒ³é‡‘å±å–å¼•æ‰€ï¼‰',
    'LNG': 'LNGï¼ˆæ¶²åŒ–å¤©ç„¶ã‚¬ã‚¹ï¼‰',
    'M&A': 'M&Aï¼ˆä¼æ¥­ã®åˆä½µãƒ»è²·åï¼‰',
    'MAS': 'MASï¼ˆã‚·ãƒ³ã‚¬ãƒãƒ¼ãƒ«é‡‘èé€šè²¨åºï¼‰',
    'MBA': 'MBAï¼ˆå…¨ç±³æŠµå½“è²¸ä»˜éŠ€è¡Œå”ä¼šï¼‰',
    'MBO': 'MBOï¼ˆçµŒå–¶é™£ã«ã‚ˆã‚‹è²·åï¼‰',
    'MBS': 'MBSï¼ˆä½å®…ãƒ­ãƒ¼ãƒ³æ‹…ä¿è¨¼åˆ¸ï¼‰',
    'NAFTA': 'NAFTAï¼ˆåŒ—ç±³è‡ªç”±è²¿æ˜“å”å®šï¼‰',
    'NAHB': 'NAHBï¼ˆå…¨ç±³ä½å®…å»ºè¨­æ¥­è€…å”ä¼šï¼‰',
    'NAIC': 'NAICï¼ˆå…¨ç±³ä¿é™ºç›£ç£å®˜å”ä¼šï¼‰',
    'NAR': 'NARï¼ˆå…¨ç±³ä¸å‹•ç”£æ¥­è€…å”ä¼šï¼‰',
    'NDF': 'NDFï¼ˆç‚ºæ›¿å…ˆæ¸¡å–å¼•ã®ã²ã¨ã¤ï¼‰',
    'NISA': 'NISAï¼ˆå°‘é¡æŠ•è³‡éèª²ç¨åˆ¶åº¦ï¼‰',
    'OECD': 'OECDï¼ˆçµŒæ¸ˆå”åŠ›é–‹ç™ºæ©Ÿæ§‹ï¼‰',
    'OEM': 'OEMï¼ˆç›¸æ‰‹å…ˆãƒ–ãƒ©ãƒ³ãƒ‰ã«ã‚ˆã‚‹ç”Ÿç”£ï¼‰',
    'OPEC': 'OPECï¼ˆçŸ³æ²¹è¼¸å‡ºå›½æ©Ÿæ§‹ï¼‰',
    'OPECãƒ—ãƒ©ã‚¹': 'OPECãƒ—ãƒ©ã‚¹ï¼ˆOPECï¼ˆçŸ³æ²¹è¼¸å‡ºå›½æ©Ÿæ§‹ï¼‰ã¨éåŠ ç›Ÿç”£æ²¹å›½ã§æ§‹æˆã™ã‚‹OPECãƒ—ãƒ©ã‚¹ï¼‰',
    'PBR': 'PBRï¼ˆæ ªä¾¡ç´”è³‡ç”£å€ç‡ï¼‰',
    'PCE': 'PCEï¼ˆå€‹äººæ¶ˆè²»æ”¯å‡ºï¼‰',
    'PCFR': 'PCFRï¼ˆæ ªä¾¡ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼å€ç‡ï¼‰',
    'PER': 'PERï¼ˆæ ªä¾¡åç›Šç‡ï¼‰',
    'PMI': 'PMIï¼ˆè³¼è²·æ‹…å½“è€…æ™¯æ°—æŒ‡æ•°ï¼‰',
    'PPI': 'PPIï¼ˆç”Ÿç”£è€…ç‰©ä¾¡æŒ‡æ•°ï¼‰',
    'QE': 'QEï¼ˆé‡çš„é‡‘èç·©å’Œï¼‰',
    'QT': 'QTï¼ˆé‡çš„å¼•ãç· ã‚ï¼‰',
    'Quad': 'Quadï¼ˆæ—¥ç±³è±ªå°æˆ¦ç•¥å¯¾è©±ï¼‰',
    'RBA': 'RBAï¼ˆè±ªå·æº–å‚™éŠ€è¡Œï¼‰',
    'RCEP': 'RCEPï¼ˆåœ°åŸŸçš„ãªåŒ…æ‹¬çš„çµŒæ¸ˆé€£æºå”å®šï¼‰',
    'RBI': 'RBIï¼ˆã‚¤ãƒ³ãƒ‰æº–å‚™éŠ€è¡Œï¼‰',
    'ROA': 'ROAï¼ˆç·è³‡ç”£åˆ©ç›Šç‡ï¼‰',
    'ROE': 'ROEï¼ˆè‡ªå·±è³‡æœ¬åˆ©ç›Šç‡ï¼‰',
    'S&L': 'S&Lï¼ˆè²¯è“„è²¸ä»˜çµ„åˆï¼‰',
    'SDGs': 'SDGsï¼ˆæŒç¶šå¯èƒ½ãªé–‹ç™ºç›®æ¨™ï¼‰',
    'SEC': 'SECï¼ˆç±³è¨¼åˆ¸å–å¼•å§”å“¡ä¼šï¼‰',
    'SQ': 'SQï¼ˆç‰¹åˆ¥æ¸…ç®—æŒ‡æ•°ï¼‰',
    'SRI': 'SRIï¼ˆç¤¾ä¼šçš„è²¬ä»»æŠ•è³‡ï¼‰',
    'SUV': 'SUVï¼ˆã‚¹ãƒãƒ¼ãƒ„ç”¨å¤šç›®çš„è»Šï¼‰',
    'TALF': 'TALFï¼ˆã‚¿ãƒ¼ãƒ ç‰©è³‡ç”£æ‹…ä¿è¨¼åˆ¸è²¸å‡ºåˆ¶åº¦ï¼‰',
    'TOB': 'TOBï¼ˆæ ªå¼å…¬é–‹è²·ä»˜ã‘ï¼‰',
    'TPP': 'TPPï¼ˆç’°å¤ªå¹³æ´‹çµŒæ¸ˆé€£æºå”å®šï¼‰',
    'UAE': 'UAEï¼ˆã‚¢ãƒ©ãƒ–é¦–é•·å›½é€£é‚¦ï¼‰',
    'UAW': 'UAWï¼ˆå…¨ç±³è‡ªå‹•è»ŠåŠ´åƒçµ„åˆï¼‰',
    'USDA': 'USDAï¼ˆç±³å›½è¾²å‹™çœï¼‰',
    'USMCA': 'USMCAï¼ˆç±³å›½ãƒ»ãƒ¡ã‚­ã‚·ã‚³ãƒ»ã‚«ãƒŠãƒ€å”å®šï¼‰',
    'USTR': 'USTRï¼ˆç±³é€šå•†ä»£è¡¨éƒ¨ï¼‰',
    'VAT': 'VATï¼ˆä»˜åŠ ä¾¡å€¤ç¨ï¼‰',
    'WTI': 'WTIï¼ˆã‚¦ã‚¨ã‚¹ãƒˆãƒ»ãƒ†ã‚­ã‚µã‚¹ãƒ»ã‚¤ãƒ³ã‚¿ãƒ¼ãƒŸãƒ‡ã‚£ã‚¨ãƒ¼ãƒˆï¼‰',
    'WTO': 'WTOï¼ˆä¸–ç•Œè²¿æ˜“æ©Ÿé–¢ï¼‰',
    'ã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³': 'ã‚¢ã‚»ãƒƒãƒˆã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆè³‡ç”£é…åˆ†ï¼‰',
    'ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆ': 'ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«æ¯”ã¹ä½ã‚ã®æŠ•è³‡æ¯”ç‡ï¼‰',
    'ã‚ªãƒ¼ãƒãƒ¼ã‚¦ã‚¨ã‚¤ãƒˆ': 'ã‚ªãƒ¼ãƒãƒ¼ã‚¦ã‚¨ã‚¤ãƒˆï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã«æ¯”ã¹é«˜ã‚ã®æŠ•è³‡æ¯”ç‡ï¼‰',
    'E-ã‚³ãƒãƒ¼ã‚¹': 'Eã‚³ãƒãƒ¼ã‚¹ï¼ˆé›»å­å•†å–å¼•ï¼‰',
    'e-ã‚³ãƒãƒ¼ã‚¹': 'eã‚³ãƒãƒ¼ã‚¹ï¼ˆé›»å­å•†å–å¼•ï¼‰',
    'ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–': 'ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ï¼ˆåˆ©å›ã‚Šæ›²ç·šï¼‰',
    'ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ãƒ»ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«': 'ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ãƒ»ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«ï¼ˆé•·çŸ­é‡‘åˆ©æ“ä½œï¼‰',
    'ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ã®ã‚¹ãƒ†ã‚£ãƒ¼ãƒ—åŒ–': 'ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ã®ã‚¹ãƒ†ã‚£ãƒ¼ãƒ—åŒ–ï¼ˆé•·ãƒ»çŸ­é‡‘åˆ©æ ¼å·®ã®æ‹¡å¤§ï¼‰',
    'ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ã®ãƒ•ãƒ©ãƒƒãƒˆåŒ–': 'ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ã®ãƒ•ãƒ©ãƒƒãƒˆåŒ–ï¼ˆé•·ãƒ»çŸ­é‡‘åˆ©æ ¼å·®ã®ç¸®å°ï¼‰',
    'ã‚¤ãƒ³ã‚«ãƒ ã‚²ã‚¤ãƒ³': 'ã‚¤ãƒ³ã‚«ãƒ ã‚²ã‚¤ãƒ³ï¼ˆåˆ©å­åå…¥ï¼‰',
    'ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–': 'ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ï¼ˆåŒæ–¹å‘æ€§ï¼‰',
    'ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ãƒ»ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹': 'ã‚¨ã‚¯ã‚¤ãƒ†ã‚£ãƒ»ãƒ•ã‚¡ã‚¤ãƒŠãƒ³ã‚¹ï¼ˆæ–°æ ªç™ºè¡Œç­‰ã«ã‚ˆã‚‹è³‡é‡‘èª¿é”ï¼‰',
    'ã‚ªãƒãƒã‚±ã‚¢': 'ã‚ªãƒãƒã‚±ã‚¢ï¼ˆåŒ»ç™‚ä¿é™ºåˆ¶åº¦æ”¹é©æ³•ï¼‰',
    'ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰': 'ã‚ªãƒ³ãƒ‡ãƒãƒ³ãƒ‰ï¼ˆæ³¨æ–‡ç”Ÿç”£ï¼‰',
    'ã‚«ãƒ³ãƒˆãƒªãƒ¼ï½¥ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³': 'ã‚«ãƒ³ãƒˆãƒªãƒ¼ï½¥ã‚¢ãƒ­ã‚±ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆå›½åˆ¥è³‡ç”£é…åˆ†ï¼‰',
    'é€†ã‚¤ãƒ¼ãƒ«ãƒ‰': 'é€†ã‚¤ãƒ¼ãƒ«ãƒ‰ï¼ˆçŸ­æœŸå‚µåˆ¸ã®åˆ©å›ã‚ŠãŒé•·æœŸå‚µåˆ¸ã®åˆ©å›ã‚Šã‚’ä¸Šå›ã£ã¦ã„ã‚‹çŠ¶æ…‹ï¼‰',
    # 'ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼': 'ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼ï¼ˆç¾é‡‘åæ”¯ï¼‰',
    'ã‚­ãƒ£ãƒ”ã‚¿ãƒ«ã‚²ã‚¤ãƒ³': 'ã‚­ãƒ£ãƒ”ã‚¿ãƒ«ã‚²ã‚¤ãƒ³ï¼ˆå€¤ä¸ŠãŒã‚Šç›Šï¼‰',
    'ã‚­ãƒ£ãƒªãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‰': 'ã‚­ãƒ£ãƒªãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‰ï¼ˆä½é‡‘åˆ©ã®è³‡é‡‘ã‚’èª¿é”ã—ã¦ã€é«˜é‡‘åˆ©ã®è³‡ç”£ã§é‹ç”¨ã™ã‚‹å–å¼•ï¼‰',
    'ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰': 'ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼ˆä¼æ¥­ã®ä¿¡ç”¨åŠ›ã®å·®ã«ã‚ˆã‚‹åˆ©å›ã‚Šã®å·®ï¼‰',
    'ã‚°ãƒ­ãƒ¼ãƒãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³': 'ã‚°ãƒ­ãƒ¼ãƒãƒªã‚¼ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆåœ°çƒè¦æ¨¡åŒ–ï¼‰',
    'ã‚³ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³': 'ã‚³ã‚¸ã‚§ãƒãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆç†±é›»ä¾›çµ¦ã‚·ã‚¹ãƒ†ãƒ ï¼‰',
    'ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ãƒˆãƒ»ã‚¬ãƒãƒŠãƒ³ã‚¹': 'ã‚³ãƒ¼ãƒãƒ¬ãƒ¼ãƒˆãƒ»ã‚¬ãƒãƒŠãƒ³ã‚¹ï¼ˆä¼æ¥­çµ±æ²»ï¼‰',
    'ã‚³ãƒ³ã‚°ãƒ­ãƒãƒªãƒƒãƒˆ': 'ã‚³ãƒ³ã‚°ãƒ­ãƒãƒªãƒƒãƒˆï¼ˆè¤‡åˆä¼æ¥­ï¼‰',
    'ã‚³ãƒ³ã‚½ãƒ¼ã‚·ã‚¢ãƒ ': 'ã‚³ãƒ³ã‚½ãƒ¼ã‚·ã‚¢ãƒ ï¼ˆå…±åŒäº‹æ¥­ï¼‰',
    'ã‚µãƒ¼ãƒ™ã‚¤ãƒ©ãƒ³ã‚¹': 'ã‚µãƒ¼ãƒ™ã‚¤ãƒ©ãƒ³ã‚¹ï¼ˆèª¿æŸ»ç›£è¦–ï¼‰',
    'ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£': 'ã‚µã‚¹ãƒ†ãƒŠãƒ“ãƒªãƒ†ã‚£ï¼ˆæŒç¶šå¯èƒ½æ€§ï¼‰',
    'ã‚µãƒ–ãƒ—ãƒ©ã‚¤ãƒ ãƒ­ãƒ¼ãƒ³': 'ã‚µãƒ–ãƒ—ãƒ©ã‚¤ãƒ ãƒ­ãƒ¼ãƒ³ï¼ˆä¿¡ç”¨åº¦ã®ä½ã„å€‹äººå‘ã‘ä½å®…èè³‡ï¼‰',
    'ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³': 'ã‚µãƒ—ãƒ©ã‚¤ãƒã‚§ãƒ¼ãƒ³ï¼ˆä¾›çµ¦ç¶²ï¼‰',
    'ã‚¸ã‚§ãƒãƒªãƒƒã‚¯åŒ»è–¬å“': 'ã‚¸ã‚§ãƒãƒªãƒƒã‚¯åŒ»è–¬å“ï¼ˆå¾Œç™ºè–¬ï¼‰',
    'ã‚·ã‚¯ãƒªã‚«ãƒ«': 'ã‚·ã‚¯ãƒªã‚«ãƒ«ï¼ˆæ™¯æ°—æ•æ„Ÿï¼‰',
    'ã‚·ãƒ£ãƒ‰ãƒ¼ãƒãƒ³ã‚­ãƒ³ã‚°': 'ã‚·ãƒ£ãƒ‰ãƒ¼ãƒãƒ³ã‚­ãƒ³ã‚°ï¼ˆå½±ã®éŠ€è¡Œï¼‰',
    'ã‚·ãƒ§ãƒ¼ãƒˆãƒã‚¸ã‚·ãƒ§ãƒ³': 'ã‚·ãƒ§ãƒ¼ãƒˆãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆå£²ã‚ŠæŒã¡ï¼‰',
    'ä¿¡ç”¨å¸‚å ´': 'ä¼æ¥­ã®ä¿¡ç”¨ãƒªã‚¹ã‚¯ã‚’å–å¼•ã™ã‚‹å¸‚å ´',
    'ã‚¹ãƒ†ã‚£ãƒ¼ãƒ—åŒ–': 'ã‚¹ãƒ†ã‚£ãƒ¼ãƒ—åŒ–ï¼ˆé•·çŸ­é‡‘åˆ©æ ¼å·®ã®æ‹¡å¤§ï¼‰',
    'ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆ': 'ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆå¥å…¨æ€§å¯©æŸ»ï¼‰',
    'ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰': 'ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ï¼ˆåˆ©å›ã‚Šæ ¼å·®ï¼‰',
    'ã‚¹ãƒãƒ¼ãƒˆã‚·ãƒ†ã‚£ãƒ¼': 'ã‚¹ãƒãƒ¼ãƒˆã‚·ãƒ†ã‚£ãƒ¼ï¼ˆITã‚’æ´»ç”¨ã—ãŸæ¬¡ä¸–ä»£å‹ã®éƒ½å¸‚ï¼‰',
    'ã‚¹ãƒãƒ¼ãƒˆãƒ¢ãƒ“ãƒªãƒ†ã‚£': 'ã‚¹ãƒãƒ¼ãƒˆãƒ¢ãƒ“ãƒªãƒ†ã‚£ï¼ˆå¾“æ¥ã®äº¤é€šãƒ»ç§»å‹•ã‚’å¤‰ãˆã‚‹æ–°ãŸãªãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ï¼‰',
    'ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒãƒƒãƒˆ': 'ã‚»ãƒ¼ãƒ•ãƒ†ã‚£ãƒãƒƒãƒˆï¼ˆå®‰å…¨ç¶²ï¼‰',
    'å…¨äººä»£': 'å…¨äººä»£ï¼ˆå…¨å›½äººæ°‘ä»£è¡¨å¤§ä¼šï¼‰',
    'ã‚½ãƒ•ãƒˆãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°': 'ã‚½ãƒ•ãƒˆãƒ©ãƒ³ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆè»Ÿç€é™¸ï¼‰',
    'ãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ†ã‚£': 'ãƒ€ã‚¤ãƒãƒ¼ã‚·ãƒ†ã‚£ï¼ˆå¤šæ§˜æ€§ï¼‰',
    'ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãƒ¬ãƒ¼ãƒˆ': 'ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãƒ¬ãƒ¼ãƒˆï¼ˆæ”¿ç­–é‡‘åˆ©ã®æœ€çµ‚åˆ°é”æ°´æº–ï¼‰',
    'ãƒ†ãƒ¼ãƒ‘ãƒªãƒ³ã‚°': 'ãƒ†ãƒ¼ãƒ‘ãƒªãƒ³ã‚°ï¼ˆé‡çš„é‡‘èç·©å’Œã®ç¸®å°ï¼‰',
    'ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚·ãƒ–': 'ãƒ‡ã‚£ãƒ•ã‚§ãƒ³ã‚·ãƒ–ï¼ˆæ™¯æ°—ã«å·¦å³ã•ã‚Œã«ãã„ï¼‰',
    'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ': 'ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆå‚µå‹™ä¸å±¥è¡Œï¼‰',
    'ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³': 'ãƒ‡ãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆé‡‘åˆ©æ„Ÿå¿œåº¦ï¼‰', 
    'ãƒ‡ãƒªãƒãƒ†ã‚£ãƒ–': 'ãƒ‡ãƒªãƒãƒ†ã‚£ãƒ–ï¼ˆé‡‘èæ´¾ç”Ÿå•†å“ï¼‰',
    'ãƒ‰ãƒ«ãƒšãƒƒã‚°åˆ¶': 'ãƒ‰ãƒ«ãƒšãƒƒã‚°ï¼ˆé€£å‹•ï¼‰åˆ¶',
    'ãƒã‚¤ã‚ªã‚·ãƒŸãƒ©ãƒ¼': 'ãƒã‚¤ã‚ªã‚·ãƒŸãƒ©ãƒ¼ï¼ˆå¾Œç¶šè–¬ï¼‰',
    'ãƒã‚¤ã‚ªãƒã‚¹': 'ãƒã‚¤ã‚ªãƒã‚¹ï¼ˆç”Ÿç‰©ã‚’åˆ©ç”¨ã—ã¦ç‰©è³ªã‚„ã‚¨ãƒãƒ«ã‚®ãƒ¼ã‚’å¾—ã‚‹ã“ã¨ï¼‰',
    'ãƒãƒ¼ãƒãƒ£ãƒ«': 'ãƒãƒ¼ãƒãƒ£ãƒ«ï¼ˆä»®æƒ³ï¼‰',
    'ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³': 'ãƒãƒªãƒ¥ã‚¨ãƒ¼ã‚·ãƒ§ãƒ³ï¼ˆæŠ•è³‡ä¾¡å€¤è©•ä¾¡ï¼‰',
    'ãƒãƒªãƒ¥ãƒ¼': 'ãƒãƒªãƒ¥ãƒ¼ï¼ˆå‰²å®‰ï¼‰',
    '5G': '5Gï¼ˆç¬¬5ä¸–ä»£ç§»å‹•é€šä¿¡ã‚·ã‚¹ãƒ†ãƒ ï¼‰',
    'ãƒ•ã‚£ãƒ³ãƒ†ãƒƒã‚¯': 'ãƒ•ã‚£ãƒ³ãƒ†ãƒƒã‚¯ï¼ˆé‡‘èã¨æŠ€è¡“ã®èåˆï¼‰', 
    'ãƒ•ã‚§ã‚¢ãƒãƒªãƒ¥ãƒ¼': 'ãƒ•ã‚§ã‚¢ãƒãƒªãƒ¥ãƒ¼ï¼ˆé©æ­£ä¾¡æ ¼ï¼‰',
    'ãƒ•ã‚§ãƒ¼ã‚º2': 'ãƒ•ã‚§ãƒ¼ã‚º2ï¼ˆè‡¨åºŠè©¦é¨“ã®ä¸­é–“æ®µéšï¼‰',
    'ãƒ•ã‚§ãƒ¼ã‚º3': 'ãƒ•ã‚§ãƒ¼ã‚º3ï¼ˆè‡¨åºŠè©¦é¨“ã®æœ€çµ‚æ®µéšï¼‰',
    'ãƒ•ãƒ¼ãƒ‰ãƒ‡ãƒªãƒãƒªãƒ¼': 'ãƒ•ãƒ¼ãƒ‰ãƒ‡ãƒªãƒãƒªãƒ¼ï¼ˆæ–™ç†ç­‰ã®å®…é…ã‚µãƒ¼ãƒ“ã‚¹ï¼‰',
    'ãƒ•ãƒ«ã‚¤ãƒ³ãƒ™ã‚¹ãƒˆãƒ¡ãƒ³ãƒˆ': 'ãƒ•ãƒ«ã‚¤ãƒ³ãƒ™ã‚¹ãƒˆãƒ¡ãƒ³ãƒˆï¼ˆé«˜ä½çµ„å…¥ï¼‰',
    'ãƒ–ãƒ­ãƒ¼ãƒ‰ãƒãƒ³ãƒ‰': 'ãƒ–ãƒ­ãƒ¼ãƒ‰ãƒãƒ³ãƒ‰ï¼ˆå¤§å®¹é‡ï½¥é«˜é€Ÿé€šä¿¡ï¼‰',
    'ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«': 'ãƒãƒ†ãƒ³ã‚·ãƒ£ãƒ«ï¼ˆæ½œåœ¨åŠ›ï¼‰',
    'ãƒãƒ”ãƒ¥ãƒªã‚ºãƒ ': 'ãƒãƒ”ãƒ¥ãƒªã‚ºãƒ ï¼ˆå¤§è¡†è¿åˆä¸»ç¾©ï¼‰',
    'ãƒ¢ãƒ¼ã‚²ãƒ¼ã‚¸': 'ãƒ¢ãƒ¼ã‚²ãƒ¼ã‚¸ï¼ˆä¸å‹•ç”£æ‹…ä¿ãƒ­ãƒ¼ãƒ³ï¼‰',
    'ãƒ¢ãƒ¼ã‚²ãƒ¼ã‚¸å‚µ': 'ãƒ¢ãƒ¼ã‚²ãƒ¼ã‚¸å‚µï¼ˆä¸å‹•ç”£ãƒ­ãƒ¼ãƒ³æ‹…ä¿å‚µåˆ¸ï¼‰',
    'ãƒ¢ãƒ©ãƒ«ãƒã‚¶ãƒ¼ãƒ‰': 'ãƒ¢ãƒ©ãƒ«ãƒã‚¶ãƒ¼ãƒ‰ï¼ˆå€«ç†å´©å£Šï¼‰',
    'ãƒªã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°': 'ãƒªã‚¨ãƒ³ã‚¸ãƒ‹ã‚¢ãƒªãƒ³ã‚°ï¼ˆæ¥­å‹™ã®æŠœæœ¬çš„é©æ–°ï¼‰',
    'ãƒªã‚ªãƒ¼ãƒ—ãƒ³': 'ãƒªã‚ªãƒ¼ãƒ—ãƒ‹ãƒ³ã‚°ï¼ˆçµŒæ¸ˆæ´»å‹•å†é–‹ï¼‰',
    'ãƒªã‚»ãƒƒã‚·ãƒ§ãƒ³': 'ãƒªã‚»ãƒƒã‚·ãƒ§ãƒ³ï¼ˆæ™¯æ°—å¾Œé€€ï¼‰',
    'ãƒªã‚¿ãƒ¼ãƒ³ãƒªãƒãƒ¼ã‚µãƒ«': 'ãƒªã‚¿ãƒ¼ãƒ³ãƒªãƒãƒ¼ã‚µãƒ«ï¼ˆéå‰°åå¿œåŠ¹æœï¼‰',
    'ãƒªãƒã‚¦ãƒ³ãƒ‰': 'ãƒªãƒã‚¦ãƒ³ãƒ‰ï¼ˆåç™ºï¼‰',
    'ãƒªãƒãƒ©ãƒ³ã‚¹': 'ãƒªãƒãƒ©ãƒ³ã‚¹ï¼ˆæŠ•è³‡æ¯”ç‡ã®å†èª¿æ•´ï¼‰',
    'ãƒ¬ãƒ‘ãƒˆãƒªæ¸›ç¨': 'ãƒ¬ãƒ‘ãƒˆãƒªï¼ˆæµ·å¤–åç›Šã®æœ¬å›½é‚„æµï¼‰æ¸›ç¨',
    'ãƒ¬ãƒãƒ¬ãƒƒã‚¸ãƒ‰ãƒ­ãƒ¼ãƒ³': 'ä½æ ¼ä»˜ã‘ç­‰ã®å€Ÿã‚Šæ‰‹å‘ã‘èè³‡',
    'ãƒ¬ãƒ©ãƒ†ã‚£ãƒ–ãƒ»ãƒãƒªãƒ¥ãƒ¼': 'ãƒ¬ãƒ©ãƒ†ã‚£ãƒ–ãƒ»ãƒãƒªãƒ¥ãƒ¼ï¼ˆç›¸å¯¾ä¾¡å€¤ï¼‰',
    'ãƒ­ãƒƒã‚¯ãƒ€ã‚¦ãƒ³': 'ãƒ­ãƒƒã‚¯ãƒ€ã‚¦ãƒ³ï¼ˆéƒ½å¸‚å°é–ï¼‰',
    'ãƒ­ãƒ³ã‚°ãƒã‚¸ã‚·ãƒ§ãƒ³': 'ãƒ­ãƒ³ã‚°ãƒã‚¸ã‚·ãƒ§ãƒ³ï¼ˆè²·ã„æŒã¡ï¼‰',
    'EDAãƒ„ãƒ¼ãƒ«': 'EDAãƒ„ãƒ¼ãƒ«ï¼ˆé›»å­è¨­è¨ˆè‡ªå‹•åŒ–ãƒ„ãƒ¼ãƒ«ï¼‰', #623
    'TOPIX':'TOPIXï¼ˆæ±è¨¼æ ªä¾¡æŒ‡æ•°ï¼‰', #63207
    'åˆ©å›ã‚Šã¯ä¸Šæ˜‡': 'åˆ©å›ã‚Šã¯ä¸Šæ˜‡ï¼ˆä¾¡æ ¼ã¯ä¸‹è½ï¼‰', #730
    'åˆ©å›ã‚Šã¯ä½ä¸‹': 'åˆ©å›ã‚Šã¯ä½ä¸‹ï¼ˆä¾¡æ ¼ã¯ä¸Šæ˜‡ï¼‰', #730

    'åˆ©å›ã‚Šã®ä¸Šæ˜‡': 'åˆ©å›ã‚Šã®ä¸Šæ˜‡(ä¾¡æ ¼ã¯ä½ä¸‹)', #730
    'åˆ©å›ã‚Šã®ä½ä¸‹': 'åˆ©å›ã‚Šã®ä½ä¸‹(ä¾¡æ ¼ã¯ä¸Šæ˜‡)', #730

    'å›½å‚µåˆ©å›ã‚Šã¯ã€æœˆé–“ã§ä¸Šæ˜‡': 'å›½å‚µåˆ©å›ã‚Šã¯ã€æœˆé–“ã§ä¸Šæ˜‡(ä¾¡æ ¼ã¯ä½ä¸‹)', #730
    'å›½å‚µåˆ©å›ã‚Šã¯ã€æœˆé–“ã§ä¸‹è½': 'å›½å‚µåˆ©å›ã‚Šã¯ã€æœˆé–“ã§ä¸‹è½(ä¾¡æ ¼ã¯ä¸Šæ˜‡)', #730

    'å‚µåˆ¸åˆ©å›ã‚Šã¯ä¸Šæ˜‡': 'å‚µåˆ¸åˆ©å›ã‚Šã¯ä¸Šæ˜‡(ä¾¡æ ¼ã¯ä½ä¸‹)', #730
    'å‚µåˆ¸åˆ©å›ã‚Šã¯ä¸‹è½': 'å‚µåˆ¸åˆ©å›ã‚Šã¯ä¸Šæ˜‡(ä¾¡æ ¼ã¯ä¸Šæ˜‡)', #730
    
    'ã‚·ãƒ£ãƒªã‚¢': 'ã‚·ãƒ£ãƒªãƒ¼ã‚¢',
    'TTM': 'ä»²å€¤',
    'æ®†ã©': 'ã»ã¨ã‚“ã©',
    'çœŸä¼¼': 'ã¾ã­',
    'äº˜ã‚‹': 'ã‚ãŸã‚‹',
    'ä½†ã—': 'ãŸã ã—',
    'ç‰½åˆ¶': 'ã‘ã‚“åˆ¶',
    'ç‰½å¼•': 'ã‘ã‚“å¼•',
    'çµ‚ç„‰': 'çµ‚ãˆã‚“',
    'åæ–‚': 'åã‚Œã‚“',
    'é€¼è¿«': 'ã²ã£è¿«',
    'ãƒ¶æœˆ': 'ãƒµæœˆ',
    'å…¥æ›¿ãˆ': 'å…¥ã‚Œæ›¿ãˆ',
    'å…¥æ›¿': 'å…¥ã‚Œæ›¿',
    'å£²ä»˜':'å£²ã‚Šä»˜ã‘',
    'å£²ä»˜ã‘':'å£²ã‚Šä»˜ã‘',
    'æ ¼ä»˜': 'æ ¼ä»˜ã‘', 
    'è²·å»ºã¦': 'è²·ã„å»ºã¦',
    'å£²å»ºã¦': 'å£²ã‚Šå»ºã¦',
    'åˆ‡ä¸Šã’': 'åˆ‡ã‚Šä¸Šã’',
    'åˆ‡æ¨ã¦': 'åˆ‡ã‚Šæ¨ã¦',
    'çµ„å…¥ã‚Œ': 'çµ„ã¿å…¥ã‚Œ', 
    'ç¹°ä¸Šã’å„Ÿé‚„': 'ç¹°ä¸Šå„Ÿé‚„',
    'å…ˆãè¡Œã': 'å…ˆè¡Œã',
    'ä¸‹æ”¯ãˆã‚‹': 'ä¸‹æ”¯ãˆã™ã‚‹',
    'å–ã‚Šå¼•ã': 'å–å¼•',
    'å¼•ä¸Šã’': 'å¼•ãä¸Šã’',
    'å¼•ä¸‹ã’': 'å¼•ãä¸‹ã’',
    'å¼•ç¶šã': 'å¼•ãç¶šã',
    'å¼•ç· ã‚': 'å¼•ãç· ã‚',
    'è–„å•†ã„': 'å–å¼•é‡ãŒå°‘ãªã',
    'ã‚³ã‚¢éŠ˜æŸ„': 'ä¸­æ ¸éŠ˜æŸ„ã€ã‚³ã‚¢ï¼ˆä¸­æ ¸ï¼‰éŠ˜æŸ„',
    'ãƒˆãƒªã‚¬ãƒ¼': 'ãã£ã‹ã‘',
    'ãƒ–ãƒ«ãƒ¼ãƒãƒƒãƒ—ä¼æ¥­': 'å„ªè‰¯ä¼æ¥­',
    'ãƒãƒˆæ´¾': 'é‡‘èç·©å’Œã«å‰å‘ã',
    'ã‚¿ã‚«æ´¾': 'é‡‘èå¼•ãç· ã‚é‡è¦–',
    'ç›¸å ´': 'å¸‚å ´',
    'é€£ã‚Œé«˜': 'å½±éŸ¿ã‚’å—ã‘ã¦ä¸Šæ˜‡',
    'ä¼æ’­': 'åºƒãŒã‚‹',
    'ã§ã‚“ã±': 'åºƒãŒã‚‹',
    'ãƒ¬ãƒ³ã‚¸': 'ç¯„å›²',
    'å›é‡‘': 'å††è»¢',
    'ãƒ­ãƒ¼ãƒ³': 'è²¸ã—ä»˜ã‘',
    'æ‰€è¬‚': 'ã„ã‚ã‚†ã‚‹',
    'æš«ã': 'ã—ã°ã‚‰ã',
    'ç•™ã¾ã‚‹': 'ã¨ã©ã¾ã‚‹',
    'æ­¢ã¾ã‚‹': 'ã¨ã©ã¾ã‚‹',
    'å°š': 'ãªãŠ',
    'ç­ˆ': 'ã¯ãš',
    'è“‹ç„¶æ€§': 'å¯èƒ½æ€§',
    'å•†ã„': 'å‡ºæ¥é«˜',
    'å¾Œå€’ã—': 'å»¶æœŸ',
    'çµŒæ¸ˆæ­£å¸¸åŒ–': 'çµŒæ¸ˆæ´»å‹•æ­£å¸¸åŒ–',
    'é‡‘èæ­£å¸¸åŒ–': 'é‡‘èæ”¿ç­–æ­£å¸¸åŒ–',
    'æ—¥æœ¬éŠ€è¡Œ': 'æ—¥éŠ€',
    'æ”¿æ²»çš„ãƒªã‚¹ã‚¯': 'æ”¿æ²»ãƒªã‚¹ã‚¯',
    'åœ°æ”¿å­¦ãƒªã‚¹ã‚¯': 'åœ°æ”¿å­¦çš„ãƒªã‚¹ã‚¯',
    'ã¸ã®çµ„ã¿å…¥ã‚Œ': 'ã®çµ„ã¿å…¥ã‚Œ',
    'ãƒã‚¤ãƒŠã‚¹ã«å¯„ä¸': 'ãƒã‚¤ãƒŠã‚¹ã«å½±éŸ¿',
    'ãƒã‚¤ãƒŠã‚¹å¯„ä¸': 'ãƒã‚¤ãƒŠã‚¹å½±éŸ¿', #829
    'ç±³å›½å›½å‚µ': 'ç±³å›½å‚µ',
    'æ–°å‹ã‚³ãƒ­ãƒŠ': 'æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹',
    'ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹': 'æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹',
    'ç«‹ã¡å¾Œã‚Œ': 'ç«‹ã¡é…ã‚Œ',
    'ä¼¸å¼µ': 'ä¼¸é•·',
    'ãƒ€ã‚¦å¹³å‡': 'ãƒ€ã‚¦å¹³å‡æ ªä¾¡',
    'NYãƒ€ã‚¦': 'ãƒ€ã‚¦å¹³å‡æ ªä¾¡',
    'ä¸­éŠ€': 'ä¸­å¤®éŠ€è¡Œ', #623
    'è¡Œã‚ã‚Œ': 'è¡Œãªã‚ã‚Œ', #623
    'è¡Œã„': 'è¡Œãªã„', #623
    'è¡Œã‚ãªã„': 'è¡Œãªã‚ãªã„', #821
    'è¡Œã£ãŸ':'è¡Œãªã£ãŸ',
    'è¡Œã†': 'è¡Œãªã†', #623
    'è¡Œã£ã¦': 'è¡Œãªã£ã¦', #623
    'è¡Œã‚ã‚Œã‚‹': 'è¡Œãªã‚ã‚Œã‚‹',
    'ãªã‚Šã—ã¾ã—ãŸ': 'ãªã—ã¾ã—ãŸ', #180015,628
    'è²·ã„ä»˜ã‘ã¾ã—ãŸ': 'è²·ã„ä»˜ã‘ã—ã¾ã—ãŸ',
    # 'è²·ã„ä»˜ã‘': 'è²·ã„ä»˜ã‘ã—', #64977 , 829 fix
    'è²·ä»˜':'è²·ã„ä»˜ã‘',
    'è²·ä»˜ã‘':'è²·ã„ä»˜ã‘ã—', #63207
    'å£²ã‚Šä»˜ã‘ã¾ã—ãŸ':'å£²ã‚Šä»˜ã‘ã—ã¾ã—ãŸ', #628
    'å£²ã‚Šç«‹ã¦ã¾ã—ãŸ':'å£²ã‚Šç«‹ã¦ã—ã¾ã—ãŸ', #628
    'å‰²å®‰ã«': 'å‰²å®‰æ„Ÿã®ã‚ã‚‹',
    'MSCIã‚¤ãƒ³ãƒ‰æŒ‡æ•°': 'MSCIã‚¤ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹',
    'ã‚µã‚¹ãƒ†ãƒŠãƒ–ãƒ«': 'ã‚µã‚¹ãƒ†ã‚£ãƒŠãƒ–ãƒ«',
    'ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ': 'ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ',
    'äº˜': 'ã‚ãŸ',
    'REIT': 'ãƒªãƒ¼ãƒˆ', #629
    'ç‡»': 'ãã™ã¶',
    'ãƒˆãƒ©ãƒ³ãƒ—æ”¿æ¨©': 'ãƒˆãƒ©ãƒ³ãƒ—ç±³æ”¿æ¨©',
    'ãƒˆãƒ©ãƒ³ãƒ—å¤§çµ±é ˜': 'ãƒˆãƒ©ãƒ³ãƒ—ç±³å¤§çµ±é ˜',
    'ç±³ãƒˆãƒ©ãƒ³ãƒ—å¤§çµ±é ˜': 'ç±³ãƒˆãƒ©ãƒ³ãƒ—å¤§çµ±é ˜',
    'å¥½æ„Ÿã•ã‚Œã€ä¸‹è½ã—': 'å¥½æ„Ÿã•ã‚Œä¸‹è½ã—',
    'å«Œæ°—ã•ã‚Œã€ä¸‹è½ã—': 'å«Œæ°—ã•ã‚Œä¸‹è½ã—',
    'å¥½æ„Ÿã•ã‚Œã€ä¸Šæ˜‡ã—': 'å¥½æ„Ÿã•ã‚Œä¸Šæ˜‡ã—',
    'å«Œæ°—ã•ã‚Œã€ä¸Šæ˜‡ã—': 'å«Œæ°—ã•ã‚Œä¸Šæ˜‡ã—',
    'ç•™': 'ã¨ã©', #629
    'å½“ç¤¾': 'åŒç¤¾', #629
    'ç‰½': 'ã‘ã‚“', #629
    'ã“ã¨ç›®æŒ‡ã—ã¦ã„ã‚‹': 'ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹', #630
    'ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§äº‹æ¥­': 'ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«äº‹æ¥­', #630
    'ç©ã¿å¢—ã™': 'ç©ã¿å¢—ã—ã™ã‚‹', #630 
    'å–çµ„ã¿': 'å–ã‚Šçµ„ã¿',
    'é­…åŠ›åº¦': '<sup>â€»</sup>é­…åŠ›åº¦',
    'ãƒ•ãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼': 'ãƒ•ãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼(ç¨å¼•å¾Œå–¶æ¥­åˆ©ç›Šã«æ¸›ä¾¡å„Ÿå´è²»ã‚’åŠ ãˆã€è¨­å‚™æŠ•è³‡é¡ã¨é‹è»¢è³‡æœ¬ã®å¢—åŠ ã‚’å·®ã—å¼•ã„ãŸã‚‚ã® )', #726
    'ãƒ•ãƒªãƒ¼ãƒ»ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼': 'ãƒ•ãƒªãƒ¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼(ç¨å¼•å¾Œå–¶æ¥­åˆ©ç›Šã«æ¸›ä¾¡å„Ÿå´è²»ã‚’åŠ ãˆã€è¨­å‚™æŠ•è³‡é¡ã¨é‹è»¢è³‡æœ¬ã®å¢—åŠ ã‚’å·®ã—å¼•ã„ãŸã‚‚ã® )', #726

    'ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£': 'ãƒœãƒ©ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆä¾¡æ ¼å¤‰å‹•æ€§ï¼‰', #829
    'ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ã‚º': 'ãƒ•ã‚¡ãƒ³ãƒ€ãƒ¡ãƒ³ã‚¿ãƒ«ã‚ºï¼ˆçµŒæ¸ˆã®åŸºç¤çš„æ¡ä»¶ï¼‰', #829

    
}

replace_rules1 ={
    'ã‚·ãƒ£ãƒªã‚¢': 'ã‚·ãƒ£ãƒªãƒ¼ã‚¢',
    'TTM': 'ä»²å€¤',
    'æ®†ã©': 'ã»ã¨ã‚“ã©',
    'çœŸä¼¼': 'ã¾ã­',
    'äº˜ã‚‹': 'ã‚ãŸã‚‹',
    'ä½†ã—': 'ãŸã ã—',
    'ç‰½åˆ¶': 'ã‘ã‚“åˆ¶',
    'ç‰½å¼•': 'ã‘ã‚“å¼•',
    'çµ‚ç„‰': 'çµ‚ãˆã‚“',
    'åæ–‚': 'åã‚Œã‚“',
    'é€¼è¿«': 'ã²ã£è¿«',
    'ãƒ¶æœˆ': 'ãƒµæœˆ',
    'å…¥æ›¿ãˆ': 'å…¥ã‚Œæ›¿ãˆ',
    'å…¥æ›¿': 'å…¥ã‚Œæ›¿',
    'å£²ä»˜':'å£²ã‚Šä»˜ã‘',
    'å£²ä»˜ã‘':'å£²ã‚Šä»˜ã‘',
    'æ ¼ä»˜': 'æ ¼ä»˜ã‘', 
    'è²·å»ºã¦': 'è²·ã„å»ºã¦',
    'å£²å»ºã¦': 'å£²ã‚Šå»ºã¦',
    'åˆ‡ä¸Šã’': 'åˆ‡ã‚Šä¸Šã’',
    'åˆ‡æ¨ã¦': 'åˆ‡ã‚Šæ¨ã¦',
    'çµ„å…¥ã‚Œ': 'çµ„ã¿å…¥ã‚Œ', 
    'ç¹°ä¸Šã’å„Ÿé‚„': 'ç¹°ä¸Šå„Ÿé‚„',
    'å…ˆãè¡Œã': 'å…ˆè¡Œã',
    'ä¸‹æ”¯ãˆã‚‹': 'ä¸‹æ”¯ãˆã™ã‚‹',
    'å–ã‚Šå¼•ã': 'å–å¼•',
    'å¼•ä¸Šã’': 'å¼•ãä¸Šã’',
    'å¼•ä¸‹ã’': 'å¼•ãä¸‹ã’',
    'å¼•ç¶šã': 'å¼•ãç¶šã',
    'å¼•ç· ã‚': 'å¼•ãç· ã‚',
    'è–„å•†ã„': 'å–å¼•é‡ãŒå°‘ãªã',
    'ã‚³ã‚¢éŠ˜æŸ„': 'ä¸­æ ¸éŠ˜æŸ„ã€ã‚³ã‚¢ï¼ˆä¸­æ ¸ï¼‰éŠ˜æŸ„',
    'ãƒˆãƒªã‚¬ãƒ¼': 'ãã£ã‹ã‘',
    'ãƒ–ãƒ«ãƒ¼ãƒãƒƒãƒ—ä¼æ¥­': 'å„ªè‰¯ä¼æ¥­',
    'ãƒãƒˆæ´¾': 'é‡‘èç·©å’Œã«å‰å‘ã',
    'ã‚¿ã‚«æ´¾': 'é‡‘èå¼•ãç· ã‚é‡è¦–',
    'ç›¸å ´': 'å¸‚å ´',
    'é€£ã‚Œé«˜': 'å½±éŸ¿ã‚’å—ã‘ã¦ä¸Šæ˜‡',
    'ä¼æ’­': 'åºƒãŒã‚‹',
    'ã§ã‚“ã±': 'åºƒãŒã‚‹',
    'ãƒ¬ãƒ³ã‚¸': 'ç¯„å›²',
    'å›é‡‘': 'å††è»¢',
    'ãƒ­ãƒ¼ãƒ³': 'è²¸ã—ä»˜ã‘',
    'æ‰€è¬‚': 'ã„ã‚ã‚†ã‚‹',
    'æš«ã': 'ã—ã°ã‚‰ã',
    'ç•™ã¾ã‚‹': 'ã¨ã©ã¾ã‚‹',
    'æ­¢ã¾ã‚‹': 'ã¨ã©ã¾ã‚‹',
    'å°š': 'ãªãŠ',
    'ç­ˆ': 'ã¯ãš',
    'è“‹ç„¶æ€§': 'å¯èƒ½æ€§',
    'å•†ã„': 'å‡ºæ¥é«˜',
    'å¾Œå€’ã—': 'å»¶æœŸ',
    'çµŒæ¸ˆæ­£å¸¸åŒ–': 'çµŒæ¸ˆæ´»å‹•æ­£å¸¸åŒ–',
    'é‡‘èæ­£å¸¸åŒ–': 'é‡‘èæ”¿ç­–æ­£å¸¸åŒ–',
    'æ—¥æœ¬éŠ€è¡Œ': 'æ—¥éŠ€',
    'æ”¿æ²»çš„ãƒªã‚¹ã‚¯': 'æ”¿æ²»ãƒªã‚¹ã‚¯',
    'åœ°æ”¿å­¦ãƒªã‚¹ã‚¯': 'åœ°æ”¿å­¦çš„ãƒªã‚¹ã‚¯',
    'ã¸ã®çµ„ã¿å…¥ã‚Œ': 'ã®çµ„ã¿å…¥ã‚Œ',
    'ãƒã‚¤ãƒŠã‚¹ã«å¯„ä¸': 'ãƒã‚¤ãƒŠã‚¹ã«å½±éŸ¿',
    'ãƒã‚¤ãƒŠã‚¹å¯„ä¸': 'ãƒã‚¤ãƒŠã‚¹å½±éŸ¿', #829
    'ç±³å›½å›½å‚µ': 'ç±³å›½å‚µ',
    'æ–°å‹ã‚³ãƒ­ãƒŠ': 'æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹',
    'ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹': 'æ–°å‹ã‚³ãƒ­ãƒŠã‚¦ã‚¤ãƒ«ã‚¹',
    'ç«‹ã¡å¾Œã‚Œ': 'ç«‹ã¡é…ã‚Œ',
    'ä¼¸å¼µ': 'ä¼¸é•·',
    'ãƒ€ã‚¦å¹³å‡': 'ãƒ€ã‚¦å¹³å‡æ ªä¾¡',
    'NYãƒ€ã‚¦': 'ãƒ€ã‚¦å¹³å‡æ ªä¾¡',
    'ä¸­éŠ€': 'ä¸­å¤®éŠ€è¡Œ', #623
    'è¡Œã‚ã‚Œ': 'è¡Œãªã‚ã‚Œ', #623
    'è¡Œã„': 'è¡Œãªã„', #623
    'è¡Œã‚ãªã„': 'è¡Œãªã‚ãªã„', #821
    'è¡Œã£ãŸ':'è¡Œãªã£ãŸ',
    'è¡Œã†': 'è¡Œãªã†', #623
    'è¡Œã£ã¦': 'è¡Œãªã£ã¦', #623
    'è¡Œã‚ã‚Œã‚‹': 'è¡Œãªã‚ã‚Œã‚‹',
    'ãªã‚Šã—ã¾ã—ãŸ': 'ãªã—ã¾ã—ãŸ', #180015,628
    'è²·ã„ä»˜ã‘ã¾ã—ãŸ': 'è²·ã„ä»˜ã‘ã—ã¾ã—ãŸ',
    # 'è²·ã„ä»˜ã‘': 'è²·ã„ä»˜ã‘ã—', #64977 , 824fix
    'è²·ä»˜':'è²·ã„ä»˜ã‘',
    'è²·ä»˜ã‘':'è²·ã„ä»˜ã‘ã—', #63207
    'å£²ã‚Šä»˜ã‘ã¾ã—ãŸ':'å£²ã‚Šä»˜ã‘ã—ã¾ã—ãŸ', #628
    'å£²ã‚Šç«‹ã¦ã¾ã—ãŸ':'å£²ã‚Šç«‹ã¦ã—ã¾ã—ãŸ', #628
    'å‰²å®‰ã«': 'å‰²å®‰æ„Ÿã®ã‚ã‚‹',
    'MSCIã‚¤ãƒ³ãƒ‰æŒ‡æ•°': 'MSCIã‚¤ãƒ³ãƒ‰ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹',
    'ã‚µã‚¹ãƒ†ãƒŠãƒ–ãƒ«': 'ã‚µã‚¹ãƒ†ã‚£ãƒŠãƒ–ãƒ«',
    'ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ¡ãƒ³ãƒˆ': 'ã‚¨ãƒ³ã‚¿ãƒ¼ãƒ†ã‚¤ãƒ³ãƒ¡ãƒ³ãƒˆ',
    'äº˜': 'ã‚ãŸ',
    'REIT': 'ãƒªãƒ¼ãƒˆ', #629
    'ç‡»': 'ãã™ã¶',
    'ãƒˆãƒ©ãƒ³ãƒ—æ”¿æ¨©': 'ãƒˆãƒ©ãƒ³ãƒ—ç±³æ”¿æ¨©',
    'ãƒˆãƒ©ãƒ³ãƒ—å¤§çµ±é ˜': 'ãƒˆãƒ©ãƒ³ãƒ—ç±³å¤§çµ±é ˜',
    'ç±³ãƒˆãƒ©ãƒ³ãƒ—å¤§çµ±é ˜': 'ç±³ãƒˆãƒ©ãƒ³ãƒ—å¤§çµ±é ˜',
    'å¥½æ„Ÿã•ã‚Œã€ä¸‹è½ã—': 'å¥½æ„Ÿã•ã‚Œä¸‹è½ã—',
    'å«Œæ°—ã•ã‚Œã€ä¸‹è½ã—': 'å«Œæ°—ã•ã‚Œä¸‹è½ã—',
    'å¥½æ„Ÿã•ã‚Œã€ä¸Šæ˜‡ã—': 'å¥½æ„Ÿã•ã‚Œä¸Šæ˜‡ã—',
    'å«Œæ°—ã•ã‚Œã€ä¸Šæ˜‡ã—': 'å«Œæ°—ã•ã‚Œä¸Šæ˜‡ã—',
    'ç•™': 'ã¨ã©', #629
    'å½“ç¤¾': 'åŒç¤¾', #629
    'ç‰½': 'ã‘ã‚“', #629
    'ã“ã¨ç›®æŒ‡ã—ã¦ã„ã‚‹': 'ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹', #630
    'ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§äº‹æ¥­': 'ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«äº‹æ¥­', #630
    'ç©ã¿å¢—ã™': 'ç©ã¿å¢—ã—ã™ã‚‹', #630 
    'å–çµ„ã¿': 'å–ã‚Šçµ„ã¿',
    'é­…åŠ›åº¦': '<sup>â€»</sup>é­…åŠ›åº¦'
}


replace_rules2 ={
    'æ”¿æ²»çš„ãƒªã‚¹ã‚¯': 'æ”¿æ²»ãƒªã‚¹ã‚¯',
    'åœ°æ”¿å­¦ãƒªã‚¹ã‚¯': 'åœ°æ”¿å­¦çš„ãƒªã‚¹ã‚¯',
}

def merge_brackets(content: str) -> str:
    """
    æ‹¬å·å†…æ¢è¡Œç¬¦: 'CPIï¼ˆæ¶ˆè²»è€…ç‰©\nä¾¡æŒ‡æ•°ï¼‰' -> 'CPIï¼ˆæ¶ˆè²»è€…ç‰©ä¾¡æŒ‡æ•°ï¼‰'
    """
    # return regcheck.sub(r'ï¼ˆ[^ï¼‰\n\r]*[\n\r]+[^ï¼‰]*ï¼‰', lambda m: m.group(0).replace("\n", "").replace("\r", ""), content)
    content = regcheck.sub(r'([^\s\n\r])[\s\n\r]+ï¼ˆ', r'\1ï¼ˆ', content)

    def replacer(match):
        inside = match.group(1)
        cleaned = regcheck.sub(r'[\s\u3000]+', '', inside)
        return f'ï¼ˆ{cleaned}ï¼‰'

    return regcheck.sub(r'ï¼ˆ(.*?)ï¼‰', replacer, content, flags=regcheck.DOTALL)


# (4æœˆ30æ—¥ â†’ 2025å¹´4æœˆ30æ—¥)
def insert_year_by_regex(date_str: str, full_text: str, date_pos: int) -> str:
    year_matches = list(regcheck.finditer(r'(\d{4})å¹´', full_text[:date_pos]))
    if year_matches:
        last_year = year_matches[-1].group(1)
        return f'{last_year}å¹´{date_str}'
    return date_str

# (4æœˆ30æ—¥ â†’ 2025å¹´4æœˆ30æ—¥)
def year_half_dict(text: str) -> str:
    full_half = {
        'ï¼': '0', 'ï¼‘': '1', 'ï¼’': '2', 'ï¼“': '3', 'ï¼”': '4',
        'ï¼•': '5', 'ï¼–': '6', 'ï¼—': '7', 'ï¼˜': '8', 'ï¼™': '9'
    }
    return ''.join(full_half.get(c, c) for c in text)


def opt_check_eng(content, rules):
    if not isinstance(rules, dict):
        raise TypeError(f"`rules` must be a dict, got {type(rules)}")
    
    content = merge_brackets(content)
    content = content.replace("(", "ï¼ˆ").replace(")", "ï¼‰")
    lines = content.strip().splitlines()

    seen_raw = set()
    seen_full = set()
    results = []

    for line in lines:
        result = []
        normalized_line = line.replace("\n", "").replace(" ", "")

        for k, v in rules.items():
            raw_key = k.replace("(", "ï¼ˆ").replace(")", "ï¼‰")
            full_key = v.replace("(", "ï¼ˆ").replace(")", "ï¼‰")

            if '(' not in full_key and 'ï¼ˆ' not in full_key:
                continue
            
            escaped_k = regcheck.escape(raw_key)
            escaped_v = regcheck.escape(full_key)

            # ------------------------------
            # keyword æ²¡æœ‰å¯¹åº”çš„pattern
            # ------------------------------
            new_k = escaped_k
            paren_pattern = f"{escaped_k}ï¼ˆ[^ï¼‰]+ï¼‰"

            if raw_key.isalpha() or raw_key in ["S&L", "M&A"]:
                if raw_key == "OPEC":
                    new_k = f"(?<![a-zA-Z]){escaped_k}(?!ãƒ—ãƒ©ã‚¹|[a-zA-Z])"
                elif raw_key == "ã‚¹ãƒ†ã‚£ãƒ¼ãƒ—åŒ–":
                    new_k = f"(?<!ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ã®){escaped_k}"
                elif raw_key == "ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–":
                    new_k = f"{escaped_k}(?!ãƒ»ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«|ã®ã‚¹ãƒ†ã‚£ãƒ¼ãƒ—åŒ–|ã®ãƒ•ãƒ©ãƒƒãƒˆåŒ–)"
                elif raw_key == "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼":
                    new_k = f"(?<!ãƒ•ãƒªãƒ¼){escaped_k}"
                elif raw_key == "ã‚­ãƒ£ãƒªãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‰":
                    new_k = f"(?<!å††){escaped_k}"
                elif raw_key == "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰":
                    new_k = f"(?<!ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ){escaped_k}"
                elif raw_key == "ãƒãƒªãƒ¥ãƒ¼":
                    new_k = f"(?<!ãƒ¬ãƒ©ãƒ†ã‚£ãƒ–ãƒ»|ãƒ•ã‚§ã‚¢){escaped_k}"
                elif raw_key == "ãƒ¢ãƒ¼ã‚²ãƒ¼ã‚¸":
                    new_k = f"{escaped_k}(?!å‚µ)"
                elif raw_key == "å•†ã„":
                    new_k = f"(?<!è–„){escaped_k}"
                else:
                    new_k = f"(?<![a-zA-Z]){escaped_k}(?![a-zA-Z])"

            matched_full = regcheck.search(escaped_v, normalized_line)
            matched_raw_with_paren = regcheck.search(paren_pattern, normalized_line)
            matched_raw = regcheck.search(new_k, normalized_line)

            # âœ… æ ¡éªŒfull_key,ç¬¬ä¸€æ¬¡å‡ºç°
            if matched_full and full_key not in seen_full:
                seen_raw.add(raw_key)
                seen_full.add(full_key)
                continue

            # âœ… full_key ,ç¬¬äºŒæ¬¡å‡ºç°
            elif matched_full and full_key in seen_full:
                result.append({full_key: "åˆ é™¤"})
            
            elif matched_raw_with_paren:
                result.append({matched_raw_with_paren.group(): full_key})
                seen_raw.add(raw_key)
                seen_full.add(full_key)

            elif matched_raw and raw_key not in seen_raw:
                result.append({raw_key: full_key})
                seen_raw.add(raw_key)
                seen_full.add(full_key)

        results.append(result)

    return results

def opt_check_ruru1(content, rules):
    content = merge_brackets(content)

    result = []
    for k, v in rules.items():
        raw_key = k.replace("(", "ï¼ˆ").replace(")", "ï¼‰")
        full_key = v.replace("(", "ï¼ˆ").replace(")", "ï¼‰")

        escaped_k = regcheck.escape(raw_key)
        escaped_v = regcheck.escape(full_key)

        new_k = escaped_k
        if raw_key.isalpha() or raw_key in ["S&L", "M&A"]:
            if raw_key == "OPEC":
                new_k = f"(?<![a-zA-Z]){escaped_k}(?!ãƒ—ãƒ©ã‚¹|[a-zA-Z])"
            elif raw_key == "ã‚¹ãƒ†ã‚£ãƒ¼ãƒ—åŒ–":
                new_k = f"(?<!ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–ã®){escaped_k}"
            elif raw_key == "ã‚¤ãƒ¼ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ–":
                new_k = f"{escaped_k}(?!ãƒ»ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ«|ã®ã‚¹ãƒ†ã‚£ãƒ¼ãƒ—åŒ–|ã®ãƒ•ãƒ©ãƒƒãƒˆåŒ–)"
            elif raw_key == "ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ãƒ­ãƒ¼":
                new_k = f"(?<!ãƒ•ãƒªãƒ¼){escaped_k}"
            elif raw_key == "ã‚­ãƒ£ãƒªãƒ¼ãƒˆãƒ¬ãƒ¼ãƒ‰":
                new_k = f"(?<!å††){escaped_k}"
            elif raw_key == "ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰":
                new_k = f"(?<!ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆ){escaped_k}"
            elif raw_key == "ãƒãƒªãƒ¥ãƒ¼":
                new_k = f"(?<!ãƒ¬ãƒ©ãƒ†ã‚£ãƒ–ãƒ»|ãƒ•ã‚§ã‚¢){escaped_k}"
            elif raw_key == "ãƒ¢ãƒ¼ã‚²ãƒ¼ã‚¸":
                new_k = f"{escaped_k}(?!å‚µ)"
            elif raw_key == "å•†ã„":
                new_k = f"(?<!è–„){escaped_k}"
            else:
                new_k = f"(?<![a-zA-Z]){escaped_k}(?![a-zA-Z])"
        #  ä¸­éŠ€
        elif raw_key == "ä¸­éŠ€":
            matches = regcheck.finditer(escaped_v, content)
            exclude = False
            for m in matches:
                prefix = content[max(0, m.start() - 2): m.start()]
                if prefix and not regcheck.match(r"[ \t\n\r]", prefix):
                    exclude = True
                    break
            if exclude:
                new_k = escaped_k
                full_match = None
            else:
                full_match = regcheck.search(escaped_v, content)

            
        raw_match = regcheck.search(new_k, content)
        full_match = regcheck.search(escaped_v, content)

        if raw_key != "ä¸­éŠ€":
            if full_match and raw_match:
                if full_match.start() <= raw_match.start():
                    continue
            elif full_match and not raw_match:
                continue

        if raw_match:
            result.append({raw_key: full_key})
        
    return result

def keyword_pair_exists(content, keyword_a, keyword_b):
    return keyword_a in content and keyword_b in content

# åœ°æ”¿å­¦ãƒªã‚¹ã‚¯/æ”¿æ²»çš„ãƒªã‚¹ã‚¯
def opt_check_ruru2(content, replace_rules2):
    content = merge_brackets(content)

    result = []

    keyword_pairs = [
        ("åœ°æ”¿å­¦ãƒªã‚¹ã‚¯", "åœ°æ”¿å­¦çš„ãƒªã‚¹ã‚¯"),
        ("æ”¿æ²»çš„ãƒªã‚¹ã‚¯", "æ”¿æ²»ãƒªã‚¹ã‚¯")
    ]

    for a, b in keyword_pairs:
        if keyword_pair_exists(content, a, b):
            result.append({a: b})

    return result

# 0501 debug
def find_corrections(corrected_text,input_text,pageNumber):
    corrections = []
    pattern = r'<span\s+style="color:red;">([\s\S]*?)<\/span>\s*\(<span>\s*ä¿®æ­£ç†ç”±[::]\s*([\s\S]*?)\s*<s[^>]*>([\s\S]*?)<\/s>\s*â†’\s*([\s\S]*?)<\/span>\)'
    matches = re.findall(pattern, corrected_text)

    print("Matches found:", matches)
    # <span style="color:red;">ä¸Šåˆ12æ—¶00åˆ†</span> (<span>ä¿®æ­£ç†ç”±: ä¸è¦ãªä¸­å›½èªè¡¨è¨˜ <s style="background:yellow;color:red">ä¸Šåˆ12æ—¶00åˆ†</s> â†’ ï¼ˆå‰Šé™¤ï¼‰</span>)

    for match in matches:
        if len(match) == 4:
            corrected_text_re = match[0]  #610 debug
            reason_type = match[1].strip()
            original_text = match[2].strip()
            target_text = match[3].strip()

            comment = f"{reason_type} {original_text} â†’ {target_text}"

            corrections.append({
                "page": pageNumber,
                "original_text": corrected_text_re,
                "comment": comment,
                "reason_type":reason_type,
                "check_point": input_text.strip(),
                "locations": [],
                "intgr": False, 
            })
    
    return corrections

# 814 ,add dotfind å¥èª­ç‚¹
#------------------------------------------------------------
def check_fullwidth_period(sentence):
    return sentence.endswith("ã€‚")

#---------------------------------------------------------------------------

# 0623 debug
def find_corrections_wording(input_text,pageNumber,tenbrend,fund_type,input_list):
    corrections = []

#-------------------
    #å¸¸ç”¨å¤–æ±‰å­—
    common_par = r"((å•|è›™|é´‰|åŸƒ|æŒ¨|æ›–|é„|è»‹|æ–¡|æŒ‰|åºµ|é|é—‡|å·²|å¤·|ç•|éŸ‹|å¸·|è|æ¤…|è‘¦|å½™|é£´|è¬‚|é–¾|æº¢|é°¯|å°¹|å’½|æ®·|æ·«|éš•|è”­|äº|è¿‚|ç›‚|çƒ|é¬±|äº‘|æšˆ|ç©¢|æ›³|æ´©|è£”|ç©|å¬°|ç¿³|è…‹|æ›°|å¥„|å®›|æ€¨|ä¿º|å†¤|è¢|å©‰|ç„‰|å °|æ·µ|ç„°|ç­µ|å­|é³¶|ç‡•|é–»|åš¥|å—š|å‡°|å˜”|é´¨|ç”•|è¥–|è¬³|é¶¯|é·—|é¸š|è‡†|ä¿¤|ç“œ|å‘µ|è‹›|ç‚|è¿¦|è¨›|è¨¶|è·|å˜©|ç‘•|æ¦|çª©|è¦|è¸|é‹|é¡†|ç‰™|ç“¦|è‡¥|ä¿„|å³¨|è¨|è›¾|è¡™|é§•|èŠ¥|ä¹–|å»»|å¾Š|æ¢|æ™¦|å º|æ½°|é‹|è«§|æªœ|èŸ¹|å’³|å´–|è“‹|æ¼‘|éª¸|é§|å–€|å»“|æ‘‘|æ”ª|æ„•|è¼|è«¤|é¡|é°|æ¨«|çµ£|ç­ˆ|è‘›|é—Š|é°¹|è±|å¥¸|ä¸²|æ—±|å‡½|å’¸|å§¦|å®¦|æŸ‘|ç«¿|æ‚|æ¡“|æ¶µ|è…|åµŒ|é‰—|æ¾—|ç¿°|è««|ç°|æª»|çŒ|ç©|é›|ç¿«|é ·|ç™Œ|è´‹|å‡ |å‰|å…¶|ç¥|è€†|åŸ¼|æ‚¸|æ†|æ¯€|ç®•|ç•¿|çªº|è«±|å¾½|æ«ƒ|å¦“|ç¥‡|é­|èŸ»|æ¬|éº´|åƒ|å±¹|æ‹®|è¬”|ä»‡|è‡¼|æ±²|ç¸|å’|é‚±|æŸ©|ç¬ˆ|èº¬|å©|å—…|èˆ…|ç‚¬|æ¸ |è£¾|å™“|å¢Ÿ|é‹¸|é½|æ¬…|åŒˆ|æ€¯|ä¿ |è„‡|è¢|ç«Ÿ|å¿|åƒ‘|å¬Œ|è•|é‹|é °|æ©¿|ç–†|é¥—|æ£˜|é«·|å·¾|åƒ…|ç¦½|é¥‰|ç‹—|æƒ§|è»€|æ‡¼|ä¿±|å–°|å¯“|çªŸ|ç²‚|åˆ|èŠ|çª|ç•¦|è„›|é ƒ|ç—™|è©£|ç¦Š|é–¨|ç¨½|é ¸|é«»|è¹Š|é®­|ç¹«|ç¨|æˆŸ|éš™|æŠ‰|é |è¨£|è•¨|å§¸|å€¦|è™”|æ²|ç‰½|å–§|ç¡¯|è…±|éµ|ç¼|é¹¼|å‘Ÿ|çœ©|èˆ·|è«º|ä¹|å§‘|ç‹|è‚¡|æ¶¸|è°|è¢´|å£º|è·¨|ç³Š|é†|é½¬|äº¢|å‹¾|å©|å°»|å¼|è‚›|å²¡|åºš|æ­|è‚´|å’¬|å¢|å··|æ|æ°|ç‹¡|æ¡|èƒ±|å´—|æ¢—|å–‰|è…”|è›¤|å¹Œ|ç…Œ|é‰¤|æ•²|ç¾|è†|é–¤|è† |ç¯|ç¸|è–¨|ç³ |è—|é®«|å£™|æ› |åŠ«|æ¯«|å‚²|å£•|æ¿ |åš™|è½Ÿ|å‰‹|å“­|éµ |ä¹|å¿½|æƒš|æ˜|ç—•|æ¸¾|è¤Œ|å‰|äº›|å—Ÿ|è“‘|ç£‹|å|æŒ«|æ™’|æŸ´|ç ¦|çŠ€|è³½|é°“|æ¦Š|æŸµ|ç‚¸|çª„|ç°€|åˆ¹|æ‹¶|ç´®|æ’’|è–©|çŠ|é¤|çº‚|éœ°|æ”¢|è®ƒ|æ–¬|æ‡º|ä»”|å¼›|æ­¤|å€|ç¥€|å±|å±|æŸ¿|èŒ¨|æ£|ç ¥|ç¥ |ç¿…|èˆ|ç–µ|è¶¾|æ–¯|è¦—|å—œ|æ»“|ç…|å¹Ÿ|æ‘¯|å˜´|ç†¾|é«­|è´„|è€Œ|å³™|ç—”|é¤Œ|ç«º|é›«|ğ ®Ÿ|æ‚‰|è›­|å«‰|è†|æ«›|æŸ˜|æ´’|å¨‘|é€™|å¥¢|é—|æ“|ç¼|ç¶½|éŒ«|é›€|æƒ¹|å¨¶|è…«|è«|é¬š|å‘ª|ç«ª|ç¶¬|èš|æ¿¡|è¥¦|å¸š|é…‹|è¢–|ç¾|è‘º|è’|ç®’|çšº|è¼¯|é¬|ç¹¡|è¹´|è®|é·²|å»¿|æ‰|çµ¨|ç²¥|æˆŒ|é–|æ¥¯|é¦´|æµ|è–¯|è—·|æ±|æŠ’|é‹¤|å¦¾|å“¨|ç§¤|å¨¼|é€|å»‚|æ¤’|æ¹˜|ç«¦|éˆ”|ç«|è›¸|é‰¦|æ‘º|è”£|è£³|èª¦|æ¼¿|è•­|è¸µ|é˜|ç¯ |è³|é¾|é†¬|å›|æ–|èŒ¸|å˜—|æ“¾|æ”˜|é¥’|æ‹­|åŸ´|èœ€|è•|ç‡­|è¤¥|æ²|èŠ¯|å‘»|å®¸|ç–¹|èœƒ|æ»²|è³‘|é¼|å£¬|è¨Š|è…|é±|å¡µ|å„˜|ç¬¥|ç¥Ÿ|è†µ|èª°|éŒ|é›–|éš‹|éš§|èŠ»|è¶¨|é®¨|ä¸¼|å‡„|æ –|æ£²|ç”¥|è²°|èœ»|é†’|éŒ†|è‡|ç€|é¯–|è„†å¼±?|è´…|è„Š|æˆš|æ™°|è¹Ÿ|æ³„|å±‘|æµ™|å•œ|æ¥”|æˆª|å°–|è‹«|ç©¿|é–ƒ|é™|é‡§|æƒ|ç…|ç¾¨|è…º|è©®|ç…½|ç®‹|æ’°|ç®­|è³¤|èŸ¬|ç™¬|å–˜|è†³|ç‹™|ç–½|ç–|ç”¦|æ¥š|é¼ |é¡|è˜‡|é½Ÿ|çˆª|å®‹|ç‚’|åŸ|èš¤|æ›¾|æ¹Š|è‘±|æ”|æ§|æ¼•|ç®|å™Œ|ç˜¡|ç˜¦|è¸ª|è‰˜|è–”|ç”‘|å¢|è—ª|èº|å›ƒ|ç«ˆ|é°º|ä»„|æ‰|å¡|ç²Ÿ|æ£|éœ|å™‚|æ¨½|é±’|ä¾˜|å’¤|è©«|é™€|æ‹¿|è¼|å”¾|èˆµ|æ¥•|é©’|è‹”|æ®†|å †|ç¢“|è…¿|é ½|æˆ´|é†|æ‰˜|é¸|å‡§|è¥·|ç‡µ|å¦|ç–¸|è€½|å•–|è›‹|æ¯¯|æ¹›|ç—°|ç¶»|æ†š|æ­|ç°|è­š|ç˜|é›‰|é¦³|èœ˜|ç·»|ç­‘|è†£|è‚˜|å†‘|ç´|é…|å¨|è››|è¨»|èª…|ç–‡|èºŠ|ä½‡|æ¥®|ç®¸|å„²|ç€¦|èº‡|åŠ|å¸–|å–‹|è²¼|ç‰’|è¶™|éŠš|å˜²|è«œ|å¯µ|æ—|æ•|æ§Œ|éš|è¾»|å‰ƒ|æŒº|é‡˜|æŸ|æ¢¯|é€|å•¼|ç¢‡|é¼|ç¶´|é„­|è–™|è«¦|è¹„|éµœ|è»|æ“¢|æºº|å§ª|è½|è¾¿|å”¸|å¡¡|ç¯†|é¡š|å›€|çº|ä½ƒ|æ·€|æ¾±|è‡€|å…|å¦¬|å…œ|å µ|å± |è³­|å®•|æ²“|å¥—|ç–¼|æ¡¶|æ·˜|è„|é€—|æ£¹|æ¨‹|è•©|é„§|æ©™|æ¿¤|æª®|æ«‚|ç¦±|æ’|ç¦¿|ç€†|æ ƒ|å’„|æ²Œ|é|é “|å|è²ª|é‚‡|åŒ‚|éŸ®|æ¶…|ç¦°|æ|æ»|æ’š|è†¿|å›Š|æ·|çˆ¬|ç¶|é —|æ’­|èŠ­|ç½µ|èŸ‡|èƒš|å¾˜|ç‰Œ|ç¨—|ç‹½|ç…¤|å¸›|æŸ|å‰|ç²•|ç®”|è«|é§|ç€‘|æ›|ç• |æŒ|æ’¥|æ½‘|é†±|ç­|è·‹|å™º|æ°¾|æ±|å›|è¢¢|çµ†|æ–‘|æ§ƒ|å¹¡|æ”€|æŒ½|ç£|è•ƒ|å±|åº‡|ç ’|è„¾|ç—º|é„™|èª¹|è‡‚|æ‡|æ¯˜|æ¢¶|åªš|çµ|è–‡|é¡|ç–‹|ç•¢|é€¼|è¬¬|è±¹|æ†‘|ç“¢|å±›|å»Ÿ|ç‰|ç€•|æ†«|é¬¢|æ–§|é˜œ|è¨ƒ|ä¿¯|é‡œ|è…‘|å­µ|é®’|å·«|è‘¡|æ’«|è•ª|è«·|ç¥“|å»|æ‰®|ç„š|ç³|å¹·|è˜|è”½|é¤…|æ–ƒ|è¢‚|åƒ»|ç’§|è¥|è”‘|ç¥|æ‰|ç¯‡|é¨™|å¨©|é­|å“º|åœƒ|è’²|æˆŠ|ç‰¡|å§¥|è©|å‘†|å½·|åº–|è‹|ç–±|æ§|é€¢|èœ‚|è“¬|é„|é‹’|ç‰Ÿ|èŠ’|èŒ«|è™»|æ¦œ|è†€|è²Œ|é‰¾|è¬—|å |åœ|å‹ƒ|æ¢µ|æ˜§|é‚|æ¡|ä¿£|æ²«|è¿„|æ›¼|è”“|ç|é¥…|é¬˜|é°»|èœœ|éµ¡|å†¥|ç‘|è¬|éºµ|è’™|æœ¦|å‹¿|ç±¾|æ‚¶|æ¶|çˆº|é‘“|å–©|æ„|æ„ˆ|æ¥¡|å°¤|é‡‰|æ¥¢|çŒ·|é£«|è¼¿|å­•|å¦–|æ‹—|æ¶Œ|ç—’|å‚­|ç†”|ç˜|è …|æ²ƒ|èº|èŠ|è•¾|æ´›|åŸ’|æ‹‰|è¾£|ç€¾|çˆ›|é¸|ç‹¸|è£¡|ç½¹|ç±¬|æˆ®|æ…„|æ |ç¬ |æºœ|æ¦´|åŠ‰|ç˜¤|ä¾¶|æ¢|èŠ|è±|å¯¥|è“¼|æ·‹|ç‡|é±—|å±¢|è›‰|è £|æ«Ÿ|ç¤«|è½¢|ç…‰|æ¼£|æ†|ç°¾|é°Š|æ”£|è³‚|é­¯|æ¿¾|å»¬|æ«“|è˜†|é·º|å¼„|ç‰¢|ç‹¼|æ¦”|ç˜»|ï¨Ÿ|è‡˜|æœ§|è Ÿ|ç± |è¾|è‚‹|å‹’|æ¼‰|éº“|çªª|æ­ª|çŒ¥|éšˆ|æˆ–|ç½ |æ¤€|ç¢—|å½|ä¸€æ—¦).{,5})"
    common_list = regcheck.findall(common_par, input_text)
    
    for word in common_list:
        reason_type = "å¸¸ç”¨å¤–æ¼¢å­—ã®ä½¿ç”¨"
        corrections.append({
            "page": pageNumber,
            "original_text": word[0],  # original_text,
            "comment": word[0],
            "reason_type": reason_type,
            "check_point": word[1],
            "locations": [],
            "intgr": False,  
        })
#-------------------
    if fund_type == 'public':
        # ï¼ˆåŠè§’â†’å…¨è§’ï¼‰ -0.09% â†’ -0.09ï¼…
        pattern_half_width_katakana = r"[ï½¦-ï¾%ï¼ ]+"
        half_width_katakana_matches = regcheck.findall(pattern_half_width_katakana, input_text)

        for match in half_width_katakana_matches:
            corrected_text_re = half_and_full_process(match,half_to_full_dict)  # åŠè§’â†’å…¨è§’
            reason_type = "åŠè§’ã‚’å…¨è§’çµ±ä¸€"
            original_text = match
            target_text = corrected_text_re
            # ã€Œï¼…ã€è¡¨è¨˜ã®çµ±ä¸€ï¼ˆåŠè§’â†’å…¨è§’ï¼‰ -0.09% â†’ -0.09ï¼…
            comment = f"{reason_type} {original_text} â†’ {target_text}"

            corrections.append({
                "page": pageNumber,
                "original_text": original_text,#corrected_text_re
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False, 
            })

        # # ï¼ˆåŠè§’æ‹¬å¼§ â†’ å…¨è§’æ‹¬å¼§ï¼‰ -() â†’ () ,with date format: \((?!\d{4}å¹´\d{1,2}æœˆ\d{1,2}æ—¥)([^)]+)\)
        # pattern_half_width_kuohao = r"\(([^)]+)\)"
        # half_width_kuohao_matches = regcheck.findall(pattern_half_width_kuohao, input_text)

        # for match in half_width_kuohao_matches:
        #     corrected_text_re = half_and_full_process(match,half_to_full_dict)  # åŠè§’â†’å…¨è§’
        #     reason_type = "åŠè§’æ‹¬å¼§ã‚’å…¨è§’æ‹¬å¼§ã«çµ±ä¸€"
        #     original_text = match
        #     converted = corrected_text_re
        #     target_text = re.sub(r'\(([^)]+)\)', r'ï¼ˆ\1ï¼‰', converted)
        #     # ()è¡¨è¨˜ã®çµ±ä¸€(åˆ†é…é‡‘å†æŠ•è³‡)ï¼‰ -(åˆ†é…é‡‘å†æŠ•è³‡) â†’ ï¼ˆåˆ†é…é‡‘å†æŠ•è³‡ï¼‰
        #     comment = f"{reason_type} {original_text} â†’ {target_text}"

        #     corrections.append({
        #         "page": pageNumber,
        #         "original_text": original_text,#corrected_text_re
        #         "comment": comment,
        #         "reason_type": reason_type,
        #         "check_point": input_text.strip(),
        #         "locations": [],
        #         "intgr": False, 
        #     })

        # åŠè§’â†’å…¨è§’
        pattern_full_width_numbers_and_letters = r"[ï¼-ï¼™ï¼¡-ï¼ºï¼‹ï¼]+"
        full_width_matches = regcheck.findall(pattern_full_width_numbers_and_letters, input_text)

        for match in full_width_matches:
            corrected_text_re = half_and_full_process(match,full_to_half_dict)  # å…¨è§’â†’åŠè§’
            reason_type = "å…¨è§’ã‚’åŠè§’çµ±ä¸€"
            original_text = match
            target_text = corrected_text_re

            comment = f"{reason_type} {original_text} â†’ {target_text}"

            corrections.append({
                "page": pageNumber,
                "original_text": original_text,
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False, 
            })
            
        # ï¼ˆæ³¨0-9ï¼‰--åˆ é™¤
        pattern_full_delete = r"ï¼ˆæ³¨[0-9]+ï¼‰"
        full_width_matches_delete = regcheck.findall(pattern_full_delete, input_text)

        for match in full_width_matches_delete:
            corrected_text_re = match
            reason_type = "åˆ é™¤"
            original_text = match
            target_text = corrected_text_re

            comment = f"{reason_type} {original_text} â†’ {target_text}"

            corrections.append({
                "page": pageNumber,
                "original_text": original_text,
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False,
            })
#-------------------
    # å¹´åº¦
    # if fund_type == 'public':
    #     cleaned_text = regcheck.sub(r'\n\s*', '', input_text)
    #     date_pattern = r'(?<!\d{4}å¹´)(\d{1,2})æœˆ(\d{1,2})æ—¥'

    #     for match in regcheck.finditer(date_pattern, cleaned_text):
    #         date_str = match.group(0)               # '4æœˆ30æ—¥'
    #         date_pos = match.start()            
    #         full_date = insert_year_by_regex(date_str, cleaned_text, date_pos)
    #         half_date = year_half_dict(full_date)

    #         context_pattern = r'.{0,8}' + regcheck.escape(date_str)
    #         context_match = regcheck.search(context_pattern, cleaned_text)
    #         original_text = context_match.group() if context_match else date_str

    #         comment = f"{original_text} â†’ {half_date}"
    #         corrections.append({
    #             "page": pageNumber,
    #             "original_text": original_text,
    #             "comment": comment,
    #             "reason_type": 'å¹´åº¦ç”¨èªã®çµ±ä¸€',
    #             "check_point": 'å¹´åº¦ç”¨èªã®çµ±ä¸€',
    #             "locations": [],
    #             "intgr": False,  # for debug
    #         })
#-------------------
    # è‹±ç•¥è¯
    if fund_type == 'public':
        results = opt_check_eng(input_text, replace_rules)

        for line_result in results:
            if line_result:
                for item in line_result:
                    if isinstance(item, dict):
                        for original_text, corrected_text_re in item.items():
                            reason_type = "ç”¨èªã®çµ±ä¸€"
                        
                            if corrected_text_re == "åˆ é™¤":
                                comment = f"{original_text} â†’ ãƒˆãƒ«ã¯ä¸è¦"
                            else:
                                comment = f"{original_text} â†’ {corrected_text_re}"

                            corrections.append({
                                "page": pageNumber,
                                "original_text": original_text,
                                "comment": comment,
                                "reason_type": reason_type,
                                "check_point": reason_type,
                                "locations": [],
                                "intgr": False,
                            })


        results_ruru1 = opt_check_ruru1(input_text, replace_rules1)
    
        for item in results_ruru1:
            for k, v in item.items():
                original_text = k
                corrected_text_re = v
                reason_type = "ç”¨èªã®çµ±ä¸€"

                comment = f"{reason_type} {original_text} â†’ {corrected_text_re}"

            corrections.append({
                "page": pageNumber,
                "original_text": extract_text(input_text, original_text),# original_text,
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False,  
            })

# è‹±ç•¥è¯ï¼Œonly åœ°æ”¿å­¦
    if fund_type == 'private':
        results_ruru2 = opt_check_ruru2(input_text, replace_rules2)
    
        for item in results_ruru2:
            for k, v in item.items():
                original_text = k  # original_text save to AI
                corrected_text_re = v  # value(v)ì„ corrected_text_re save to AIï¼ˆäººå·¥çŸ¥èƒ½ï¼‰
                reason_type = "ç”¨èªã®çµ±ä¸€"

                comment = f"{reason_type} {original_text} â†’ {corrected_text_re}"

            corrections.append({
                "page": pageNumber,
                "original_text": extract_text(input_text, original_text),# original_text,
                "comment": comment,
                "reason_type": reason_type,
                "check_point": reason_type,
                "locations": [],
                "intgr": False,
            })

# -----------------
    if fund_type == 'public':
        word_re = regcheck.findall(r"å¤–å›½äººæŠ•è³‡å®¶ã‹ã‚‰ã®è³‡é‡‘æµå…¥|å¤–å›½äººæŠ•è³‡å®¶ã®è³‡é‡‘æµå‡º|åŠ é€Ÿ", input_text)
        for word_result in word_re:
            corrections.append({
                "page": pageNumber,
                "original_text": word_result,
                "comment": f"{word_result} â†’ ", #word_result,
                "reason_type": "ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º",
                "check_point": word_result,
                "locations": [],  
                "intgr": False,  
            })

        day_re = regcheck.findall(r"\d{1,2}[~ï½]\d{1,2}æœˆæœŸ|\d{1,2}æœˆ\d{1,2}æ—¥[~ï½]\d{1,2}æœˆ\d{1,2}æ—¥", input_text)
        for day_result in day_re:
            cor_day = day_result.replace("~", "-").replace("ï½", "-")
            corrections.append({
                "page": pageNumber,
                "original_text": day_result,
                "comment": f"{day_result} â†’ {cor_day}",
                "reason_type": "æ³¢ãƒ€ãƒƒã‚·ãƒ¥ã®ä¿®æ­£",
                "check_point": day_result,
                "locations": [],  
                "intgr": False,  
            })

        score_re = regcheck.findall(r"[\d.]+?[ï½~][\d.]+?[%ï¼…]", input_text)
        for score_result in score_re:
            cor_score = score_result.replace("ï½", "ï¼…ï½").replace("~", "ï¼…~")
            corrections.append({
                "page": pageNumber,
                "original_text": score_result,
                "comment": f"{score_result} â†’ {cor_score}",
                "reason_type": "æ³¢ãƒ€ãƒƒã‚·ãƒ¥ã®ä¿®æ­£",
                "check_point": score_result,
                "locations": [],  
                "intgr": False,  
            })

    half_re = regcheck.findall(r"\d{2,4}å¹´ç¬¬[1-4ä¸€äºŒä¸‰å››]å››åŠæœŸ", input_text)
    for half_result in half_re:
        half_num = half_result[-3]
        if half_num in ["1", "ä¸€"]:
            time_range = "1-3"
        elif half_num in ["2", "äºŒ"]:
            time_range = "4-6"
        elif half_num in ["3", "ä¸‰"]:
            time_range = "7-9"
        else:
            time_range = "10-12"
        cor_half = half_result[: -4] + time_range + half_result[-3:]
        cor_half_len = len(cor_half.split("å¹´", 1)[0])
        if cor_half_len < 4:
            cor_half = "20" + cor_half[cor_half_len - 2:]
        corrections.append({
            "page": pageNumber,
            "original_text": half_result,
            "comment": f"{half_result} â†’ {cor_half}",
            "reason_type": "æ—¥ä»˜ã®ä¿®æ­£",
            "check_point": half_result,
            "locations": [],  
            "intgr": False,  
        })


#-------------------
    # tenbrend
    if isinstance(tenbrend, list):
        for item in tenbrend:
            if not isinstance(item, dict):
                continue

            old_text = item.get("å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬", "").strip()
            new_text = item.get("æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬", "").strip()

            corrections.append({
                "check_point": "çµ„å…¥éŠ˜æŸ„è§£èª¬",
                "comment": f"{old_text} â†’ {new_text}",
                "intgr": False,
                "locations": [],
                "original_text": new_text[:20],
                "page": pageNumber,
                "reason_type": item.get("åˆ†é¡", "")
            })

#---------------------
# dotfind å¥ç‚¹ã®è¿½åŠ   å¥èª­ç‚¹  
    for sentence in input_list:
        if "ï¼ˆå‡ºæ‰€ï¼‰" in sentence:
            continue

        if not check_fullwidth_period(sentence):
            sentence_split = re.sub(r"\s+$", "", sentence)[-30:]
            corrections.append({
                "check_point": "å¥ç‚¹ã®è¿½åŠ ",
                "comment": f"{sentence_split} â†’ {sentence_split}ã€‚",
                "intgr": False,
                "locations": [],
                "original_text": sentence_split,
                "page": pageNumber,
                "reason_type": "å¥ç‚¹ã®è¿½åŠ ",
            })
#--------------------add--0905--
            # ä¸»èªæ¬ è½ã‚’æ¤œçŸ¥ã™ã‚‹æ­£è¦è¡¨ç¾ pattern
            # ã€Œã€œã¨ç¤ºå”†ã—ãŸã€ ã®ç›´å‰ã«ã€ŒãŒ|ã¯ã€ãªã©ã®ä¸»èª 
            pattern = r"([^ã€ã€‚]+?ã¨ç¤ºå”†ã—ãŸ)"

            matches = re.finditer(pattern, input_text)

            for match in matches:
                original_text = match.group(0)
                # ä¿®æ­£æ¡ˆ: ã€ŒåŒç¤¾ãŒã€œã“ã¨ã‚’ç¤ºå”†ã—ãŸã€
                corrected_text_re = f"åŒç¤¾ãŒ{match.group(1)}ã“ã¨ã‚’ç¤ºå”†ã—ãŸ"
                reason_type = "ä¸»èªã®æ¬ è½"

                comment = f"{reason_type}: {original_text} â†’ {corrected_text_re}"

                corrections.append({
                    "page": pageNumber,
                    "original_text": original_text,
                    "comment": comment,
                    "reason_type": reason_type,
                    "check_point": reason_type,
                    "locations": [],   # å¿…è¦ãªã‚‰ä½ç½®æƒ…å ±ã‚’è¿½åŠ 
                    "intgr": False,
                })
#--------------------
    return corrections

def extract_text(input_text, original_text):
    pattern = rf"{original_text}ï¼ˆ[^ï¼‰]*ï¼‰|{original_text}"

    match = regcheck.search(pattern, input_text)
    
    if match:
        return match.group(0)
    else:
        return None

def clean_percent_prefix(value: str):
    if not isinstance(value, str):
        return None
    for symbol in ['ï¼…', '%', 'ãƒã‚¤ãƒ³ãƒˆ']:
        if symbol in value:
            value = value.split(symbol)[0].strip()
            return f"{value}{symbol}"
    return value.strip()
                
def extract_parts_with_direction(text: str):
    parts = re.split(r'[ã€ã€‚\n]', text)
    
    segments = []

    for part in parts:
        part = part.strip()
        if not part:
            continue

        # pattern = r'[^ï¼…%ãƒã‚¤ãƒ³ãƒˆä¸Šä¸‹ã€ã€‚\n]*[+-âˆ’]?\d+(?:\.\d+)?(?:ï¼…|%|ãƒã‚¤ãƒ³ãƒˆ)'
        pattern = r'[^ï¼…%ã€ã€‚\n]*[+-âˆ’]{0,2}\d+(?:\.\d+)?(?:ï¼…|%|ãƒã‚¤ãƒ³ãƒˆ)'
        segments.extend(re.findall(pattern, part))

        # ä¸Šä¸‹æ–¹å‘
        # direction_match = re.findall(r'(ä¸Šå›ã‚Šã¾ã—ãŸ|ä¸‹å›ã‚Šã¾ã—ãŸ)', part)
        # segments.extend(direction_match)

    return segments

def extract_corrections(corrected_text, input_text,pageNumber):
    corrections = []
    
    # correction span
    pattern_alt = re.compile(
        r'<span.*?>(.*?)<\/span>\s*'
        r'\(<span>æç¤º:\s*(.*?)\s*<s.*?>(.*?)<\/s>\s*â†’\s*(.*?)<\/span>\)',
        re.DOTALL
    )

    matches = pattern_alt.findall(corrected_text)

    for match in matches:
        original = match[0].strip()
        reason = match[2].strip()
        reason_type = match[1].strip()
        corrected = match[3].strip()

        comment = f"{reason} â†’ {corrected}" if corrected else reason
        # "%": "ï¼…"
        corrections.append({
            "page": pageNumber,
            "original_text": clean_percent_prefix(reason),
            "comment": comment, # +0.2% â†’ 0.85% , ä¸Šå‡ -> ä¸‹è½
            "reason_type": reason_type, # ãƒ•ã‚¡ãƒ³ãƒ‰ã®é¨°è½ç‡ï¼ŒB-xxx

            "check_point": input_text.strip(), # å½“æœˆã®ãƒ•ã‚¡ãƒ³ãƒ‰ã®é¨°è½ç‡ã¯+0.2%ã¨ãªã‚Šã¾ã—ãŸã€‚ A B -xxx
            "locations": [],
            "intgr": True,
        })

    return corrections

    
def add_comments_to_pdf(pdf_bytes, corrections):
    if not isinstance(pdf_bytes, bytes):
        raise ValueError("pdf_bytes must be a bytes object.")
    if not isinstance(corrections, list):
        raise ValueError("corrections must be a list of dictionaries.")
    for correction in corrections:
        if not all(key in correction for key in ["page", "original_text", "comment"]):
            raise ValueError("Each correction must contain 'page', 'original_text', and 'comment' keys.")

    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    except Exception as e:
        raise ValueError(f"Invalid PDF file: {str(e)}")

    for idx, correction in enumerate(corrections):
        page_num = correction["page"]
        comment = correction["comment"]
        reason_type = correction["reason_type"]
        locations = correction["locations"][0]
        text_instances = [fitz.Rect(locations["x0"], locations["y0"], locations["x1"], locations["y1"])]
        if int(text_instances[0][0]) == 0:
            continue
        colorSetFill= (1, 1, 0)

        if page_num < 0 or page_num >= len(doc):
            raise ValueError(f"Invalid page number: {page_num}")

        page = doc.load_page(page_num)

        if correction["intgr"]:
            colorSetFill = (172/255, 228/255, 230/255)
        else:
            colorSetFill= (1, 1, 0)

        for rect in text_instances:
            highlight = page.add_rect_annot(rect)
            highlight.set_colors(stroke=None, fill=colorSetFill)
            highlight.set_opacity(0.5)
            highlight.set_info({
                "title": reason_type,  # å¯é€‰ï¼šæ˜¾ç¤ºåœ¨æ³¨é‡Šæ¡†æ ‡é¢˜æ 
                "content": comment
            })
            highlight.update()

    output = io.BytesIO()
    doc.save(output)
    output.seek(0)
    doc.close()

    return output


def add_comments_to_excel(excel_bytes, corrections):
    excel_file = io.BytesIO(excel_bytes)
    workbook = load_workbook(excel_file)  # openpyxl

    for sheet_name in workbook.sheetnames:
        sheet = workbook[sheet_name]
        for correction in corrections:
            if correction["sheet"] == sheet_name:
                cell = correction["cell"]  # : "A1", "B2"
                original_text = correction["original_text"]
                comment = correction["comment"]

                if sheet[cell].value and original_text in str(sheet[cell].value):
                    fill = PatternFill(start_color="FFFF0000", end_color="FFFF0000", fill_type="solid")
                    sheet[cell].fill = fill

                    sheet[cell].comment = Comment(comment, "Author")

    output = io.BytesIO()
    workbook.save(output)
    output.seek(0)
    return output

# pre processing
def normalize_text_for_search(text: str) -> str:
    import re
    replacements = {
        "ï¼ˆ": "(", "ï¼‰": ")", "ã€": "[", "ã€‘": "]",
        "ã€Œ": "\"", "ã€": "\"", "ã€": "\"", "ã€": "\"",
        "ã€€": " ", "â—‹": "ã€‡", "ãƒ»": "ï½¥", 
        "â€“": "-", "â€•": "-", "âˆ’": "-", "ãƒ¼": "-"
    }
    for k, v in replacements.items():
        text = text.replace(k, v)
    text = text.replace("\n", " ").replace("\r", " ")
    text = re.sub(r"[\u200b\u200c\u200d\u00a0]", "", text)
    return re.sub(r"\s+", " ", text).strip()


#0617 debug
def find_locations_in_pdf(pdf_bytes, corrections):
    try:
        doc = fitz.open(stream=pdf_bytes, filetype="pdf")
    except Exception as e:
        raise ValueError(f"Invalid PDF file: {str(e)}")

    for idx, correction in enumerate(corrections):
        page_num = correction.get("page", 0)
        original_text = correction["original_text"]

        if page_num < 0 or page_num >= len(doc):
            print(f"Warning: Invalid page number: {page_num}")
            continue

        page = doc[page_num]
        found_locations = []
        # pre processing
        text_instances = page.search_for(original_text)

        if not text_instances:
            print(f"Warning: Text '{original_text}' not found on page {page_num}.")
            found_locations.append({
                "x0": 0,
                "y0": 0,
                "x1": 0,
                "y1": 0
            })
            
        else:
            for inst in text_instances:
                rect = fitz.Rect(inst)
                found_locations.append({
                    "x0": rect.x0,
                    "y0": rect.y0,
                    "x1": rect.x1,
                    "y1": rect.y1
                })

        if "locations" not in corrections[idx]:
            corrections[idx]["locations"] = []
        corrections[idx]["locations"].extend(found_locations)

    doc.close()
    return corrections


# db and save blob
PUBLIC_FUND_CONTAINER_NAME = "public_Fund"
PRIVATE_FUND_CONTAINER_NAME = "private_Fund"
CHECKED_PDF_CONTAINER = "checked_pdf"

public_container = get_db_connection(PUBLIC_FUND_CONTAINER_NAME)
private_container = get_db_connection(PRIVATE_FUND_CONTAINER_NAME)
checked_pdf_container = get_db_connection(CHECKED_PDF_CONTAINER)

def upload_to_azure_storage(pdf_bytes, file_name, fund_type):
        """Azure Blob Storage PDF"""
        container_name = PUBLIC_FUND_CONTAINER_NAME if fund_type == 'public' else PRIVATE_FUND_CONTAINER_NAME
        
        container_client = get_storage_container()

        try:
            blob_client = container_client.get_blob_client(file_name)
            blob_client.upload_blob(pdf_bytes, overwrite=True)
            logging.info(f"âœ… Blob uploaded: {file_name} to {container_name}")
            return blob_client.url
        except Exception as e:
            logging.error(f"âŒ Storage Upload error: {e}")
            return None
def upload_checked_pdf_to_azure_storage(pdf_bytes, file_name, fund_type):
        """Azure Blob Storage PDF"""
        container_name = CHECKED_PDF_CONTAINER

        container_client = get_storage_container()

        try:
            blob_client = container_client.get_blob_client(file_name)
            blob_client.upload_blob(pdf_bytes, overwrite=True)
            logging.info(f"âœ… Blob uploaded: {file_name} to {container_name}")
            return blob_client.url
        except Exception as e:
            logging.error(f"âŒ Storage Upload error: {e}")
            return None
def download_checked_pdf_from_azure_storage(file_name: str, fund_type: str = None) -> bytes:
    """
    ä» Azure Blob Storage ä¸‹è½½ PDF
    :param file_name: æ–‡ä»¶åï¼Œä¾‹å¦‚ "a_checked.pdf"
    :param fund_type: å…¬å‹Ÿæˆ–è€…ç§å‹Ÿ
    :return: PDF æ–‡ä»¶çš„å­—èŠ‚æµï¼ˆbytesï¼‰ï¼Œå¤±è´¥æ—¶è¿”å› None
    """
    container_name = CHECKED_PDF_CONTAINER
    container_client = get_storage_container()

    try:
        blob_client = container_client.get_blob_client(file_name)
        # ä¸‹è½½ blob åˆ°å†…å­˜
        download_stream = blob_client.download_blob()
        pdf_bytes = download_stream.readall()
        logging.info(f"ğŸ“¥ Blob downloaded: {file_name} from {container_name}")
        return pdf_bytes
    except Exception as e:
        logging.error(f"âŒ Storage Download error: {e}")
        return None

def save_to_cosmos(file_name, response_data, link_url, fund_type, upload_type='', comment_type='',icon=''):
    """Cosmos DB Save"""
    # Cosmos DB è¿æ¥
    container = public_container if fund_type == 'public' else private_container

    # match = re.search(r'(\d{0,}(?:-\d+)?_M\d{4})', file_name)
    # if match:
    #     file_id = match.group(1)
    # else:
    #     file_id = file_name

    item = {
        'id': file_name,
        'fileName': file_name,
        'result': response_data,
        'link': link_url,
        'updateTime': datetime.utcnow().isoformat(),
        'status': "issue", 
        'readStatus': "unread",
        'icon': icon,
    }
    if upload_type:
        item.update(upload_type=upload_type)
    if comment_type:
        item.update(comment_type=comment_type)


    try:
        existing_item = list(container.query_items(
                query="SELECT * FROM c WHERE c.id = @id",
                parameters=[{"name": "@id", "value": file_name}],
                enable_cross_partition_query=True
            ))

        if not existing_item:
                container.create_item(body=item)
                logging.info(f"âœ… Cosmos DB ã¯ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™: {file_name}")
        else:
            existing_item[0].update(item)
            container.upsert_item(existing_item[0])

            logging.info(f"ğŸ”„ Cosmos DB æ›´æ–°å®Œäº†: {file_name}")
                
    except CosmosHttpResponseError as e:
        logging.error(f"âŒCosmos DB save error: {e}")

def save_checked_pdf_cosmos(file_name, response_data, link_url, fund_type, upload_type='', comment_type='',icon=''):
    """Cosmos DB Save"""
    # Cosmos DB è¿æ¥
    container = 'checked_pdf'

    item = {
        'id': file_name,
        'fileName': file_name,
        'result': response_data,
        'link': link_url,
        'updateTime': datetime.utcnow().isoformat(),
        'status': "checked", 
        'readStatus': "unread",
        'icon': icon,
    }
    if upload_type:
        item.update(upload_type=upload_type)
    if comment_type:
        item.update(comment_type=comment_type)


    try:
        existing_item = list(container.query_items(
                query="SELECT * FROM c WHERE c.id = @id",
                parameters=[{"name": "@id", "value": file_name}],
                enable_cross_partition_query=True
            ))

        if not existing_item:
                container.create_item(body=item)
                logging.info(f"âœ… Cosmos DB ã¯ä¿å­˜ã•ã‚Œã¦ã„ã¾ã™: {file_name}")
        else:
            existing_item[0].update(item)
            container.upsert_item(existing_item[0])

            logging.info(f"ğŸ”„ Cosmos DB æ›´æ–°å®Œäº†: {file_name}")
                
    except CosmosHttpResponseError as e:
        logging.error(f"âŒCosmos DB save error: {e}")

@app.route('/api/file_status', methods=['POST'])
def get_file_status():
    data = request.json
    fund_type = data.get("fund_type", "public_Fund")
    file_name = data.get("file_name")
    container = public_container if fund_type == 'public_Fund' else private_container
    if file_name:
        query = f"SELECT * FROM c WHERE c.fileName = '{file_name}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        if items:
            return jsonify({"success": True, "status": True}), 200
    return jsonify({"success": True, "status": False}), 200


@app.route('/api/download_checked_pdf', methods=['POST'])
def download_checked_pdf():
    try:
        data = request.json
        fund_type = data.get("fund_type", "public_Fund")
        file_name = data.get("file_name")
        root, ext = os.path.splitext(file_name)
        if ext.lower() == ".pdf":
            file_name = root + "_checked" + ext
        container = get_db_connection(CHECKED_PDF_CONTAINER)

        query = f"SELECT link_url FROM c WHERE c.file_name = '{file_name}' AND c.fund_type = '{fund_type}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))
        if items:
            link_url = items[0].get("link_url")
            return jsonify({"success": True, "status": True,"link_url":link_url}), 200
    


    except Exception as e:
        logging.error(f"âŒ Error in write_checked_pdf: {e}")
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/write_upload_save', methods=['POST'])
def write_upload_save():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("âœ… token upload")

        data = request.json
        pdf_base64 = data.get("pdf_bytes", "")
        excel_base64 = data.get("excel_bytes", "")
        docx_base64 = data.get("docx_bytes", "")
        resutlmap = data.get("original_text", "")
        fund_type = data.get("fund_type", "public")  # 'public'
        file_name_decoding = data.get("file_name", "")
        upload_type = data.get("upload_type", "")
        comment_type = data.get("comment_type", "")
        icon = data.get("icon", "")
        change_flag = data.get("change_flag", "")

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        #---------EXCEL-----------
        if excel_base64:
            try:
                excel_bytes = base64.b64decode(excel_base64)
                response_data = {
                    "success": True,
                    "corrections": []
                }

                # Blob Upload
                link_url = upload_to_azure_storage(excel_bytes, file_name, fund_type)
                if not link_url:
                    return jsonify({"success": False, "error": "Blob upload failed"}), 500

                # Cosmos DB Save
                save_to_cosmos(file_name, response_data, link_url, fund_type, upload_type, comment_type,icon)
                if upload_type != "å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«" and change_flag == "change":
                    container = get_db_connection(FILE_MONITOR_ITEM)
                    container.upsert_item({"id": str(uuid.uuid4()), "file_name": file_name, "flag": "wait",
                                            "link": link_url, "fund_type": fund_type})

            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500

            # 3) return xlsx
            return jsonify({
                "success": True,
                "corrections": [],
                "code": 200,
            })
        # ---------PDF -----------
        if pdf_base64:
            try:
                pdf_bytes = base64.b64decode(pdf_base64)

                response_data = {
                    "corrections": []
                }

                # Blob Upload
                link_url = upload_to_azure_storage(pdf_bytes, file_name, fund_type)
                if not link_url:
                    return jsonify({"success": False, "error": "Blob upload failed"}), 500

                # Cosmos DB Save
                save_to_cosmos(file_name, response_data, link_url, fund_type, upload_type, comment_type,icon)

            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500
            
        # ---------DOCX -----------
        if docx_base64:
            try:
                docx_bytes = base64.b64decode(docx_base64)

                response_data = {
                    "corrections": []
                }

                # Blob Upload
                link_url = upload_to_azure_storage(docx_bytes, file_name, fund_type)
                if not link_url:
                    return jsonify({"success": False, "error": "Blob upload failed"}), 500

                # Cosmos DB Save
                save_to_cosmos(file_name, response_data, link_url, fund_type, upload_type, comment_type,icon)
                if upload_type != "å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«" and change_flag == "change":
                    container = get_db_connection(FILE_MONITOR_ITEM)
                    container.upsert_item({"id": str(uuid.uuid4()), "file_name": file_name, "flag": "wait",
                                            "link": link_url, "fund_type": fund_type})

            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500
            
            # return JSON
            return jsonify({
                "success": True,
                "corrections": [],
                "code": 200,
            })

        # return JSON
        return jsonify({
            "success": True,
            "corrections": [],
            "code": 200,
        })

    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

def apply_manual_corrections(text, correction_map):
    if text in correction_map:
        result = correction_map[text]
    # for old_text, new_text in correction_map.items():
    #     if old_text in text:
    #         text = text.replace(old_text, new_text)
    return result

def gpt_correct_text(prompt):
    token = token_cache.get_token()
    openai.api_key = token
    print("âœ… Token Update SUCCESS")
    
    if not prompt.strip():
        return prompt
    hyogaiKanjiList = []
    
    corrected_text = detect_hyogai_kanji(prompt, hyogaiKanjiList)
    prompt_result = f"""
    You are a professional Japanese text proofreading assistant. Please carefully proofread the following Japanese text and provide corrections in a structured `corrected_map` format.

    **Text to Proofread:**
    - {prompt}

    **Proofreading Requirements:**
    1. **Check for typos and missing characters (èª¤å­—è„±å­—ãŒãªã„ã“ã¨):**
    - Ensure there are no spelling errors or missing characters in the content of the report.
    - If errors are found, add them to the `corrected_map` in the format: "incorrect": "correct".

    2. **Follow the Fund Manager Comment Terminology Guide (ãƒ•ã‚¡ãƒ³ãƒ‰ãƒãƒãƒ¼ã‚¸ãƒ£ã‚³ãƒ¡ãƒ³ãƒˆç”¨èªé›†ã«æ²¿ã£ãŸè¨˜è¼‰ã¨ãªã£ã¦ã„ã‚‹ã“ã¨):**
    - **Consistent Terminology (è¡¨è¨˜ã®çµ±ä¸€):**
        - Ensure the writing format of terms is consistent throughout the report.
    - **Prohibited Words and Phrases (ç¦æ­¢ï¼ˆNGï¼‰ãƒ¯ãƒ¼ãƒ‰åŠã³æ–‡ç« ã®æ³¨æ„äº‹é …):**
        - Check if any prohibited words or phrases are used in the report and correct them as per the guidelines.
    - **Replaceable and Recommended Terms/Expressions (ç½®ãæ›ãˆãŒå¿…è¦ãªç”¨èª/è¡¨ç¾ã€ç½®ãæ›ãˆã‚’æ¨å¥¨ã™ã‚‹ç”¨èª/è¡¨ç¾):**
        - If you find terms or expressions that need to be replaced, revise them according to the provided rules.
    - **Use of Hiragana (ã²ã‚‰ãŒãªã‚’è¡¨è¨˜ã™ã‚‹ã‚‚ã®):**
        - Ensure the report follows the rules for hiragana notation, replacing content that does not conform to commonly used kanji.
    - **Kana Notation for Non-Standard Kanji (ä¸€éƒ¨ã‹ãªæ›¸ãç­‰ã§è¡¨è¨˜ã™ã‚‹ã‚‚ã®):**
        - Ensure non-standard kanji are replaced with kana as the standard writing format.
    - **Correct Usage of Okurigana (ä¸€èˆ¬çš„ãªé€ã‚Šä»®åãªã©):**
        - Ensure the correct usage of okurigana is applied.
    - **English Abbreviations, Loanwords, and Technical Terms (è‹±ç•¥èªã€å¤–æ¥èªã€å°‚é–€ç”¨èªãªã©):**
        - Check if English abbreviations, loanwords, and technical terms are expressed correctly.
    - **Identify and mark any å¸¸ç”¨å¤–æ¼¢å­— (HyÅgai kanji):**
        - Identify and mark any å¸¸ç”¨å¤–æ¼¢å­— (HyÅgai kanji) in the following text.
        - å¸¸ç”¨å¤–æ¼¢å­— refers to Chinese characters that are not included in the [å¸¸ç”¨æ¼¢å­—è¡¨ (JÅyÅ kanji list)](https://ja.wikipedia.org/wiki/å¸¸ç”¨æ¼¢å­—), the official list of commonly used kanji in Japan.
        - For any å¸¸ç”¨å¤–æ¼¢å­— identified, mark the character with (å¸¸ç”¨å¤–æ¼¢å­—) next to it.

        1. å…¥åŠ›ã•ã‚ŒãŸå…¨æ–‡ï¼ˆReport Content to Proofreadï¼‰ã‚’ **ä¸€æ–‡å­—ãšã¤** èµ°æŸ»ã—ã¦ãã ã•ã„ï¼ˆå˜èªå˜ä½ã§ã¯ãªãæ–‡å­—å˜ä½ã®ç…§åˆã§ã™ï¼‰ã€‚
        2. å„æ–‡å­—ã‚’ã€æŒ‡å®šã•ã‚ŒãŸ hyogaiKanjiList ã®æ–‡å­—ã¨ **å®Œå…¨ä¸€è‡´** ã§æ¯”è¼ƒã—ã¦ãã ã•ã„ã€‚
        3. ä¸€è‡´ã™ã‚‹æ–‡å­—ãŒã‚ã‚‹å ´åˆã€ãã®æ–‡å­—ã‚’ã€Œå¸¸ç”¨å¤–æ¼¢å­—ã€ã¨ã—ã¦æ¤œå‡ºã—ã¦ãã ã•ã„ã€‚
        4. ä¸€è‡´ã—ãªã„æ–‡å­—ã¯ã€å¸¸ç”¨æ¼¢å­—ã¨ã—ã¦ç„¡è¦–ã—ã¦ãã ã•ã„ï¼ˆèª¤æ¤œå‡ºã‚’é¿ã‘ã‚‹ãŸã‚ï¼‰ã€‚
        5. æ¤œå‡ºã•ã‚ŒãŸå¸¸ç”¨å¤–æ¼¢å­—ã¯ã€ä»¥ä¸‹ã®ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§æ³¨é‡ˆã‚’ã¤ã‘ã¦è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚

        ---

        ã€æ³¨é‡ˆãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã€‘

        æ¬¡ã®ã‚ˆã†ã«ã€å…ƒã®æ¼¢å­—ã« <s> ã‚¿ã‚°ã¨èƒŒæ™¯è‰²ã‚’ä»˜ã‘ã€èª­ã¿ã¾ãŸã¯ä»£æ›¿èªã‚’èµ¤å­—ã§ç¤ºã—ã€ãã®å¾Œã«ç†ç”±ã‚’æ·»ãˆã¦ãã ã•ã„ã€‚

        ä¾‹:
        <span style="color:red;">ãœã„</span> (<span>ä¿®æ­£ç†ç”±: å¸¸ç”¨å¤–æ¼¢å­—ã®ä½¿ç”¨ <s style="background:yellow;color:red">è„†</s> â†’ ãœã„</span>)

        ---

        ã€Report Content to Proofreadã€‘:
        {corrected_text}

        **1: Typographical Errors (è„±å­—ãƒ»èª¤å­—) Detection**
            -Detect any missing characters (è„±å­—) or misused characters (èª¤å­—) that cause unnatural expressions or misinterpretation.

            **Proofreading Requirements**:
            - Detect and correct all genuine missing characters (è„±å­—) or misused characters (èª¤å­—) that cause grammatical errors or change the intended meaning.
            - Always detect and correct any incorrect conjugations, misused readings, or wrong kanji/verb usage, even if they superficially look natural.
            - Do not point out stylistic variations, natural auxiliary expressions, or acceptable conjugations unless they are grammatically incorrect.
            - Confirm that each kanji matches the intended meaning precisely.
            - Detect cases where non-verb terms are incorrectly used as if they were verbs.

            - â€ã¨â€ã‚’è„±å­—ã—ã¾ã—ãŸ:
                -Example:
                input: æœˆé–“ã§ã¯ã»ã¼å¤‰ã‚ã‚‰ãšãªã‚Šã¾ã—ãŸã€‚
                output:
                <span style="color:red;">æœˆé–“ã§ã¯ã»ã¼å¤‰ã‚ã‚‰ãšãªã‚Šã¾ã—ãŸã€‚</span> (<span>ä¿®æ­£ç†ç”±: èª¤å­— <s style="background:yellow;color:red">æœˆé–“ã§ã¯ã»ã¼å¤‰ã‚ã‚‰ãšãªã‚Šã¾ã—ãŸã€‚</s> â†’ æœˆé–“ã§ã¯ã»ã¼å¤‰ã‚ã‚‰ãšã¨ãªã‚Šã¾ã—ãŸã€‚</span>)
            - The kanji 'å‰¤' was incorrectly used instead of 'æ¸ˆ', resulting in a wrong word formation.
                -Example:
                input: çµŒå‰¤æˆé•·
                output:
                <span style="color:red;">çµŒæ¸ˆæˆé•·</span> (<span>ä¿®æ­£ç†ç”±: èª¤å­— <s style="background:yellow;color:red">çµŒå‰¤æˆé•·</s> â†’ çµŒæ¸ˆæˆé•·</span>)
            - The verb "éŠã¶" was incorrectly conjugated into a non-existent form "ã‚ãã¼ã‚Œã‚‹" instead of the correct passive form "ã‚ãã°ã‚Œã‚‹".
                -Example:
                input: ã‚ãã¼ã‚Œã¾ã™ã‹ã€‚
                output:
                <span style="color:red;">ã‚ãã°ã‚Œã¾ã™ã‹ã€‚</span> (<span>ä¿®æ­£ç†ç”±: å‹•è©æ´»ç”¨ã®èª¤ã‚Š <s style="background:yellow;color:red">ã‚ãã¼ã‚Œã¾ã™ã‹ã€‚</s> â†’ ã‚ãã°ã‚Œã¾ã™ã‹ã€‚</span>)
            - "ã¨"ã‚’çœç•¥ã—ãŸã‚‰ã€ã€Œã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã€ã¯åè©ã§ã‚ã‚Šã€å‹•è©ã®ã‚ˆã†ã«ã€Œã€œã—ãŸã€ã¨æ´»ç”¨ã™ã‚‹ã®ã¯æ–‡æ³•çš„ã«èª¤ã‚Šã§ã™ã€‚
                -Example:
                input: ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆï¼ˆå‚è€ƒæŒ‡æ•°ã¨æ¯”ã¹ä½ã‚ã®æŠ•è³‡æ¯”ç‡ï¼‰ã—ãŸ
                output:
                <span style="color:red;">ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆï¼ˆå‚è€ƒæŒ‡æ•°ã¨æ¯”ã¹ä½ã‚ã®æŠ•è³‡æ¯”ç‡ï¼‰ã¨ã—ãŸ</span> (<span>ä¿®æ­£ç†ç”±: å‹•è©æ´»ç”¨ã®èª¤ã‚Š <s style="background:yellow;color:red">ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆï¼ˆå‚è€ƒæŒ‡æ•°ã¨æ¯”ã¹ä½ã‚ã®æŠ•è³‡æ¯”ç‡ï¼‰ã—ãŸ</s> â†’ ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆï¼ˆå‚è€ƒæŒ‡æ•°ã¨æ¯”ã¹ä½ã‚ã®æŠ•è³‡æ¯”ç‡ï¼‰ã¨ã—ãŸ</span>)


            **correct Example*:
            - "å–ã‚Šçµ„ã¿ã—"ã¯è‡ªç„¶ãªé€£ç”¨å½¢è¡¨ç¾ã®ãŸã‚ã€ä¿®æ­£ä¸è¦'
                -Example:
                input: ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§äº‹æ¥­ã‚’å±•é–‹ã™ã‚‹
                output:
                <span style="color:red;">ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«äº‹æ¥­ã‚’å±•é–‹ã™ã‚‹</span> (<span>ä¿®æ­£ç†ç”±: æ ¼åŠ©è©ã®èª¤ç”¨ <s style="background:yellow;color:red">ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§äº‹æ¥­ã‚’å±•é–‹ã™ã‚‹</s> â†’ ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«äº‹æ¥­ã‚’å±•é–‹ã™ã‚‹</span>)

        **2: Punctuation (å¥èª­ç‚¹) Usage Check**
            -Detect missing, excessive, or incorrect use of punctuation marks (ã€ã€‚).

            **Proofreading Requirements**:
            -Ensure sentences correctly end withã€Œã€‚ã€where appropriate.
            -Avoid redundant commasã€Œã€ã€in unnatural positions.
            -Maintain standard business writing style.

            -Example:
            input: åç›Šè¦‹é€šã—ãŒæœŸå¾…ã§ãã‚‹ä¼æ¥­ã‚’ä¸­å¿ƒã«æŠ•è³‡ã‚’è¡Œãªã†æ–¹é‡ã§ã™
            output:
            <span style="color:red;">åç›Šè¦‹é€šã—ãŒæœŸå¾…ã§ãã‚‹ä¼æ¥­ã‚’ä¸­å¿ƒã«æŠ•è³‡ã‚’è¡Œãªã†æ–¹é‡ã§ã™</span> (<span>ä¿®æ­£ç†ç”±: æ–‡æœ«å¥ç‚¹ã®æ¬ å¦‚ <s style="background:yellow;color:red"åç›Šè¦‹é€šã—ãŒæœŸå¾…ã§ãã‚‹ä¼æ¥­ã‚’ä¸­å¿ƒã«æŠ•è³‡ã‚’è¡Œãªã†æ–¹é‡ã§ã™</s> â†’ åç›Šè¦‹é€šã—ãŒæœŸå¾…ã§ãã‚‹ä¼æ¥­ã‚’ä¸­å¿ƒã«æŠ•è³‡ã‚’è¡Œãªã†æ–¹é‡ã§ã™ã€‚</span>)

        **3: Unnatural Spaces (ä¸è‡ªç„¶ãªç©ºç™½) Detection**
            -Detect unnecessary half-width or full-width spaces within sentences.

            **Proofreading Requirements**:
            -Remove any redundant spaces between words or inside terms.
            -Confirm that spacing follows standard Japanese document conventions.

            -Example:
            input: é€ é…é›»è¨­å‚™
            output:
            <span style="color:red;">é€é…é›»è¨­å‚™</span> (<span>ä¿®æ­£ç†ç”±: ä¸è¦ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤ <s style="background:yellow;color:red">é€ é…é›»è¨­å‚™</s> â†’ é€é…é›»è¨­å‚™</span>)

            -Example:
            input: ãƒã‚¤ã‚¯ãƒ­ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚„ é–¢é€£ã®è¤‡åˆä¿¡å·è£½å“
            output:
            <span style="color:red;">ãƒã‚¤ã‚¯ãƒ­ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚„é–¢é€£ã®è¤‡åˆä¿¡å·è£½å“</span> (<span>ä¿®æ­£ç†ç”±: ä¸è¦ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤ <s style="background:yellow;color:red">ãƒã‚¤ã‚¯ãƒ­ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚„ é–¢é€£ã®è¤‡åˆä¿¡å·è£½å“</s> â†’ ãƒã‚¤ã‚¯ãƒ­ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚„é–¢é€£ã®è¤‡åˆä¿¡å·è£½å“</span>)


        **4: Omission or Misuse of Particles (åŠ©è©ã®çœç•¥ãƒ»èª¤ç”¨) Detection**
            - Detect omissions and misuses of grammatical particles (åŠ©è©), especiallyã€Œã®ã€ã€Œã‚’ã€ã€Œã«ã€, that lead to structurally incorrect or unnatural expressions.

            **Proofreading Requirements**:

            - Carefully examine whether all necessary particlesâ€”particularlyã€Œã®ã€ã€Œã‚’ã€ã€Œã«ã€â€”are correctly used in every sentence.
            - Do not tolerate the omission of any structurally required particle, even if the sentence appears understandable or natural overall.
            - Focus on grammatical correctness, not perceived readability.
            - In long texts, perform sentence-by-sentence proofreading to ensure no required particle is missing at any position.
            - If a particle should be present according to standard Japanese grammar but is omitted, it must be explicitly identified and corrected.

            -Example:
            input: æ¬§å·ãªã©å¸‚å ´èª¿æŸ»é–‹å§‹ã—ã¦
            output:
            <span style="color:red;">æ¬§å·ãªã©ã®å¸‚å ´èª¿æŸ»ã‚’é–‹å§‹ã—ã¦</span> (<span>ä¿®æ­£ç†ç”±: é€£ä½“ä¿®é£¾ã®åŠ©è©çœç•¥ <s style="background:yellow;color:red">æ¬§å·ãªã©å¸‚å ´èª¿æŸ»é–‹å§‹ã—ã¦</s> â†’ æ¬§å·ãªã©ã®å¸‚å ´èª¿æŸ»ã‚’é–‹å§‹ã—ã¦</span>)

            -Example:
            input: ECBï¼ˆæ¬§å·ä¸­å¤®éŠ€è¡Œï¼‰ãªã©æµ·å¤–ä¸»è¦ä¸­éŠ€ã«ã‚ˆã‚‹
            output:
            <span style="color:red;">ECBï¼ˆæ¬§å·ä¸­å¤®éŠ€è¡Œï¼‰ãªã©ã®æµ·å¤–ä¸»è¦ä¸­éŠ€ã«ã‚ˆã‚‹</span> (<span>ä¿®æ­£ç†ç”±: æ‰€æœ‰æ ¼åŠ©è©ã€Œã®ã€ã®çœç•¥ <s style="background:yellow;color:red">ECBï¼ˆæ¬§å·ä¸­å¤®éŠ€è¡Œï¼‰ãªã©æµ·å¤–ä¸»è¦ä¸­éŠ€ã«ã‚ˆã‚‹</s> â†’ ECBï¼ˆæ¬§å·ä¸­å¤®éŠ€è¡Œï¼‰ãªã©ã®æµ·å¤–ä¸»è¦ä¸­éŠ€ã«ã‚ˆã‚‹</span>)

            -Example:
            input: 5000å„„å††
            output:
            <span style="color:red;">5,000å„„å††</span> (<span>ä¿®æ­£ç†ç”±: é‡‘é¡ã‚«ãƒ³ãƒåŒºåˆ‡ã‚Š <s style="background:yellow;color:red">5000å„„å††</s> â†’ 5,000å„„å††</span>)

        **5: Monetary Unit & Number Format (é‡‘é¡è¡¨è¨˜ãƒ»æ•°å€¤ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ) Check**

            -Detect mistakes in number formatting, especially monetary values.
            -Proofreading Requirements:
            -Apply comma separator every three digits for numbers over 1,000.
            -Ensure currency units (å††ã€å…†å††ã€å„„å††) are correctly used.
            -Standardize half-width characters where needed.

            -Example:
            input: å¯¾å¿œã«ã¯æ–°ãŸãªæŠ€è¡“é–‹ç™ºã‚„åˆ¶åº¦æ”¹é©ã®å¿…è¦æ€§ãŒæŒ‡æ‘˜ã•ã‚Œã¦ã„ã¾ã™ã€‚
            output:
            <span style="color:red;">å¯¾å¿œã¯æ–°ãŸãªæŠ€è¡“é–‹ç™ºã‚„åˆ¶åº¦æ”¹é©ã®å¿…è¦æ€§ãŒæŒ‡æ‘˜ã•ã‚Œã¦ã„ã¾ã™ã€‚</span> (<span>ä¿®æ­£ç†ç”±: æ ¼åŠ©è©ã€Œã«ã¯ã€ã®èª¤ç”¨ <s style="background:yellow;color:red">å¯¾å¿œã«ã¯æ–°ãŸãªæŠ€è¡“é–‹ç™ºã‚„åˆ¶åº¦æ”¹é©ã®å¿…è¦æ€§ãŒæŒ‡æ‘˜ã•ã‚Œã¦ã„ã¾ã™ã€‚</s> â†’ å¯¾å¿œã¯æ–°ãŸãªæŠ€è¡“é–‹ç™ºã‚„åˆ¶åº¦æ”¹é©ã®å¿…è¦æ€§ãŒæŒ‡æ‘˜ã•ã‚Œã¦ã„ã¾ã™ã€‚</span>)


            **Special Instructions**:
            - Always annotate all detected HyÅgai Kanji.
            - Never replace or modify the character unless explicitly instructed.

        **6: Detection of Misused Enumerative Particleã€Œã‚„ã€**
            **Proofreading Targets**:
            - Detect inappropriate use of the enumerative particleã€Œã‚„ã€when it connects elements with different grammatical structures.
            - The particleã€Œã‚„ã€must only be used to list **nouns or noun phrases** that are grammatically equivalent.
            - If the item followingã€Œã‚„ã€is a **verb phrase**, **adverbial clause**, or a structurally different element, thenã€Œã‚„ã€is incorrect.
            - In such cases, replaceã€Œã‚„ã€with a commaã€Œã€ã€to properly separate clauses or adjust the sentence structure.


        **Special Rules:**
        1. **Do not modify proper nouns (e.g., names of people, places, or organizations) unless they are clearly misspelled.**
            -Exsample:
            ãƒ™ãƒƒã‚»ãƒ³ãƒˆæ°: Since this is correct and not a misspelling, it will not be modified.
        2. **Remove unnecessary or redundant text instead of replacing it with other characters.**
            -Exsample:
            ãƒ¦ãƒ¼ãƒ­åœåŸŸå†…ã®æ™¯æ°—å§”: Only the redundant character å§” will be removed, and no additional characters like ã® will be added. The corrected text will be: ãƒ¦ãƒ¼ãƒ­åœåŸŸå†…ã®æ™¯æ°—.
        3. **Preserve spaces between words in the original text unless they are at the beginning or end of the text.**
            -Example:
            input: æœˆã® å‰åŠã¯ç±³å›½ã® å‚µåˆ¸åˆ©å›ã‚Šã®ä¸Šæ˜‡ ã«ã¤ã‚Œã¦
            Output: æœˆã® å‰åŠã¯ç±³å›½ã® å‚µåˆ¸åˆ©å›ã‚Šã®ä¸Šæ˜‡ ã«ã¤ã‚Œã¦ (spaces between words are preserved).
        - **Task**: Header Date Format Validation & Correction  
        - **Target Area**: Date notation in parentheses following "ä»Šå¾Œé‹ç”¨æ–¹é‡ (Future Policy Decision Basis)"  
        ---
        ### Validation Requirements  
        1. **Full Format Compliance Check**:  
        - Must follow "YYYYå¹´MMæœˆDDæ—¥ç¾åœ¨" (Year-Month-Day as of)  
        - **Year**: 4-digit number (e.g., 2024)  
        - **Month**: 2-digit (01-12, e.g., April â†’ 04)  
        - **Day**: 2-digit (01-31, e.g., 5th â†’ 05)  
        - **Suffix**: Must end with "ç¾åœ¨" (as of)  

        2. **Common Error Pattern Detection**:  
        âŒ "1æœˆ0æ—¥" â†’ Missing month leading zero + invalid day 0  
        âŒ "2024å¹´4æœˆ1æ—¥" â†’ Missing month leading zero (should be 04)  
        âŒ "2024å¹´12æœˆ" â†’ Missing day value  
        âŒ "2024-04-05ç¾åœ¨" â†’ Incorrect separator usage (hyphen/slash)  
        ---
        ### Correction Protocol  
        1. **Leading Zero Enforcement**  
        - Add leading zeros to single-digit months/days (4æœˆ â†’ 04æœˆ, 5æ—¥ â†’ 05æ—¥)  

        2. **Day 0 Handling**  
        - Replace day 0 with YYYYMMDD Date Format  
        - Example: 2024å¹´4æœˆ0æ—¥ â†’ 2024å¹´04æœˆ00æ—¥

        3. **Separator Standardization**  
        - Convert hyphens/slashes to CJK characters:  
            `2024/04/05` â†’ `2024å¹´04æœˆ05æ—¥`  


        4. **Consistency with Report Data Section (ãƒ¬ãƒãƒ¼ãƒˆã®ãƒ‡ãƒ¼ã‚¿éƒ¨ã¨ã®æ•´åˆæ€§ç¢ºèª):**
        - Ensure the textual description in the report is completely consistent with the data section, without any logical or content-related discrepancies.

        5. **Eliminate language fluency(å˜èªé–“ã®ä¸è¦ãªã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤):**
        - Ensure that there are no extra spaces.
            -Example:
            input:æ™¯æ°—æµ®æšãŒæ„ è­˜ã•ã‚ŒãŸã“ã¨ã§
            output:æ™¯æ°—æµ®æšãŒæ„è­˜ã•ã‚ŒãŸã“ã¨ã§
        
        6.  **Layout and Formatting Rules (ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã«é–¢ã™ã‚‹çµ±ä¸€):**
            - **æ–‡é ­ã®ã€Œâ—‹ã€å°ã¨ä¸€æ–‡å­—ç›®ã®é–“éš”ã‚’çµ±ä¸€:**
                - When a sentence begins with the "â—‹" symbol, ensure the spacing between the symbol and the first character is consistent across the document.
            - **æ–‡ç« ã®é–“éš”ã®çµ±ä¸€:**
                - If a sentence begins with "â—‹", ensure that the spacing within the frame remains consistent.
            - **ä¸Šä½10éŠ˜æŸ„ ã‚³ãƒ¡ãƒ³ãƒˆæ¬„ã«ã¤ã„ã¦ã€æ å†…ã«é©åˆ‡ã«åã¾ã£ã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯:**
                - If the stock commentary contains a large amount of text, confirm whether it fits within the designated frame. 
                - If the ranking changes in the following month, adjust the frame accordingly.
                - **Check point**
                    1. **æ–‡å­—æ•°åˆ¶é™å†…ã«åã¾ã£ã¦ã„ã‚‹ã‹ï¼Ÿ**
                    - 1æ ã‚ãŸã‚Šã®æœ€å¤§æ–‡å­—æ•°ã‚’è¶…ãˆã¦ã„ãªã„ã‹ï¼Ÿ
                    - é©åˆ‡ãªè¡Œæ•°ã§åã¾ã£ã¦ã„ã‚‹ã‹ï¼Ÿ

                    2. **æ¬¡æœˆã®é †ä½å¤‰å‹•ã«ä¼´ã†æ èª¿æ•´ã®å¿…è¦æ€§**
                    - é †ä½ãŒå¤‰æ›´ã•ã‚Œã‚‹ã¨æ èª¿æ•´ãŒå¿…è¦ãªãŸã‚ã€èª¿æ•´ãŒå¿…è¦ãªç®‡æ‰€ã‚’ç‰¹å®š

                    3. **æ å†…ã«åã¾ã‚‰ãªã„å ´åˆã®ä¿®æ­£ææ¡ˆ**
                    - å¿…è¦ã«å¿œã˜ã¦ã€çŸ­ç¸®è¡¨ç¾ã‚„ä¸è¦ãªæƒ…å ±ã®å‰Šé™¤ã‚’ææ¡ˆ
                    - é‡è¦ãªæƒ…å ±ã‚’æãªã‚ãšã«é©åˆ‡ã«ãƒªãƒ©ã‚¤ãƒˆ

                    output Format:
                    - **ã‚³ãƒ¡ãƒ³ãƒˆã®æ è¶…éãƒã‚§ãƒƒã‚¯**
                    - (æ è¶…éã—ã¦ã„ã‚‹ã‹: ã¯ã„ / ã„ã„ãˆ)
                    - (è¶…éã—ã¦ã„ã‚‹å ´åˆã€ã‚ªãƒ¼ãƒãƒ¼ã—ãŸæ–‡å­—æ•°)

                    - **é †ä½å¤‰å‹•ã«ã‚ˆã‚‹æ èª¿æ•´ã®å¿…è¦æ€§**
                    - (èª¿æ•´ãŒå¿…è¦ãªã‚³ãƒ¡ãƒ³ãƒˆãƒªã‚¹ãƒˆ)

                    - **ä¿®æ­£ææ¡ˆ**
                    - (æ å†…ã«åã‚ã‚‹ãŸã‚ã®ä¿®æ­£å¾Œã®ã‚³ãƒ¡ãƒ³ãƒˆ)

            **Standardized Notation (è¡¨è¨˜ã®çµ±ä¸€):**
            - **åŸºæº–ä¾¡é¡ã®é¨°è½ç‡:**
                - When there are three decimal places, round off using the round-half-up method to the second decimal place. If there are only two decimal places, keep the value unchanged.

            - **ï¼…ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼‰ã€ã‚«ã‚¿ã‚«ãƒŠ:**
                - **åŠè§’ã‚«ã‚¿ã‚«ãƒŠ â†’ å…¨è§’ã‚«ã‚¿ã‚«ãƒŠ**ï¼ˆä¾‹:ã€Œï½¶ï¾€ï½¶ï¾…ã€â†’ã€Œã‚«ã‚¿ã‚«ãƒŠã€ï¼‰
                - **åŠè§’è¨˜å· â†’ å…¨è§’è¨˜å·**ï¼ˆä¾‹:ã€Œ%ã€â†’ã€Œï¼…ã€ã€ã€Œ@ã€â†’ã€Œï¼ ã€ï¼‰
                    Example:
                        input: % ï½¶ï¾€ï½¶ï¾… 
                        output: ï¼… ã‚«ã‚¿ã‚«ãƒŠ

            - **æ•°å­—ã€ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã€ã€Œï¼‹ã€ãƒ»ã€Œï¼ã€:**
                - **å…¨è§’æ•°å­—ãƒ»ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ â†’ åŠè§’æ•°å­—ãƒ»ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆ**ï¼ˆä¾‹:ã€Œï¼‘ï¼’ï¼“ã€â†’ã€Œ123ã€ã€ã€Œï¼¡ï¼¢ï¼£ã€â†’ã€ŒABCã€ï¼‰
                - **å…¨è§’ã€Œï¼‹ã€ã€Œï¼ã€ â†’ åŠè§’ã€Œ+ã€ã€Œ-ã€**ï¼ˆä¾‹:ã€Œï¼‹ï¼ã€â†’ã€Œ+-ã€
                    Example:
                        input: ï¼‘ï¼’ï¼“ ï¼¡ï¼¢ï¼£ ï½±ï½²ï½³ ï¼‹ï¼
                        output: 123 ABC ã‚¢ã‚¤ã‚¦ +-

            - **ã‚¹ãƒšãƒ¼ã‚¹ã¯å¤‰æ›´ãªã—**  

            - **ã€Œâ€»ã€ã®ä½¿ç”¨:**
                - ã€Œâ€»ã€ã¯å¯èƒ½ã§ã‚ã‚Œã° **ä¸Šä»˜ãæ–‡å­—ï¼ˆsuperscriptï¼‰â€»** ã«å¤‰æ›ã—ã¦ãã ã•ã„ã€‚
                - å‡ºåŠ›å½¢å¼ã®ä¾‹:
                - ã€Œé‡è¦äº‹é …â€»ã€ â†’ ã€Œé‡è¦äº‹é …<sup>â€»</sup>ã€

            - **ï¼ˆã‚«ãƒƒã‚³æ›¸ãï¼‰:**
                - Parenthetical notes should only be included in their first occurrence in a comment.
                ä»¥ä¸‹ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã«ãŠã„ã¦ã€ã‚«ãƒƒã‚³æ›¸ãï¼ˆbracket "ï¼ˆ ï¼‰"ï¼‰ãŒé©åˆ‡ã«ä½¿ç”¨ã•ã‚Œã¦ã„ã‚‹ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚

                **Check point**
                    1. **ã‚«ãƒƒã‚³æ›¸ãã¯ã€ã‚³ãƒ¡ãƒ³ãƒˆã®åˆå‡ºã®ã¿ã«è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‹ï¼Ÿ**
                    - åŒã˜ã‚«ãƒƒã‚³æ›¸ããŒ2å›ä»¥ä¸Šç™»å ´ã—ã¦ã„ãªã„ã‹ï¼Ÿ
                    - åˆå‡ºãƒšãƒ¼ã‚¸ä»¥é™ã®ã‚³ãƒ¡ãƒ³ãƒˆã«ã‚«ãƒƒã‚³æ›¸ããŒé‡è¤‡ã—ã¦è¨˜è¼‰ã•ã‚Œã¦ã„ãªã„ã‹ï¼Ÿ

                    2. **ãƒ‡ã‚£ã‚¹ã‚¯ãƒ­ã®ãƒšãƒ¼ã‚¸ç•ªå·é †ã«å¾“ã£ã¦ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨**
                    - ã‚·ãƒ¼ãƒˆã®é †ç•ªã§ã¯ãªãã€å®Ÿéš›ã®ãƒšãƒ¼ã‚¸ç•ªå·ã‚’åŸºæº–ã«ã™ã‚‹ã€‚

                    3. **ä¾‹å¤–å‡¦ç†**
                    - ã€Œä¸€éƒ¨ä¾‹å¤–ãƒ•ã‚¡ãƒ³ãƒ‰ã‚ã‚Šã€ã¨ã‚ã‚‹ãŸã‚ã€ä¾‹å¤–çš„ã«ã‚«ãƒƒã‚³æ›¸ããŒè¤‡æ•°å›ç™»å ´ã™ã‚‹ã‚±ãƒ¼ã‚¹ã‚’è€ƒæ…®ã™ã‚‹ã€‚
                    - ä¾‹å¤–ã¨ã—ã¦èªã‚ã‚‰ã‚Œã‚‹ã‚±ãƒ¼ã‚¹ã‚’åˆ¤æ–­ã—ã€é©åˆ‡ã«æŒ‡æ‘˜ã€‚

                    output Format:
                    - **ã‚«ãƒƒã‚³æ›¸ãã®åˆå‡ºãƒªã‚¹ãƒˆ**ï¼ˆã©ã®ãƒšãƒ¼ã‚¸ã«æœ€åˆã«ç™»å ´ã—ãŸã‹ï¼‰
                    - **é‡è¤‡ãƒã‚§ãƒƒã‚¯çµæœ**ï¼ˆã©ã®ãƒšãƒ¼ã‚¸ã§äºŒé‡è¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹ã‹ï¼‰
                    - **ä¿®æ­£ææ¡ˆ**ï¼ˆã©ã®ãƒšãƒ¼ã‚¸ã®ã‚«ãƒƒã‚³æ›¸ãã‚’å‰Šé™¤ã™ã¹ãã‹ï¼‰
                    - **ä¾‹å¤–ãƒ•ã‚¡ãƒ³ãƒ‰ãŒé©ç”¨ã•ã‚Œã‚‹å ´åˆã€è£œè¶³æƒ…å ±**

            - **ä¼šè¨ˆæœŸé–“ã®è¡¨è¨˜:**
                - The use of "ï½" is prohibited; always use "-".
                - Example: 6ï½8æœˆæœŸï¼ˆÃ—ï¼‰ â†’ 6-8æœˆæœŸï¼ˆâ—‹ï¼‰
            - **å¹´åº¦è¡¨è¨˜:**
                - Use four-digit notation for years.
                - Make modifications directly in this article and explain the reasons for the modifications.
                - Example: 22å¹´ï¼ˆÃ—ï¼‰ â†’ 2022å¹´ï¼ˆâ—‹ï¼‰
            - **ãƒ¬ãƒ³ã‚¸ã®è¡¨è¨˜:**
                - Always append "%" when indicating a range.
                - Make modifications directly in this article and explain the reasons for the modifications.
                - Example: -1ï½0.5%ï¼ˆÃ—ï¼‰ â†’ -1%ï½0.5%ï¼ˆâ—‹ï¼‰
            - **æŠ•è³‡ç’°å¢ƒã®è¨˜è¿°:**
                **ã€Œå…ˆæœˆã®æŠ•è³‡ç’°å¢ƒã€**ã®éƒ¨åˆ†ã§ã€Œå…ˆæœˆæœ«ã€ã®è¨˜è¿°ãŒå«ã¾ã‚Œã‚‹å ´åˆã€ã€Œå‰æœˆæœ«ã€ã«å¤‰æ›´ã—ã¦ãã ã•ã„ã€‚
                Example:
                ä¿®æ­£å‰: å…ˆæœˆæœ«ã®å¸‚å ´å‹•å‘ã‚’åˆ†æã™ã‚‹ã¨â€¦
                ä¿®æ­£å¾Œ: å‰æœˆæœ«ã®å¸‚å ´å‹•å‘ã‚’åˆ†æã™ã‚‹ã¨â€¦

            - **é€šè²¨è¡¨è¨˜ã®çµ±ä¸€:**
                - Standardize currency notation across the document.
                - æ—¥æœ¬å††ã¯ã€ŒJPYã€ã¾ãŸã¯ã€ŒÂ¥ã€ã§çµ±ä¸€ å††.
                - Exsample: 
                input: Â¥100 or JPY 100
                output: 100 å††

            **Preferred and Recommended Terminology (ç½®ãæ›ãˆãŒå¿…è¦ãªç”¨èª/è¡¨ç¾):**
            - **ç¬¬1å››åŠæœŸ:**
                - Ensure the period is clearly stated.
                - Example: 18å¹´ç¬¬4å››åŠæœŸï¼ˆÃ—ï¼‰ â†’ 2018å¹´10-12æœˆæœŸï¼ˆâ—‹ï¼‰
            - **ç´„â—‹ï¼…ç¨‹åº¦:**
                - Do not use "ç´„" (approximately) and "ç¨‹åº¦" (extent) together. Choose either one.
                - Example: ç´„â—‹ï¼…ç¨‹åº¦ï¼ˆÃ—ï¼‰ â†’ ç´„â—‹ï¼… or â—‹ï¼…ç¨‹åº¦ï¼ˆâ—‹ï¼‰
            - **å¤§æ‰‹ä¼æ¥­è¡¨è¨˜ã®æ˜ç¢ºåŒ–**  
            - **ã€Œâ—‹â—‹å¤§æ‰‹ã€** ãŒå«ã¾ã‚Œã‚‹å ´åˆã€æ–‡ä¸­ã® **ä¼šç¤¾åã‚’æŠ½å‡º** ã—ã€  
                **ã€Œå¤§æ‰‹â—‹â—‹ä¼šç¤¾/ä¼æ¥­ã€** ã®å½¢å¼ã«ä¿®æ­£ã™ã‚‹ã€‚  
            - **å…¥åŠ›ä¾‹:**  
                - ã€Œå¤§æ‰‹ãƒ¡ãƒ¼ã‚«ãƒ¼/ä¼šç¤¾/ä¼æ¥­ã€  
                - **å‡ºåŠ›:** ã€Œå¤§æ‰‹ä¸å‹•ç”£ä¼šç¤¾ã€å¤§æ‰‹åŠå°ä½“ãƒ¡ãƒ¼ã‚«ãƒ¼ã€  
            - **The actual company name must be found and converted in the article


        **Special Rules:**
        1. **Do not modify proper nouns (e.g., names of people, places, or organizations) unless they are clearly misspelled.**
            -Exsample:
            ãƒ™ãƒƒã‚»ãƒ³ãƒˆæ°: Since this is correct and not a misspelling, it will not be modified.
        2. **Remove unnecessary or redundant text instead of replacing it with other characters.**
            -Exsample:
            ãƒ¦ãƒ¼ãƒ­åœåŸŸå†…ã®æ™¯æ°—å§”: Only the redundant character å§” will be removed, and no additional characters like ã® will be added. The corrected text will be: ãƒ¦ãƒ¼ãƒ­åœåŸŸå†…ã®æ™¯æ°—.
        3. **Preserve spaces between words in the original text unless they are at the beginning or end of the text.**
            -Example:
            input: æœˆã® å‰åŠã¯ç±³å›½ã® å‚µåˆ¸åˆ©å›ã‚Šã®ä¸Šæ˜‡ ã«ã¤ã‚Œã¦
            Output: æœˆã® å‰åŠã¯ç±³å›½ã® å‚µåˆ¸åˆ©å›ã‚Šã®ä¸Šæ˜‡ ã«ã¤ã‚Œã¦ (spaces between words are preserved).
        ---

        ### **Correction Rules:**
        1. **Only output the corrected_map dictionary. No explanations or extra text.**
        2. **Only incorrect words and their corrected versions should be included.**
        3. **Do not include full sentence corrections.**
        4. **Ensure the corrected_map output is in valid Python dictionary format.**
        5. **Return only the following structure:**
        
        **Output Format:**
        {{
            "incorrect1": "corrected1",
            "incorrect2": "corrected2"
        }}

        """

    
    # ChatCompletion Call
    response = openai.ChatCompletion.create(
        deployment_id=deployment_id,  # Deploy Name
        messages=[
            {"role": "system", "content": "You are a professional Japanese text proofreading assistant."},
            {"role": "user", "content": prompt_result}
        ],
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  #seed
    )

    try:
        answer = response['choices'][0]['message']['content']
        corrected_map = parse_gpt_response(answer)
        
        full_corrected = prompt
        for k, v in corrected_map.items():
            full_corrected = full_corrected.replace(k, v)

        dynamic_corrections = detect_corrections(prompt, full_corrected)
        corrected_map.update(dynamic_corrections)

        return {k: v for k, v in corrected_map.items() if k and v and k != v}

    except Exception as e:
        print(f"req error: {e}")
        return {}

def correct_text_box_in_excel(input_bytes,corrected_map):
    # 1)  in-memory zip
    in_memory_zip = zipfile.ZipFile(io.BytesIO(input_bytes), 'r')
    
    # BytesIO
    output_buffer = io.BytesIO()
    new_zip = zipfile.ZipFile(output_buffer, 'w', zipfile.ZIP_DEFLATED)

    for item in in_memory_zip.infolist():
        file_data = in_memory_zip.read(item.filename)

        # 3) drawingN.xml
        if item.filename.startswith("xl/drawings/drawing") and item.filename.endswith(".xml"):
            try:
                tree = ET.fromstring(file_data)
                ns = {'a': 'http://schemas.openxmlformats.org/drawingml/2006/main'}

                # 4) find <a:t> tag 
                for t_element in tree.findall(".//a:t", ns):
                    original_text = t_element.text
                #----------------------------------------------------------------
                    # if original_text:  # None 
                    #     original_text_gpt = gpt_correct_text(original_text)

                    #     if original_text_gpt and original_text_gpt.strip() in corrected_map:
                    #         t_element.text = corrected_map[original_text_gpt.strip()]
                #----------------------------------------------------------------
                    # resultMap = gpt_correct_text(original_text)

                    if original_text in corrected_map:
                        t_element.text = corrected_map[original_text]
                #----------------------------------------------------------------

                file_data = ET.tostring(tree, encoding='utf-8', standalone=False)
                
            except Exception as e:
                print(f"Warning: Parsing {item.filename} failed - {e}")

        new_zip.writestr(item, file_data)

    in_memory_zip.close()
    new_zip.close()
    output_buffer.seek(0)
    return output_buffer.getvalue()

# Excel read -for debug
@app.route("/api/excel_upload", methods=["POST"])
def excel_upload():
    file = request.files["file"]  # XLSX
    original_bytes = file.read()

    corrected_map = {
        "åœ°æ”¿å­¦ãƒªã‚¹ã‚¯": "åœ°æ”¿å­¦çš„ãƒªã‚¹ã‚¯"
    }

    # 2) ìˆ˜ì •
    modified_bytes = correct_text_box_in_excel(original_bytes, corrected_map)

    # 3) return xlsx
    return send_file(
        io.BytesIO(modified_bytes),
        mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
        as_attachment=True,
        download_name="annotated.xlsx"
    )

# T-STARãƒ˜ãƒ«ãƒ— API --for debug
@app.route('/api/prompt_upload', methods=['POST'])
def prompt_upload():
    try:
        data = request.json
        prompt = data.get("input", "")
        original_text = data.get("original_text", "")

        if not prompt:
            return jsonify({"success": False, "error": "No input provided"}), 400
        
        prompt_result = f"""
        Please analyze the provided {original_text} and generate results based on the specified {prompt}.

        **Requirements**:
        1. **Extract relevant information**:
        - Extract only the information that directly answers the {prompt}.
        2. **Process the content**:
        - Process the extracted information to provide a clear and concise response.
        3. **Output in Japanese**:
        - Provide the results in Japanese, strictly based on the {prompt}.
        - Do not include any unrelated information or additional explanations.

        **Output**:
        - The output must be accurate, concise, and fully aligned with the {prompt}.
        - Only provide the response in Japanese.

        **Example**:
        - If the {prompt} is "å£²ä¸Šæˆé•·ç‡ã‚’æ•™ãˆã¦ãã ã•ã„", the output should be:
        "2023å¹´ã®å£²ä¸Šæˆé•·ç‡ã¯15ï¼…ã§ã™ã€‚"

        Ensure the output is accurate, concise, and aligned with the given {prompt} requirements.
        """

        # ChatCompletion Call
        response = openai.ChatCompletion.create(
            deployment_id=deployment_id,  # Deploy Name
            messages=[
                {"role": "system", "content": "You are a professional Japanese text proofreading assistant."},
                {"role": "user", "content": prompt_result}
            ],
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE,
            seed=SEED  # seed
        )
        answer = response['choices'][0]['message']['content'].strip()
        re_answer = remove_code_blocks(answer)

        # return JSON
        return jsonify({
            "success": True,
            "original_text": prompt,
            "corrected_text": re_answer,
            # "corrections": corrections
        })

    except ValueError as e:
        return jsonify({"success": False, "error": str(e)}), 400
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

#------Auto app update API

@app.route('/api/auto_save_cosmos', methods=['POST'])
def auto_save_cosmos():
    try:
        data = request.json
        response_data = data['result']
        link_url = data['link']
        container_name = data['containerName']
        file_name_decoding = data['fileName']

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        container = get_db_connection(container_name)

        item = {
            'id': file_name,
            'fileName': file_name,
            'result': response_data,
            'link': link_url,
            'updateTime': datetime.utcnow().isoformat(),
        }

        existing_item = list(container.query_items(
            query="SELECT * FROM c WHERE c.id = @id",
            parameters=[{"name": "@id", "value": file_name}],
            enable_cross_partition_query=True
        ))

        if not existing_item:
            container.create_item(body=item)
            logging.info(f"âœ… Cosmos DB Update Success: {file_name}")
        else:
            existing_id = existing_item[0]['id']
            item['id'] = existing_id
            container.replace_item(item=existing_item[0], body=item)
            logging.info(f"ğŸ”„ Cosmos DB update success: {file_name}")

        return jsonify({"success": True, "message": "Data Update Success"}), 200

    except CosmosHttpResponseError as e:
        logging.error(f"âŒ Cosmos DB Save error: {e}")
        return jsonify({"success": False, "message": str(e)}), 500
    except Exception as e:
        logging.error(f"âŒ API Save error: {e}")
        return jsonify({"success": False, "message": str(e)}), 500
    
#----auto app save to blob
@app.route('/api/auto_save_blob', methods=['POST'])
def auto_save_blob():
    try:
        if 'file' not in request.files:
            return jsonify({"success": False, "message": "no find file."}), 400

        file = request.files['file']
        blob_name = file.filename
        
        container_client = get_storage_container()

        blob_client = container_client.get_blob_client(blob_name)

        blob_client.upload_blob(file, overwrite=True)

        file_url = blob_client.url
        logging.info(f"âœ… Azure Blob Storage Update Success: {blob_name}")

        return jsonify({"success": True, "url": file_url}), 200

    except Exception as e:
        logging.error(f"âŒ Azure Blob Storage update error: {e}")
        return jsonify({"success": False, "message": str(e)}), 500

#----auto app save log 
@app.route('/api/auto_save_log_cosmos', methods=['POST','PUT'])
def auto_save_log_cosmos():
    """log Cosmos DB Save to API"""
    try:
        container = get_db_connection(APPLOG_CONTAINER_NAME)

        log_data = request.json
        log_by_date = log_data.get("logs", {})

        # âœ… Cosmos DB Save
        for log_id, logs in log_by_date.items():
            existing_logs = list(container.query_items(
                query="SELECT * FROM c WHERE c.id = @log_id",
                parameters=[{"name": "@log_id", "value": log_id}],
                enable_cross_partition_query=True
            ))

            if existing_logs:
                existing_log = existing_logs[0]
                existing_log["logEntries"].extend(logs)
                existing_log["timestamp"] = datetime.utcnow().isoformat(), 
                #update
                container.replace_item(item=existing_log["id"], body=existing_log)
                logging.info(f"ğŸ”„ SUCCESS: Update Log Success: {log_id}")
            else:
                log_data = {
                    "id": log_id,  # YYYYMMDD format ID
                    "logEntries": logs,
                    "timestamp": datetime.utcnow().isoformat(),
                }
                #create
                container.create_item(body=log_data)
                logging.info(f"âœ… SUCCESS: Save to Log Success: {log_id}")

        return jsonify({"code": 200, "message": "Logs saved successfully."}), 200

    except Exception as e:
        logging.error(f"âŒ ERROR: Save Log Error: {e}")
        return jsonify({"code": 500, "message": "Error saving logs."}), 500


# integeration ruru

@app.route('/api/integeration_ruru_cosmos', methods=['POST'])
def integeration_ruru_cosmos():
    try:
        data = request.json

        base_month = data['Base_Month']
        fundType = data['fundType']
        fcode = data['Fcode']
        org_sheet_name = data['Org_SheetName']
        org_title = data['Org_Title']
        org_text = data['Org_Text']
        org_type = data['Org_Type']
        target_sheet_name = data['Target_SheetName']
        target_text = data['Target_Text']
        target_type = data['Target_Type']
        target_condition = data['Target_Condition']
        result = data['result']
        Target_Consult = data['Target_Consult']
        flag = data['flag']
        id = data['id']
        No = data['No']

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        query = f"SELECT * FROM c WHERE c.Fcode = '{data['Fcode']}' AND c.Base_Month = '{data['Base_Month']}' AND c.fundType = '{data['fundType']}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        common_item = {
            "id": id,
            "No": No,
            "fundType": fundType,
            "Base_Month": base_month,
            "Fcode": fcode,
            "Org_SheetName": org_sheet_name,
            "Org_Title": org_title,
            "Org_Text": org_text,
            "Org_Type": org_type,
            "Target_SheetName": target_sheet_name,
            "Target_Text": target_text,
            "flag": flag,
            "Target_Type": target_type,
            "Target_Condition": target_condition,
            "updateTime": datetime.utcnow().isoformat(),  
        }

        if flag == 'close':
            common_item["result"] = result

        elif flag == 'open':
            common_item["Target_Consult"] = Target_Consult

        item = common_item

        if items:
            # container.upsert_item(item)
            items[0].update(item)
            container.upsert_item(items[0])
            logging.info("âœ… Data updated in Cosmos DB successfully.")
            return jsonify({"success": True, "message": "Data updated successfully."}), 200
        else:
            container.upsert_item(item)
            logging.info("âœ… Data inserted into Cosmos DB successfully.")
            return jsonify({"success": True, "message": "Data inserted successfully."}), 200

        # if items:
        #     item["id"] = items[0]["id"]
        #     container.replace_item(item=items[0], body=item)
        #     logging.info("âœ… Data updated in Cosmos DB successfully.")
        #     return jsonify({"success": True, "message": "Data updated successfully."}), 200
        # else:
        #     container.create_item(body=item)
        #     logging.info("âœ… Data inserted into Cosmos DB successfully.")
        #     return jsonify({"success": True, "message": "Data inserted successfully."}), 200

    except Exception as e:
        logging.error(f"âŒ Cosmos DB save error: {e}")
        return jsonify({"success": False, "message": str(e)}), 500

@app.route('/api/integeration_ruru_cosmos', methods=['GET'])
def get_integeration_ruru_cosmos():
    # Cosmos DB è¿æ¥
    container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

    flag = request.args.get("flag")
    base_month = request.args.get("Base_Month")

    query = "SELECT * FROM c"
    parameters = []

    if flag and base_month:
        query += " WHERE c.flag = @flag AND c.Base_Month = @base_month"
        parameters = [
            {"name": "@flag", "value": flag},
            {"name": "@base_month", "value": base_month}
        ]
    elif flag:
        query += " WHERE c.flag = @flag"
        parameters = [{"name": "@flag", "value": flag}]
    elif base_month:
        query += " WHERE c.Base_Month = @base_month"
        parameters = [{"name": "@base_month", "value": base_month}]

    users = list(container.query_items(
        query=query,
        parameters=parameters,
        enable_cross_partition_query=True
    ))

    response = {
        "code": 200,
        "data": users
    }
    return jsonify(response), 200

# common ruru add logic
def common_ruru_text(text):
    corrections = []
    seen = set()

    # â‘  ãƒ•ã‚¡ãƒ³ãƒ‰ï¼‹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ä¸¡æ–¹ â†’ è¶…éåç›Š 
    pattern_excess = (
        r"åŸºæº–ä¾¡é¡ã®é¨°è½ç‡ã¯([+-]?\d+(\.\d+)?)ï¼…ã€"
        r"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®é¨°è½ç‡ã¯([+-]?\d+(\.\d+)?)ï¼…"
    )
    match = re.search(pattern_excess, text)

    if match:
        fund_return = float(match.group(1))
        benchmark_return = float(match.group(3))

        # round 2
        calculated_excess = round(fund_return - benchmark_return, 2)

        result = {
            "é¨°è½ç‡": fund_return,
            "ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®é¨°è½ç‡": benchmark_return,
            "è¶…éåç›Šï¼ˆãƒã‚¤ãƒ³ãƒˆå·®ï¼‰": calculated_excess,
            "reason": "åŸºæº–ä¾¡é¡ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®å·®ã‚’è¨ˆç®—ã—ã¾ã—ãŸ"
        }

        key = str(result)  # dict set duipli
        if key not in seen:
            seen.add(key)
            corrections.append(result)


    else:
        # pass
        # â‘¡ å€‹åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒã‚§ãƒƒã‚¯
        patterns = {
            # "fund_only": r"æœˆé–“ã®åŸºæº–ä¾¡é¡ã®é¨°è½ç‡ã¯[+-]?\d+(\.\d+)?ï¼…",
            "benchmark_only": r"ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã®é¨°è½ç‡ã¯[+-]?\d+(\.\d+)?ï¼…",
            "course_multi": r"([ï¼¡-ï¼ºA-Zã-ã‚“ã‚¡-ãƒ³ä¸€-é¾¥]+ã‚³ãƒ¼ã‚¹)ãŒ[+-]?\d+(\.\d+)?ï¼…",
            "hedge": r"(ç‚ºæ›¿ãƒ˜ãƒƒã‚¸ã‚ã‚Š|ç‚ºæ›¿ãƒ˜ãƒƒã‚¸ãªã—)ã¯[+-]?\d+(\.\d+)?ï¼…",
            "currency_type": r"(å††æŠ•è³‡å‹|ç±³ãƒ‰ãƒ«æŠ•è³‡å‹)ã®æœˆé–“é¨°è½ç‡ã¯[+-]?\d+(\.\d+)?ï¼…",
            # "global_type": r"ã€[^ã€‘]+ã€‘[+-]?\d+(\.\d+)?ï¼…",
            # "point_value": r"[+-]?\d+(\.\d+)?ãƒã‚¤ãƒ³ãƒˆ",
            "select_course": r"é€šè²¨ã‚»ãƒ¬ã‚¯ãƒˆã‚³ãƒ¼ã‚¹.*?(ä¸Šæ˜‡|ä¸‹è½)",
            "fund_updown": r"åŸºæº–ä¾¡é¡ï¼ˆåˆ†é…é‡‘å†æŠ•è³‡ï¼‰ã¯.*?(ä¸Šæ˜‡|ä¸‹è½)"
        }

        # ãƒ•ã‚¡ãƒ³ãƒ‰å‹: å½“ãƒ•ã‚¡ãƒ³ãƒ‰ + ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        pattern_fund = r"å½“ãƒ•ã‚¡ãƒ³ãƒ‰ã®æœˆé–“é¨°è½ç‡.*?ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯[^ã€‚]*?ãƒã‚¤ãƒ³ãƒˆ[^ã€‚]"
        # å¸‚æ³å‹: æ ªå¼å¸‚å ´ + TOPIX
        pattern_market = r"TOPIXï¼ˆæ±è¨¼æ ªä¾¡æŒ‡æ•°ï¼‰[^ã€‚]*"

        # --- ãƒ•ã‚¡ãƒ³ãƒ‰å‹ ---
        fund_sentences = re.findall(pattern_fund, text)
        for sentence in fund_sentences:
            for m in re.finditer(r"[^ã€ã€‚]+?(ï¼…|ãƒã‚¤ãƒ³ãƒˆ)", sentence):
                extracted = m.group(0).strip()
                if extracted not in seen:
                    seen.add(extracted)
                    corrections.append({"extract": extracted})

        # --- å¸‚æ³å‹  ---
        market_sentences = re.findall(pattern_market, text)
        for sentence in market_sentences:
            for m in re.finditer(r"[^ã€ã€‚]+?(ï¼…|ãƒã‚¤ãƒ³ãƒˆ)", sentence):
                extracted = m.group(0).strip()
                if extracted not in seen:
                    seen.add(extracted)
                    corrections.append({"extract": extracted})


        # ãã®ä»–ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ä¸€æ‹¬æŠ½å‡º
        for name, pat in patterns.items():
            for m in re.finditer(pat, text):
                extracted_other = m.group(0)
                if extracted_other not in seen:
                    seen.add(extracted_other)
                    corrections.append({"extract": extracted_other})

    return corrections

# --- common ruru api
@app.route('/api/common_ruru', methods=['POST'])
def common_ruru():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("âœ… Token Update SUCCESS")
        
        data = request.json
        input_list = data.get("input", "")
        pdf_base64 = data.get("pdf_bytes", "")
        pageNumber = data.get('pageNumber',0)

        if not input_list:
            return jsonify({"success": False, "error": "No input provided"}), 400
        
        corrections = []
        if isinstance(input_list, list):
            for idx, t in enumerate(input_list, start=1):
                part_result = common_ruru_text(t) 
                for pr in part_result:
                    corrections.append({
                        "page": pageNumber,
                        "original_text": pr.get("extract", t),
                        "comment": f"{pr.get("extract", t)} â†’ ",
                        "reason_type": pr.get("reason", "æ•´åˆæ€§"),
                        "check_point": pr.get("extract", t),
                        "locations": [], 
                        "intgr": True
                    })
        else:
            part_result = common_ruru_text(input_list)
            for pr in part_result:
                corrections.append({
                    "page": pageNumber,
                    "original_text": input_list,
                    "comment": f"{input_list} â†’ {pr.get('extract', pr.get('è¶…éåç›Šï¼ˆãƒã‚¤ãƒ³ãƒˆå·®ï¼‰', ''))}",
                    "reason_type": pr.get("reason", "æ•´åˆæ€§"),
                    "check_point": pr.get("extract", input_list),
                    "locations": [],
                    "intgr": True
                })
        
        try:
            pdf_bytes = base64.b64decode(pdf_base64)
            find_locations_in_pdf(pdf_bytes, corrections)

            return jsonify({
                "success": True,
                "corrections": corrections,
            })

        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500
        
        
    except Exception as e:
        # exception return JSON 
        return jsonify({"success": False, "error": str(e)}), 500

# --- ruru test api

@app.route('/api/ruru_search_db', methods=['POST'])
def ruru_search_db():
    try:
        data = request.json

        fcode = data.get('fcode')
        base_month = data.get('Base_Month')
        fund_type = data.get('fundType', 'private')

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        query = f"SELECT * FROM c WHERE c.Fcode = '{fcode}' AND c.Base_Month = '{base_month}' AND c.fundType = '{fund_type}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        if items:
            # results = [{"id": item["id"], "result": item["result"],"Org_Text":item["Org_Text"],"Org_Type":item["Org_Type"],"Target_Condition":item["Target_Condition"]} for item in items]
            results = [item if item.get("flag") else {"id": item["id"], "result": item["result"],"Org_Text":item["Org_Text"],"Org_Type":item["Org_Type"],"Target_Condition":item["Target_Condition"]} for item in items]
            return jsonify({"success": True, "data": results}), 200
        else:
            return jsonify({"success": False, "message": "No matching data found in DB."}), 200

    except Exception as e:
        logging.error(f"âŒ Error occurred while searching DB: {e}")
        return jsonify({"success": False, "message": str(e)}), 500

@app.route('/api/refer_operate', methods=['GET'])
def get_rule():
    try:
        data = request.args
        flag = data.get('flag', "")
        fund_type = data.get('fundType', 'private')

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        query = f"SELECT * FROM c WHERE c.flag = '{flag}' AND c.fundType = '{fund_type}'"
        items = list(container.query_items(query=query, enable_cross_partition_query=True))

        if items:
            items_map = list(map(lambda y: dict(filter(lambda x: not x[0].startswith("_"), y.items())), items))
            return jsonify({"success": True, "data": items_map}), 200
        else:
            return jsonify({"success": False, "message": "No matching data found in DB."}), 404

    except Exception as e:
        logging.error(f"âŒ Error occurred while searching DB: {e}")
        return jsonify({"success": False, "message": str(e)}), 500
    

@app.route('/api/open_cosmos_data', methods=['POST'])
def get_open_data():
    data = request.json
    f_code = data.get("f_code", "")
    flag = data.get("flag", "open")
    base_month = data.get("base_month", "M2411")
    query = f"SELECT * FROM c WHERE c.flag = '{flag}' AND c.Base_Month = '{base_month}' and c.Fcode = '{f_code}'"
    items = list(integeration_container.query_items(query=query, enable_cross_partition_query=True))

    if items:
        return jsonify({"success": True, "data": items}), 200
    else:
        return jsonify({"success": False, "message": "No matching data found in DB."}), 200

@app.route('/api/save_cosmos_data', methods=['POST'])
def save_open_data():
    data = request.json
    item = data.get("item")

    if item:
        integeration_container.upsert_item(item)
        return jsonify({"success": True}), 200
    else:
        return jsonify({"success": False}), 200


@app.route('/api/refer_operate', methods=['POST'])
def insert_rule():
    try:
        data = request.json

        base_month = data.get('Base_Month', '')
        fund_type = data.get('fundType', '')
        fcode = data.get('Fcode', '')
        org_sheet_name = data.get('Org_SheetName', '')
        org_title = data.get('Org_Title', '')
        org_text = data.get('Org_Text', '')
        org_type = data.get('Org_Type', '')
        target_sheet_name = data.get('Target_SheetName', '')
        target_title = data.get('Target_Title', '')
        target_text = data.get('Target_Text', '')
        target_type = data.get('Target_Type', '')
        target_condition = data.get('Target_Condition', '')
        target_consult = data.get('Target_Consult', '')
        id = str(uuid.uuid4())

        container = get_db_connection(INTEGERATION_RURU_CONTAINER_NAME)

        item = {
            "id": id,
            "No": id,
            "fundType": fund_type,
            "Base_Month": base_month,
            "Fcode": fcode,
            "Org_SheetName": org_sheet_name,
            "Org_Title": org_title,
            "Org_Text": org_text,
            "Org_Type": org_type,
            "Target_SheetName": target_sheet_name,
            "Target_Text": target_text,
            "Target_Title": target_title,
            "Target_Type": target_type,
            "Target_Condition": target_condition,
            "Target_Consult": target_consult,
            "flag": "open",
            "updateTime": datetime.now().isoformat()
        }
        container.upsert_item(item)
        return jsonify({"success": True}), 200
    except Exception as e:
        return jsonify({"success": False, "message": str(e)}), 500

async def get_original(input_data, org_text, file_name="", target_text=""):
    dt = [
        "æ–‡ç« ã‹ã‚‰åŸæ–‡ã«é¡ä¼¼ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",
        "å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§ãŠé¡˜ã„ã—ã¾ã™:",
        "- [{'target': '[æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:]'}]",
        "- é¡ä¼¼ã—ãŸã‚‚ã®ãŒãªã„å ´åˆã¯ã€ç©ºã®æ–‡å­—åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚",
        "æŠ½å‡ºãƒ«ãƒ¼ãƒ«ï¼š",
        "- åŸæ–‡ã¨ã€Œèªé †ãƒ»æ–‡æ§‹é€ ãƒ»æ–‡æ³•ãƒ‘ã‚¿ãƒ¼ãƒ³ã€ãŒé«˜ãä¸€è‡´ã—ã¦ã„ã‚‹æ–‡ã¯ã€ãŸã¨ãˆèªå¥ï¼ˆåè©ã‚„ä¸»èªãªã©ï¼‰ãŒä¸€éƒ¨é•ã£ã¦ã„ã¦ã‚‚ã€**å¿…ãšæŠ½å‡ºã—ã¦ãã ã•ã„**ã€‚",
        "- é¡ä¼¼åº¦ãŒ50%ä»¥ä¸Šã®æ–‡ã‚’ã™ã¹ã¦æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚",
        "- **åŸæ–‡ã¨æ§‹é€ ãŒä¼¼ã¦ã„ã‚‹æ–‡ã‚‚è¦‹è½ã¨ã•ãšã«æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚**",
        "- **æŠ½å‡ºã™ã‚‹æ–‡ãŒåŸæ–‡ã®è¨€ã„æ›ãˆãƒ»æ–‡å‹ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å…±é€šæ€§ãŒã‚ã‚‹å ´åˆã€ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã®é•ã„ãŒã‚ã£ã¦ã‚‚å¯¾è±¡ã«å«ã‚ã¦ãã ã•ã„ã€‚**",
        "- æœ€ã‚‚é¡ä¼¼ã—ãŸä¸€æ–‡ã ã‘ã‚’è¿”ã•ãšã€æ¡ä»¶ã‚’æº€ãŸã™ã™ã¹ã¦ã®æ–‡ã‚’å¿…ãšæŠ½å‡ºã—ã¦ãã ã•ã„ã€‚",

        f"åŸæ–‡:{org_text}\næ–‡ç« :{input_data}"
    ]
    input_data = "\n".join(dt)

    question = [
        {"role": "system", "content": "ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"},
        {"role": "user", "content": input_data},
        {"role": "user", "content": input_data}
    ]
    response = await openai.ChatCompletion.acreate(
        deployment_id=deployment_id,  # Deploy Name
        messages=question,
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  # seed
    )
    answer = response['choices'][0]['message']['content'].strip().replace("`", "").replace("json", "", 1)

    src_score = 0.5
    src_content = ""
    if answer:
        parsed_data = ast.literal_eval(answer)
        for once in parsed_data:
            similar_content = once.get("target")
            if file_name.startswith("180015"):
                if org_text[:4] in similar_content[:6]:
                    src_content = similar_content
                    break
            elif re.search("180332|180358|180359|180360|180344|180345", file_name):
                if org_text[:5] in similar_content[:10]:
                    if target_text in ["ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥é…åˆ†", "ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥å¯„ä¸åº¦"]:
                        re_content = re.search("(ã‚»ã‚¯ã‚¿ãƒ¼åˆ¥.*)å€‹åˆ¥ã®å¯„ä¸åº¦", similar_content, re.DOTALL)
                        if re_content:
                            src_content = re_content.groups(1)[0]
                            break
                    elif target_text in ["å¯„ä¸åº¦", "å¯„ä¸åº¦ï¼ã€ä¸Šä½5éŠ˜æŸ„ã€‘"]:
                        re_content = re.search("å€‹åˆ¥ã®å¯„ä¸åº¦.*", similar_content, re.DOTALL)
                        if re_content:
                            src_content = re_content.group()
                            break
            elif re.search("180001|180002|180003|180004|180015|180021|180022|180023", file_name):
                if org_text[1: 5] in similar_content[: 10]:
                    src_content = similar_content
                    break
            elif re.search("140672", file_name):
                if org_text[1: 6] in similar_content[: 10]:
                    src_content = similar_content
                    break
            if similar_content:
                score = SequenceMatcher(None, org_text, similar_content).ratio()
                if score > src_score:
                    src_score = score
                    src_content = similar_content
    return src_content, answer


LOCAL_LINK = "local_link"
@app.route('/api/getaths', methods=['GET'])
def get_local_link():
    try:
        container = get_db_connection(LOCAL_LINK)
        log_data = list(container.query_items(
            query=f"SELECT * FROM c",
            enable_cross_partition_query=True
        ))
        log_map = list(map(lambda y: dict(filter(lambda x: x[1] and not x[0].startswith("_"), y.items())), log_data))
        return jsonify({"success": True, "data": log_map}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 404


@app.route('/api/saveaths', methods=['POST'])
def save_local_link():
    try:
        data = request.json
        commonComment = data.get("commonComment")
        individualCheckPath = data.get("individualCheckPath")
        individualComment = data.get("individualComment")
        individualExcelPath = data.get("individualExcelPath")
        individualPdfPath = data.get("individualPdfPath")
        meigaramaster = data.get("meigaramaster")
        reportData = data.get("reportData")
        simu = data.get("simu")
        resultngPath = data.get("resultngPath")
        resultokPath = data.get("resultokPath")
        fund_type = data.get("fund_type")
        container = get_db_connection(LOCAL_LINK)
        link_data = list(container.query_items(
           query=f"SELECT * FROM c WHERE c.fund_type='{fund_type}'",
            enable_cross_partition_query=True
        ))
        update_data = dict(
                fund_type=fund_type,
                commonComment=commonComment,
                individualCheckPath=individualCheckPath,
                individualComment=individualComment,
                individualExcelPath=individualExcelPath,
                individualPdfPath=individualPdfPath,
                meigaramaster=meigaramaster,
                reportData=reportData,
                simu=simu,
                resultngPath=resultngPath,
                resultokPath=resultokPath
        )

        if not link_data:
            update_data.update(id=str(uuid.uuid4()))
            container.upsert_item(update_data)
        else:
            effective_data = dict(filter(lambda x: x[1] is not None, update_data.items()))
            link_data[0].update(effective_data)
            container.upsert_item(link_data[0])
        return jsonify({"success": True}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 404


@app.route('/api/log_operate')
def get_log():
    try:
        # 1)  page=1, size=15
        page = int(request.args.get('page', 1))
        size = int(request.args.get('size', 15))
        file_name = request.args.get('fileName', "")
        log_controller = get_db_connection(LOG_RECORD_CONTAINER_NAME)
        offset = (page - 1) * size
        if file_name:
            file_query = f"SELECT * FROM c WHERE CONTAINS(c.fileName, '{file_name}') OFFSET {offset} LIMIT {size}"
            total_file = list(log_controller.query_items(
                query=file_query,
                enable_cross_partition_query=True
            ))
            name_count = f"SELECT VALUE COUNT(1) FROM c WHERE CONTAINS(c.fileName, '{file_name}')"
            count_result = list(log_controller.query_items(
                query=name_count,
                enable_cross_partition_query=True
            ))[0]
            return jsonify({
                "success": True,
                "data": total_file,
                "total": count_result

            }), 200
        count_query = "SELECT VALUE COUNT(1) FROM c"
        total_count = list(log_controller.query_items(
            query=count_query,
            enable_cross_partition_query=True
        ))[0]

        query = f"""
                SELECT * FROM c
                ORDER BY c.created_at DESC
                OFFSET {offset} LIMIT {size}
                """
        log_data = list(log_controller.query_items(
            query=query,
            enable_cross_partition_query=True
        ))

        log_map = list(map(lambda y: dict(filter(lambda x: x[1] and not x[0].startswith("_"), y.items())), log_data))

        return jsonify({
            "success": True,
            "data": log_map,
            "total": total_count
        }), 200
    
        # return jsonify({"success": True, "data": log_map}), 200

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

@app.route('/api/check_file', methods=['POST'])
def check_file_statue():
    try:
        data = request.json
        file_name = data.get("file_name")
        fund_type = data.get("fund_type")
        comment_type = data.get("comment_type")
        upload_type = data.get("upload_type", "")
        container = get_db_connection(FILE_MONITOR_ITEM)
        file_data = list(container.query_items(
            query=f"SELECT * FROM u WHERE u.file_name = '{file_name}'",
            enable_cross_partition_query=True
        ))
        if file_data:
            file_info = file_data[0]
            if file_info.get("flag") == "success":
                pdf_name = re.sub(r"\.(xlsx|xlsm|xls|docx|doc)", ".pdf", file_name)
                result = {
                    "corrections": file_info.get("corrections", [])
                }
                is_url = file_info.get("link", "")
                link_url = re.sub(r"\.(xlsx|xlsm|xls|docx|doc)", ".pdf", is_url)
                save_to_cosmos(pdf_name, result, link_url, fund_type, comment_type=comment_type, upload_type=upload_type)
                return jsonify({"success": True}), 200
        return jsonify({"success": False}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 303


@app.route('/api/file_status_update', methods=['POST'])
def file_update():
    try:
        data = request.json
        id = data.get("id", "")
        flag = data.get("flag", "")
        file_name = data.get("file_name", "")
        link_url = data.get("link", "")
        error_space = data.get("error_space", "")
        if id and flag and file_name:
            container = get_db_connection(FILE_MONITOR_ITEM)
            corrections = list(map(lambda x: dict(
                check_point=x.get("original_text"),
                original_text=x.get("original_text"),
                comment=x.get("original_text"),
                intgr=False,
                page=0,
                reason_type=x.get("reason_type"),
                locations=[{"x0": 0, "x1": 0, "y0": 0, "y1": 0}]
                ), error_space))
            container.upsert_item({"id": id, "flag": flag, "file_name": file_name, "link": link_url, "corrections": corrections})
            return jsonify({"success": True}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 303


@app.route('/api/file_status_search', methods=['GET'])
def file_search():
    try:
        container = get_db_connection(FILE_MONITOR_ITEM)
        file_data = list(container.query_items(
            query=f"SELECT * FROM u WHERE u.flag = 'wait'",
            enable_cross_partition_query=True
        ))
        if file_data:
            results = []
            for file_info in file_data:
                results.append(file_info)
            return jsonify({"success": True, "data": results}), 200
        else:
            return jsonify({"success": False, "data": []}), 200
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 303


@app.route('/api/ruru_ask_gpt_enhance', methods=['POST'])
def integrate_enhance():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("âœ… Token Update Done")

        data = request.json
        _content = data.get("input", "")
        condition = data.get("Target_Condition", "")
        category = data.get("Org_Type", "")
        consult = data.get("Target_Consult", "")
        base_month = data.get("Base_month", "")
        pageNumber = data.get('pageNumber',0)
        file_name = data.get("file_name", "")
        target_text = data.get("Target_Text", "")

        org_text = data.get("Org_Text", "")
        __answer = ""

        if org_text == "ãƒªã‚¹ã‚¯æŠ‘åˆ¶æˆ¦ç•¥ã®çŠ¶æ³":
            if "ãƒªã‚¹ã‚¯æŠ‘åˆ¶æˆ¦ç•¥ã®çŠ¶æ³" in _content:
                return jsonify({
                    "success": True,
                    "corrections": [{
                        "page": pageNumber,
                        "original_text": "ãƒªã‚¹ã‚¯æŠ‘åˆ¶æˆ¦ç•¥ã®çŠ¶æ³",
                        "check_point": "ãƒªã‚¹ã‚¯æŠ‘åˆ¶æˆ¦ç•¥ã®çŠ¶æ³",
                        "comment": f"ãƒªã‚¹ã‚¯æŠ‘åˆ¶æˆ¦ç•¥ã®çŠ¶æ³ â†’ ",
                        "reason_type":"æ•´åˆæ€§", 
                        "locations": [{"x0": 0, "x1": 0, "y0": 0, "y1": 0}],
                        "intgr": True, 
                    }]
                })
            else:
                return jsonify({
                    "success": True,
                    "corrections": [{
                        "page": pageNumber,
                        "original_text": "ãƒªã‚¹ã‚¯æŠ‘åˆ¶æˆ¦ç•¥ã®çŠ¶æ³",
                        "check_point": "ãƒªã‚¹ã‚¯æŠ‘åˆ¶æˆ¦ç•¥ã®çŠ¶æ³",
                        "comment": f"ãƒªã‚¹ã‚¯æŠ‘åˆ¶æˆ¦ç•¥ã®çŠ¶æ³ â†’ ",
                        "reason_type": "ãƒªã‚¹ã‚¯æŠ‘åˆ¶æˆ¦ç•¥ã®çŠ¶æ³ãŒå­˜åœ¨ã—ã¦ã„ã¾ã›ã‚“ã€‚",  
                        "locations": [{"x0": 0, "x1": 0, "y0": 0, "y1": 0}],
                        "intgr": True,  
                    }]
                })

        elif org_text == "éŠ˜æŸ„å1ï½10":
            content = _content
        elif org_text == "ã€éŠ˜æŸ„åã€‘Lâ€™Occitane en Provenceï¼ˆæ¬§å·ï¼‰":
            content_re = re.search("ã€éŠ˜æŸ„åã€‘.{,100}", _content)
            if content_re:
                content = content_re.group()
            else:
                content = ""


        else:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            content, __answer = loop.run_until_complete(get_original(_content, org_text, file_name, target_text))

            if not content:
                return jsonify({
                    "success": True,
                    "answer": __answer,
                    "corrections": []
                })

        pdf_base64 = data.get("pdf_bytes", "")

        file_name_decoding = data.get("file_name", "")

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        if condition:
            result_temp = []
            table_list = condition.split("\n")
            for data in table_list:
                if data:
                    if category in ["æ¯”ç‡", "é…åˆ†"]:
                        re_num = re.search(r"([-\d. ]+)(%|ï¼…)", content)
                        if re_num:
                            num = re_num.groups()[0]
                            float_num = len(str(num).split(".")[1]) if "." in num else 0
                            old_data = pd.read_json(StringIO(data))
                            result_temp.append(old_data.applymap(
                                lambda x: (str(round(x * 100, float_num)) + "%" if float_num != 0 else str(
                                    int(round(x * 100, float_num))) + "%")
                                if not pd.isna(x) and isinstance(x, float) else x).to_json(force_ascii=False))
                        else:
                            result_temp.append(pd.read_json(StringIO(data)).to_json(force_ascii=False))
                    else:
                        result_temp.append(pd.read_json(StringIO(data)).to_json(force_ascii=False))
            if len(result_temp) > 1:
                result_data = "\n".join(result_temp)
            else:
                result_data = result_temp[0]
        else:
            result_data = ""

        input_list = [
            "ä»¥ä¸‹ã®å†…å®¹ã«åŸºã¥ã„ã¦ã€åŸæ–‡ã®è¨˜è¿°ãŒæ­£ã—ã„ã‹ã©ã†ã‹ã‚’åˆ¤æ–­ã—ã¦ãã ã•ã„", "è¦ä»¶:",
            "- ã€å‚è€ƒãƒ‡ãƒ¼ã‚¿ã€ã«è©²å½“ã™ã‚‹æƒ…å ±ãŒãªã„å ´åˆã€ãã®è¨˜è¿°ã«ã¤ã„ã¦ã¯åˆ¤æ–­ã‚’è¡Œã‚ãšã€ã€Œåˆ¤å®šå¯¾è±¡å¤–ã€ã¨æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚",
            "- æœ€å¾Œã«åŸæ–‡ã®è¨˜è¿°ãŒæ­£ã—ã„ã‹ã©ã†ã‹ã‚’æ˜ç¢ºã«åˆ¤æ–­ã—ã€æ–‡æœ«ã«ã€OKã€ã¾ãŸã¯ã€NGã€ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„",
            f"- **ç¾åœ¨ã®å‚è€ƒãƒ‡ãƒ¼ã‚¿ã¯20{base_month[1:3]}å¹´{base_month[3:]}æœˆã®å‚è€ƒãƒ‡ãƒ¼ã‚¿ã§ã™**",
            f"- æ–‡ä¸­ã«ã€å…ˆæœˆæœ«ã€ã€å‰æœˆæœ«ã€ã€â—‹æœˆæœ«ã€ãªã©ã®è¡¨ç¾ãŒã‚ã£ã¦ã‚‚ã€ç¾åœ¨ã®å‚è€ƒãƒ‡ãƒ¼ã‚¿ï¼ˆæœˆï¼‰ã‚’åŸºæº–ã¨ã—ã¦åˆ¤æ–­ã—ã¦ãã ã•ã„",
            f"åŸæ–‡ã®åˆ¤æ–­:'{content}'\nå‚è€ƒãƒ‡ãƒ¼ã‚¿:\n'{result_data}'",
        ]

        if consult:
            input_list.insert(3, consult)
        input_data = "\n".join(input_list)
        question = [
            {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªæ–‡æ›¸ã®æ ¡æ­£ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"},
            {"role": "user", "content": input_data}
        ]

        response = openai.ChatCompletion.create(
            deployment_id=deployment_id,  # Deploy Name
            messages=question,
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE,
            seed=SEED  # seed
        )
        answer = response['choices'][0]['message']['content'].strip()
        if answer:
            dt = [
                "ä»¥ä¸‹ã®åˆ†æçµæœã«åŸºã¥ãã€åŸæ–‡ä¸­ã®èª¤ã‚Šã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",
                "å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§ãŠé¡˜ã„ã—ã¾ã™:",
                "- [{'original': '[åŸæ–‡ä¸­ã®èª¤ã£ã¦ã„ã‚‹éƒ¨åˆ†:]', 'reason': '[ç†ç”±:]'}]",
                "- åŸæ–‡ã®æœ«å°¾ã«ã€ŒOKã€ãŒã‚ã‚‹å ´åˆã¯ã€ç©ºæ–‡å­—åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„",
                f"åŸæ–‡:'{content}'\nåˆ†æçµæœ:'{answer}'"
            ]
            summarize = "\n".join(dt)
            _question = [
                {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªæ–‡æ›¸ã®æ ¡æ­£ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"},
                {"role": "user", "content": summarize}
            ]
            _response = openai.ChatCompletion.create(
                deployment_id=deployment_id,  # Deploy Name
                messages=_question,
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                seed=SEED  # seed
            )
            _answer = _response['choices'][0]['message']['content'].strip().replace("`", "").replace("json", "", 1)
            parsed_data = ast.literal_eval(_answer)
            corrections = []
            if parsed_data:
                for once in parsed_data:
                    error_data = once.get("original", "")
                    reason = once.get("reason", "")
                    corrections.append({
                        "page": pageNumber,
                        "original_text": get_src(error_data, _content).replace("ã€‚â—‹","").replace("ã€‚â—¯","").strip().rsplit('\n', 1)[0],
                        "check_point": content,
                        "comment": f"{error_data} â†’ {reason}", #
                        "reason_type":reason, 
                        "locations": [],
                        "intgr": True, 
                    })
            else:
                corrections.append({
                    "page": pageNumber,
                    "original_text": get_src(content, _content).replace("ã€‚â—‹","").replace("ã€‚â—¯","").strip().rsplit('\n', 1)[0],
                    "check_point": content,
                    "comment": f"{content} â†’ ",
                    "reason_type": "æ•´åˆæ€§",  
                    "locations": [],
                    "intgr": True,
                })

            try:
                pdf_bytes = base64.b64decode(pdf_base64)
                find_locations_in_pdf(pdf_bytes, corrections)
                

            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500

            return jsonify({
                "success": True,
                "answer": __answer,
                "first_answer": answer,
                "input_data": input_data,
                "corrections": corrections
            })
        else:
            return jsonify({
                "success": True,
                "corrections": [{
                    "page": pageNumber,
                    "original_text": content,
                    "check_point": content,
                    "comment": f"{content} â†’ ",
                    "reason_type":"æ•´åˆæ€§", 
                    "locations": [{"x0": 0, "x1": 0, "y0": 0, "y1": 0}],
                    "intgr": True, 
                }]  
            })

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 200

def extract_or_return(sentence):
    pattern = (
    r"(?P<fund_return>(?:ãƒ•ã‚¡ãƒ³ãƒ‰|åŸºæº–ä¾¡é¡(?:ï¼ˆåˆ†é…é‡‘å†æŠ•è³‡ï¼‰)?|åŸºæº–ä¾¡é¡ã®å¤‰å‹•ç‡|åŸºæº–ä¾¡é¡é¨°è½ç‡|é¨°è½ç‡)[ã®]?(?:å¤‰å‹•ç‡|é¨°è½ç‡)?[-âˆ’]?\d+\.?\d*ï¼…?)?.*?"
    r"(?P<benchmark_return>(?:BM|ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯|å‚è€ƒæŒ‡æ•°)[ã®]?(?:é¨°è½ç‡|å¤‰å‹•ç‡)?[-âˆ’]?\d+\.?\d*ï¼…?)?.*?"
    r"(?P<diff_points>\d+\.?\d*ãƒã‚¤ãƒ³ãƒˆ)?.*?"
    r"(?P<direction>(ä¸Šå›[ã‚Š]*|ä¸‹å›[ã‚Š]*))?"
    )

    match = re.search(pattern, sentence)

    extracted = [v for v in match.groupdict().values() if v]

    return extracted if extracted else [sentence]

@app.route('/api/ruru_ask_gpt', methods=['POST'])
def ruru_ask_gpt():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("âœ… Token Update SUCCESS")
        
        data = request.json
        _input = data.get("input", "")
        result = data.get("result", "")
        orgtext = data.get("Org_Text", "")
        OrgType = data.get("Org_Type", "")
        TargetCondition = data.get("Target_Condition", "")
        pageNumber = data.get('pageNumber',0)
        
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        input, __answer = loop.run_until_complete(get_original(_input, orgtext))
        corrections = []
        pdf_base64 = data.get("pdf_bytes", "")
        if not input:
            dt = [
            "æ–‡ç« ã‹ã‚‰åŸæ–‡ã«é¡ä¼¼ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",
            "å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§ãŠé¡˜ã„ã—ã¾ã™:",
            "- {'target': '[æŠ½å‡ºã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆ:]'}",
            "- é¡ä¼¼ã—ãŸã‚‚ã®ãŒãªã„å ´åˆã¯ã€ç©ºã®æ–‡å­—åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„",
            "- é¡ä¼¼ã—ãŸã‚‚ã®ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ã‚‚ã®ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",

            f"åŸæ–‡:{orgtext}\næ–‡ç« :{_input}"
            ]
            input_data = "\n".join(dt)

            question = [
                {"role": "system", "content": "ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"},
                {"role": "user", "content": input_data}
            ]
            response = openai.ChatCompletion.create(
                deployment_id=deployment_id,  # Deploy Name
                messages=question,
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                seed=SEED  # seed
            )
            _answer = response['choices'][0]['message']['content'].strip().strip().replace("`", "").replace("json", "", 1)
            _parsed_data = ast.literal_eval(_answer)
            _similar = _parsed_data.get("target")

            pattern = r'([ABCDEF]ã‚³ãƒ¼ã‚¹.?[+-]?\d+(?:\.\d+)?ï¼…|[ABCDEF]ã‚³ãƒ¼ã‚¹.?åŸºæº–ä¾¡é¡ã¯(?:ä¸‹è½|ä¸Šæ˜‡)(?:ã¾ã—ãŸ)?)'

            matches_list = re.findall(pattern, _similar)
            for re_result in matches_list:
                                
                corrections.append({
                        "page": pageNumber,
                        "original_text": re_result,
                        "check_point": re_result,
                        "comment": f"{re_result} â†’ ", # +0.2% â†’ 0.85% f"{reason} â†’ {corrected}"
                        "reason_type": "æ•´åˆæ€§",  
                        "locations": [],
                        "intgr": True,
                    })
                
            try:
                pdf_bytes = base64.b64decode(pdf_base64)
                
                find_locations_in_pdf(pdf_bytes, corrections)
                
            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500

        else:
            if not input:
                return jsonify({"success": False, "error": "No input provided"}), 400
            
            # add the write logic
            dt = [
                "æ–‡ç« ã‹ã‚‰åŸæ–‡ã«é¡ä¼¼ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",
                "å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§ãŠé¡˜ã„ã—ã¾ã™:",
                "- [{'original': '[åŸæ–‡ä¸­ã®èª¤ã£ã¦ã„ã‚‹éƒ¨åˆ†:]', 'reason': '[ç†ç”±:]'}]",
                "- é¡ä¼¼ã—ãŸã‚‚ã®ãŒãªã„å ´åˆã¯ã€ç©ºã®æ–‡å­—åˆ—ã‚’è¿”ã—ã¦ãã ã•ã„",
                "- é¡ä¼¼ã—ãŸã‚‚ã®ãŒå­˜åœ¨ã™ã‚‹å ´åˆã¯ã€æœ€ã‚‚é¡ä¼¼åº¦ã®é«˜ã„ã‚‚ã®ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",

                "ã‚ãªãŸã¯æ—¥æœ¬ã®é‡‘èãƒ¬ãƒãƒ¼ãƒˆã‚’å°‚é–€ã¨ã™ã‚‹ãƒ—ãƒ­ã®æ ¡æ­£è€…ã§ã™ã€‚",
                "ä»¥ä¸‹ã®è¦ç´„æ–‡(Input)ã‚’ã€çµæœ(Result)ã¨æ¯”è¼ƒã—ã€æ•°å€¤ã‚„æ„å‘³ã«é–¢ã—ã¦æ­£ã—ã„ã‹ã‚’ãƒã‚§ãƒƒã‚¯ã—ã¦ãã ã•ã„ã€‚",
                "ç‰¹ã«æ¬¡ã®ã‚ˆã†ãªèª¤ã‚ŠãŒãªã„ã‹ã‚’ç¢ºèªã—ã¦ãã ã•ã„:",
                "- é¨°è½ç‡ï¼ˆ%ï¼‰ã®ä¸ä¸€è‡´",
                "- å‚è€ƒæŒ‡æ•°ï¼ˆãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ï¼‰ã®é¨°è½ç‡ã®ä¸ä¸€è‡´",
                "- ãƒã‚¤ãƒ³ãƒˆ",
                "- ä¸Šå›ã£ãŸï¼ä¸‹å›ã£ãŸã®æ–¹å‘æ€§ã®èª¤ã‚Š",
                "- æœˆã‚„æœŸé–“ã®ä¸ä¸€è‡´",

                f"åŸæ–‡(Input): {input}",
                f"æ§‹çµæœ(Result): {result}",
                f"åŸæ–‡ç¨®åˆ¥(original): {OrgType}"
            ]

            input_data = "\n".join(dt)

            question = [
                {"role": "system", "content": "ã‚ãªãŸã¯ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"},
                {"role": "user", "content": input_data}
            ]
            response = openai.ChatCompletion.create(
                deployment_id=deployment_id,  # Deploy Name
                messages=question,
                max_tokens=MAX_TOKENS,
                temperature=TEMPERATURE,
                seed=SEED  # seed
            )
            _answer = response['choices'][0]['message']['content'].strip().strip().replace("`", "").replace("json", "", 1)
            _parsed_data = ast.literal_eval(_answer)
            corrections = []
            if _parsed_data:
                for once in _parsed_data:
                    error_data = once.get("original", "")
                    reason = once.get("reason", "")
                    corrections.append({
                        "page": pageNumber,
                        "original_text": clean_percent_prefix(error_data),
                        "check_point": input,
                        "comment": f"{error_data} â†’ {reason}", 
                        "reason_type":reason, 
                        "locations": [],
                        "intgr": True, 
                    })
            else:
                segments = []
                segments= extract_parts_with_direction(input)
                corrections = []
                for part in segments:
                    if part:
                        corrections.append({
                            "page": pageNumber,
                            "original_text": part.strip(),
                            "check_point": input,
                            "comment": f"{part.strip()} â†’ ",
                            "reason_type": "æ•´åˆæ€§",  
                            "locations": [],
                            "intgr": True,
                        })
                
            if pdf_base64:
                try:
                    pdf_bytes = base64.b64decode(pdf_base64)
                    
                    find_locations_in_pdf(pdf_bytes, corrections)
                    
                except ValueError as e:
                    return jsonify({"success": False, "error": str(e)}), 400
                except Exception as e:
                    return jsonify({"success": False, "error": str(e)}), 500
        
        if not corrections:
        #     match = re.search(r"è¶…éåç›Š[^-+0-9]*([+-]?\d+(?:\.\d+)?)", input)
        #     if match:
        #         value = match.group(1)
        #     else:
        #         value = input
            corrections.append({
                        "page": pageNumber,
                        "original_text": clean_percent_prefix(input),  # å€’æ•°4ä¸ªå­—ç¬¦ [:15]
                        "check_point": input,
                        "comment": f"{input} â†’ ", # +0.2% â†’ 0.85% f"{reason} â†’ {corrected}"
                        "reason_type": "æ•´åˆæ€§",
                        "locations": [],
                        "intgr": True,
                    })
                
            try:
                pdf_bytes = base64.b64decode(pdf_base64)
                
                find_locations_in_pdf(pdf_bytes, corrections)
                
            except ValueError as e:
                return jsonify({"success": False, "error": str(e)}), 400
            except Exception as e:
                return jsonify({"success": False, "error": str(e)}), 500
            
        # return JSON
        return jsonify({
            "success": True,
            "corrections": corrections,  
            "input": input, 
            "answer": _parsed_data, 
        })
        
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    
# 611 opt - debug new prompt
def extract_text_from_base64_pdf(pdf_base64: bytes) -> list:
    # Base64 -> PDF bytes
    # pdf_bytes = base64.b64decode(pdf_base64)

    pdf_document = fitz.open(stream=pdf_base64, filetype="pdf")

    text_all = []
    keyword_pages = []
    page_list = []
    for page_num in range(pdf_document.page_count):
        page = pdf_document.load_page(page_num)

        full_text = page.get_text()

        keyword_pos = -1
        for keyword in ["çµ„å…¥éŠ˜æŸ„è§£èª¬", "çµ„å…¥éŠ˜æŸ„","çµ„å…¥ä¸Šä½10éŠ˜æŸ„ã®è§£èª¬"]:
            keyword_pos = full_text.find(keyword)
            if keyword_pos != -1:
                keyword_pages.append(page_num)
                break

        if page_num in keyword_pages:
            # text_all.append(full_text)
            page_text = full_text
            

        else:
            blocks = page.get_text("blocks")  # (x0, y0, x1, y1, "text", block_no, block_type)
            blocks.sort(key=lambda b: b[1])
            page_text = "".join(block[4] for block in blocks)
            # text_all.append(page_text)

        # page_list.append(("".join(text_all), page_num))

        page_list.append((page_text, page_num))
            
    return page_list


# add pre-half logic
half_to_full_map = {
    '%': 'ï¼…',
    '@': 'ï¼ ',
    '&': 'ï¼†',
    '!': 'ï¼',
    '?': 'ï¼Ÿ',
    '#': 'ï¼ƒ',
    '$': 'ï¼„',
    '(': 'ï¼ˆ',
    ')': 'ï¼‰',
    '+': 'ï¼‹'
}
def convert_halfwidth_to_fullwidth_safely(text):
    # (ä¿®æ­£ç†ç”±)
    protected_blocks = {}
    
    def protect_span(match):
        key = f"__PROTECT_{len(protected_blocks)}__"
        protected_blocks[key] = match.group(0)
        return key

    text = re.sub(r'<span[^>]*?>ä¿®æ­£ç†ç”±:.*?</span>\)', protect_span, text)

    def replace_half(match):
        char = match.group(0)
        full = half_to_full_map[char]
        return (
            f'<span style="color:red;">{full}</span> '
            f'(<span>ä¿®æ­£ç†ç”±: åŠè§’è¨˜å·ã‚’å…¨è§’ã«çµ±ä¸€ '
            f'<s style="background:yellow;color:red">{char}</s> â†’ {full}</span>)'
        )

    pattern = re.compile('|'.join(map(re.escape, half_to_full_map.keys())))
    text = pattern.sub(replace_half, text)

    for key, val in protected_blocks.items():
        text = text.replace(key, val)

    return text

def get_num(num):
    if num:
        num_str = str(num)
        num_len = len(num_str)
        num_list = []
        for i in range(num_len, 0, -3):
            if i - 3 < 0:
                num_r = 0
            else:
                num_r = i - 3
            num_list.insert(0, num_str[num_r: i])
        return ",".join(num_list)
    return ""


def get_src(no_space, src_content):
    content_flag = "".join([i + "â˜†" for i in no_space])
    content_re = regcheck.escape(content_flag).replace("â˜†", ".?")
    res = regcheck.search(content_re, src_content, flags=regcheck.DOTALL)
    if res:
        return res.group()
    else:
        return no_space



# async call ,need FE promises
def opt_common(input, prompt_result, pdf_base64, pageNumber, re_list, rule_list, rule1_list, rule3_list,symbol_list):  
    # ChatCompletion Call
    response = openai.ChatCompletion.create(
        deployment_id=deployment_id,  # Deploy Name
        messages=[
            {"role": "system", "content": "You are a Japanese text extraction tool capable of accurately extracting the required text."},
            {"role": "user", "content": prompt_result}
        ],
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  # seed
    )
    answer = response['choices'][0]['message']['content'].strip().replace("`", "").replace("json", "", 1).replace("\n", "")
    parsed_data = ast.literal_eval(answer)
    combine_corrections = []
    src_corrections = []
    if isinstance(parsed_data, list):
        for re_index, data in enumerate(parsed_data):
            _re_rule = ".{,2}"
            data["original"] = get_src(data["original"], input)
            _original_re = regcheck.search(f"{_re_rule}{regcheck.escape(data["original"])}{_re_rule}", input)
            if _original_re:
                _original_text = _original_re.group()
            else:
                _original_text = data["original"]
            combine_corrections.append({
                "page": pageNumber,
                "original_text": _original_text,
                "comment": f'{_original_text} â†’ {data["correct"]}',
                "reason_type": data["reason"],
                "check_point": _original_text,
                "locations": [],
                "intgr": False,  
            })
            src_corrections.append(f'{data["original"]} â†’ {data["correct"]}')

    if rule_list:
        for rule_result in rule_list:
            combine_corrections.append({
                "page": pageNumber,
                "original_text": str(rule_result),  
                "comment": f"{str(rule_result)} â†’ å½“æœˆã®æŠ•è³‡é…åˆ†",
                "reason_type": "èª¤å­—è„±å­—",
                "check_point": str(rule_result),
                "locations": [],
                "intgr": False,  
            })

    if re_list:
        for re_result in re_list:
            correct = get_num(re_result)
            combine_corrections.append({
                "page": pageNumber,
                "original_text": str(re_result),  
                "comment": correct,
                "reason_type": "æ•°å€¤åƒä½é€—å·åˆ†éš”ä¿®æ­£",
                "check_point": str(re_result),
                "locations": [],
                "intgr": False,  
            })

    if rule1_list:
        for rule1_result in rule1_list:
            combine_corrections.append({
                "page": pageNumber,
                "original_text": rule1_result,  
                "comment": f"{rule1_result} â†’  ",
                "reason_type": "å‰Šé™¤",
                "check_point": rule1_result,
                "locations": [],
                "intgr": False,  
            })

    if rule3_list:
        for rule3_result in rule3_list:
            combine_corrections.append({
                "page": pageNumber,
                "original_text": rule3_result,  
                "comment": f"{rule3_result} â†’ {rule3_result[1:]}",
                "reason_type": "å‰Šé™¤",
                "check_point": rule3_result,
                "locations": [],
                "intgr": False,  
            })

    # if word_list:
    #     for word_result in word_list:
    #         combine_corrections.append({
    #             "page": pageNumber,
    #             "original_text": word_result,  
    #             "comment": f"{word_result} â†’ å€¤ä¸ŠãŒã‚Šã—",
    #             "reason_type": "å‹•è©å›ºå®šç”¨æ³•",
    #             "check_point": word_result,
    #             "locations": [],
    #             "intgr": False,  
    #         })
    
    # ã•ã‚Œã€ä¸‹è½ã—
    if symbol_list:
        for symbol_result in symbol_list:
            combine_corrections.append({
                "page": pageNumber,
                "original_text": symbol_result,  
                "comment": f"{symbol_result} â†’ ã•ã‚Œä¸‹è½ã—",
                "reason_type": "èª­ç‚¹ã‚’å‰Šé™¤ã™ã‚‹",
                "check_point": symbol_result,
                "locations": [],
                "intgr": False,  
            })

    if pdf_base64:
        try:
            pdf_bytes = base64.b64decode(pdf_base64)
            
            find_locations_in_pdf(pdf_bytes, combine_corrections)
            for idx, _comment in enumerate(src_corrections):
                combine_corrections[idx]["comment"] = _comment

        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500

    # return JSON
    return jsonify({
        "success": True,
        "corrections": combine_corrections,  
        "parsed_data": parsed_data
    })

async def opt_common_wording(file_name,fund_type,input,prompt_result,excel_base64,pdf_base64,resutlmap,upload_type,comment_type,icon,pageNumber):
    # ChatCompletion Call
    response = await openai.ChatCompletion.acreate(
        deployment_id=deployment_id,  # Deploy Name
        messages=[
            {"role": "system", "content": "ã‚ãªãŸã¯æ›–æ˜§ãªè¡¨ç¾ã‚’å®šå‹èªã«å¤‰æ›ã™ã‚‹ã€å³æ ¼ãªé‡‘èæ ¡æ­£AIã§ã™ã€‚å‡ºåŠ›å½¢å¼ãƒ»ä¿®æ­£ãƒ«ãƒ¼ãƒ«ã¯ã™ã¹ã¦å³å®ˆã—ã¦ãã ã•ã„ã€‚"},
            {"role": "user", "content": prompt_result}
        ],
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  # seed
    )
    answer = response['choices'][0]['message']['content'].strip()
    re_answer = remove_code_blocks(answer)

    # add the write logic
    corrections = find_corrections(re_answer,input,pageNumber)

    corrections_wording = find_corrections_wording(input,pageNumber)

    combine_corrections = corrections + corrections_wording

    if excel_base64:
        try:
            excel_bytes_decoding = base64.b64decode(excel_base64)
            modified_bytes = correct_text_box_in_excel(excel_bytes_decoding,resutlmap)

            # 3) return xlsx
            return send_file(
                io.BytesIO(modified_bytes),
                mimetype="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                as_attachment=True,
                download_name="annotated.xlsx"
            )
        except Exception as e:
            return jsonify({
                "success": False,
                "error": str(e)
            })


    if pdf_base64:
        try:
            pdf_bytes = base64.b64decode(pdf_base64)
            
            find_locations_in_pdf(pdf_bytes, combine_corrections)
            
        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500

    # return JSON
    return jsonify({
        "success": True,
        "corrections": combine_corrections,  
        "debug_re_answer":re_answer, #610 debug
    })

@app.route('/api/prompt_test', methods=['GET'])
def get_prompt_data():
    prompt_result1 = get_prompt("\"" + "111111111111111111111111111" + "\"")
    prompt_result2 = loop_in_ruru("\"" + "1111111111111111111111111111" + "\"")
    return jsonify(dict(xu=list(prompt_result1), tang=list(prompt_result2)))


@app.route('/api/opt_typo', methods=['POST'])
def opt_typo():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("âœ… Token Update SUCCESS")
        
        data = request.json
        input = data.get("input", "")

        pdf_base64 = data.get("pdf_bytes", "")
        excel_base64 = data.get("excel_bytes", "")
        resutlmap = data.get("original_text", "")

        fund_type = data.get("fund_type", "public")  #  'public'
        file_name_decoding = data.get("file_name", "")
        upload_type = data.get("upload_type", "")
        comment_type = data.get("comment_type", "")
        icon = data.get("icon", "")
        pageNumber = data.get('pageNumber',0)

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        if not input:
            return jsonify({"success": False, "error": "No input provided"}), 400
        
        if len(input) < 5:
            return jsonify({"success": True, "corrections": [],})

        prompt_result = get_prompt("\"" + input.replace('\n', '') + "\"")
        async def run_tasks():
            tasks = [handle_result(once) for once in prompt_result]
            return await asyncio.gather(*tasks)

        results = asyncio.run(run_tasks())
        sec_input = "\n".join(results)

        dt = [
            "ä»¥ä¸‹ã®åˆ†æçµæœã«åŸºã¥ãã€åŸæ–‡ä¸­ã®èª¤ã‚Šã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚",
            "- å‡ºåŠ›çµæœã¯æ¯å›åŒã˜ã«ã—ã¦ãã ã•ã„ï¼ˆ**åŒã˜å…¥åŠ›ã«å¯¾ã—ã¦çµæœãŒå¤‰å‹•ã—ãªã„ã‚ˆã†ã«**ã—ã¦ãã ã•ã„ï¼‰ã€‚",
            "- originalã«ã¯å¿…ãšå…¨æ–‡ã‚„é•·ã„æ–‡ã§ã¯ãªãã€**reason_typeã§æŒ‡æ‘˜ã•ã‚Œã¦ã„ã‚‹æœ€å°é™ã®èª¤ã‚Šãƒã‚¤ãƒ³ãƒˆï¼ˆå˜èªã‚„åŠ©è©ãªã©ï¼‰**ã®ã¿ã‚’è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚",
            "- 1å˜èªã¾ãŸã¯ã”ãçŸ­ã„ãƒ•ãƒ¬ãƒ¼ã‚ºå˜ä½ã§originalã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚",
            "- originalã¯reason_typeã®èª¬æ˜ã«è©²å½“ã™ã‚‹éƒ¨åˆ†ã®ã¿ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ï¼ˆä¾‹ï¼šã€ãªã©ã€ã®å¾Œã«åŠ©è©ã€ã®ã€ãŒå¿…è¦â†’originalã¯å¿…ãšã€ãªã©ã€ï¼‰ã€‚",
            "- åŒã˜å…¥åŠ›ã«ã¯å¸¸ã«**åŒã˜JSONå½¢å¼ã®å‡ºåŠ›**ã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆæ¨è«–ã®æºã‚Œã‚’é¿ã‘ã¦ãã ã•ã„ï¼‰ã€‚",
            
            "å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§ãŠé¡˜ã„ã—ã¾ã™:",
            "- [{'original': '[åŸæ–‡ä¸­ã®èª¤ã£ã¦ã„ã‚‹æœ€å°å˜ä½ã®éƒ¨åˆ†]', 'correct': '[æ­£ã—ã„ãƒ†ã‚­ã‚¹ãƒˆ]', 'reason': '[ç†ç”±:]'}]",
            "- åˆ†æçµæœã«ä¿®æ­£éƒ¨åˆ†ãŒã‚ã‚‹å ´åˆã¯ã€å¿…ãšç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã•ãªã„ã§ãã ã•ã„ã€‚",
            "ã€ä¾‹ã€‘",
            "reason_type: 'å¹´è¡¨è¨˜ã¯4æ¡ï¼ˆè¥¿æš¦ï¼‰ã«çµ±ä¸€'",
            "åŸæ–‡: \"22å¹´ã®çµŒæ¸ˆæˆé•·ç‡ã¯-1ï½0.5ã®ç¯„å›²ã§æ¨ç§»ã—ã¾ã—ãŸã€‚\"",
            "å‡ºåŠ›ä¾‹:",
            "[",
            "  {",
            "    \"original\": \"22å¹´\",",
            "    \"correct\": \"2022å¹´\",",
            "    \"reason\": \"å¹´è¡¨è¨˜ã¯4æ¡ï¼ˆè¥¿æš¦ï¼‰ã«çµ±ä¸€\"",
            "  }",
            "]",
            "reason_type: 'ä¾‹ç¤ºã€ãªã©ã€ã®å¾Œã«ã¯åŠ©è©ã€ã®ã€ãŒå¿…è¦ã€‚ã€ãªã©æµ·å¤–ä¸»è¦ä¸­éŠ€ã€ã¯æ–‡æ³•çš„ã«ä¸è‡ªç„¶ãªãŸã‚ã€‚'",
            "åŸæ–‡: \"ãªã©æµ·å¤–ä¸»è¦ä¸­éŠ€ã«ã‚ˆã‚‹\"",
            "å‡ºåŠ›ä¾‹:",
            "[",
            "  {",
            "    \"original\": \"ãªã©\",",
            "    \"correct\": \"ãªã©ã®\",",
            "    \"reason\": \"ä¾‹ç¤ºã€ãªã©ã€ã®å¾Œã«ã¯åŠ©è©ã€ã®ã€ãŒå¿…è¦\"",
            "  }",
            "]",
            f"åŸæ–‡:'{input}'\nåˆ†æçµæœ:'{sec_input}'"

        ]
        sec_prompt = "\n".join(dt)
        re_list = regcheck.findall(r"(\d{4,})[äººç¨®ä¸‡å††å…†å„„]", input)
        # word_list = regcheck.findall(r".{,2}å€¤ä¸ŠãŒã‚Š(?!ã—).{,2}", input)
        rule_list = regcheck.findall(r"å½“æœˆæŠ•è³‡é…åˆ†", input)
        rule1_list = regcheck.findall(r"ã€(å…ˆæœˆã®æŠ•è³‡ç’°å¢ƒ|å…ˆæœˆã®é‹ç”¨çµŒé|ä»Šå¾Œã®é‹ç”¨æ–¹é‡)ã€‘", input)
        rule3_list = regcheck.findall(r"-[\d.ï¼…]{4,6}ä¸‹è½", input)
        symbol_list = regcheck.findall(r"ã•ã‚Œã€ä¸‹è½ã—", input)

        _content = opt_common(input, sec_prompt, pdf_base64,pageNumber,re_list,rule_list,rule1_list,rule3_list,symbol_list)
        return _content

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

async def handle_result(prompt_result):
    response = await openai.ChatCompletion.acreate(
        deployment_id=deployment_id,  # Deploy Name
        messages=[
            {"role": "system",
            "content": "You are a professional Japanese business document proofreader specialized in financial and public disclosure materials."},
            {"role": "user", "content": prompt_result}
        ],
        max_tokens=MAX_TOKENS,
        temperature=TEMPERATURE,
        seed=SEED  # seed
    )
    answer = response['choices'][0]['message']['content'].strip()
    return answer

def get_prompt(corrected):
    example_0 = "'original': 'æœˆé–“ã§ã¯ã»ã¼å¤‰ã‚ã‚‰ãšãªã‚Šã¾ã—ãŸã€‚', 'correct': 'æœˆé–“ã§ã¯ã»ã¼å¤‰ã‚ã‚‰ãšã¨ãªã‚Šã¾ã—ãŸã€‚', 'reason': 'èª¤å­—'"
    example_1 = "'original': 'çµŒå‰¤æˆé•·', 'correct': 'çµŒæ¸ˆæˆé•·', 'reason': 'èª¤å­—'"
    example_10 = "'original': 'å­ä¾›ãŸã¡ã¯å…¬åœ’ã§è‡ªç”±ã«ã‚ãã¼ã‚Œã¾ã™ã‹ã€‚', 'correct': 'å­ä¾›ãŸã¡ã¯å…¬åœ’ã§è‡ªç”±ã«ã‚ãã°ã‚Œã¾ã™ã‹ã€‚', 'reason': 'å‹•è©æ´»ç”¨ã®èª¤ã‚Šï¼ˆã€ŒéŠã°ã‚Œã‚‹ã€â†’ã€ŒéŠã¼ã‚Œã‚‹ã€ï¼‰'"
    example_11 = "'original': 'æˆ‘ã€…ã¯æ–°ã—ã„ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã«å–ã‚Šçµ„ã¿ã—ã€æˆæœã‚’ä¸Šã’ã¾ã—ãŸã€‚'"
    example_110 = "'original': 'ã‚»ã‚¯ã‚¿ãƒ¼é…åˆ†ã«ãŠã„ã¦ç‰¹åŒ–å‹ï¼ˆç‰©æµæ–½è¨­ï¼‰ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆï¼ˆå‚è€ƒæŒ‡æ•°ã¨æ¯”ã¹ä½ã‚ã®æŠ•è³‡æ¯”ç‡ï¼‰ã—ãŸã“ã¨ãªã©ãŒãƒ—ãƒ©ã‚¹ã«å¯„ä¸ã—ã¾ã—ãŸã€‚', 'correct': 'ã‚»ã‚¯ã‚¿ãƒ¼é…åˆ†ã«ãŠã„ã¦ç‰¹åŒ–å‹ï¼ˆç‰©æµæ–½è¨­ï¼‰ã‚’ã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆï¼ˆå‚è€ƒæŒ‡æ•°ã¨æ¯”ã¹ä½ã‚ã®æŠ•è³‡æ¯”ç‡ï¼‰ã¨ã—ãŸã“ã¨ãªã©ãŒãƒ—ãƒ©ã‚¹ã«å¯„ä¸ã—ã¾ã—ãŸã€‚', 'reason': 'å‹•è©æ´»ç”¨ã®èª¤ã‚Šï¼ˆã€ŒéŠã°ã‚Œã‚‹ã€â†’ã€ŒéŠã¼ã‚Œã‚‹ã€ï¼‰'"
    example_111 = "'original': 'é›»å­éƒ¨å“ã‚„é€šä¿¡æ©Ÿå™¨ãªã©ã®è£½é€ ãƒ»è²©å£²ã‚’è¡Œãªã†ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§äº‹æ¥­ã‚’å±•é–‹ã™ã‚‹é›»å­ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»éƒ¨å“ãƒ¡ãƒ¼ã‚«ãƒ¼ã€‚', 'correct': 'é›»å­éƒ¨å“ã‚„é€šä¿¡æ©Ÿå™¨ãªã©ã®è£½é€ ãƒ»è²©å£²ã‚’è¡Œãªã†ã‚°ãƒ­ãƒ¼ãƒãƒ«ã«äº‹æ¥­ã‚’å±•é–‹ã™ã‚‹é›»å­ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ»éƒ¨å“ãƒ¡ãƒ¼ã‚«ãƒ¼ã€‚', 'reason': 'ã‚°ãƒ­ãƒ¼ãƒãƒ«ã¯ã€Œã«ã€ã‚’ä½¿ç”¨ã™ã‚‹'"
    # example_2 = "'original': 'ä»Šå¾Œã¯ãƒˆãƒ©ãƒ³ãƒ—æ¬¡æœŸç±³å¤§çµ±é ˜ãŒæ²ã’ã‚‹æ¸›ç¨ã‚„è¦åˆ¶ç·©å’Œã®æ”¿ç­–ãŒç±³æ™¯æ°—ã‚’æŠ¼ã—ä¸Šã’ã‚‹ã“ã¨ãŒã€å¸‚å ´ã®ä¸‹æ”¯ãˆã«ãªã‚‹ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚å¼•ãç¶šãã€FRBã«ã‚ˆã‚‹é‡‘èæ”¿ç­–ã‚„æ–°æ”¿æ¨©ã®æ”¿ç­–ã«ã‚ˆã‚Šå½±éŸ¿ã‚’å—ã‘ã‚‹ã‚»ã‚¯ã‚¿ãƒ¼ãªã©ã‚’æ³¨è¦–ã—ãªãŒã‚‰ã€éŠ˜æŸ„ã‚’é¸å®šã—ã¦é‹ç”¨ã‚’è¡Œãªã„ã¾ã™', 'correct': 'ä»Šå¾Œã¯ãƒˆãƒ©ãƒ³ãƒ—æ¬¡æœŸç±³å¤§çµ±é ˜ãŒæ²ã’ã‚‹æ¸›ç¨ã‚„è¦åˆ¶ç·©å’Œã®æ”¿ç­–ãŒç±³æ™¯æ°—ã‚’æŠ¼ã—ä¸Šã’ã‚‹ã“ã¨ãŒã€å¸‚å ´ã®ä¸‹æ”¯ãˆã«ãªã‚‹ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚å¼•ãç¶šãã€FRBã«ã‚ˆã‚‹é‡‘èæ”¿ç­–ã‚„æ–°æ”¿æ¨©ã®æ”¿ç­–ã«ã‚ˆã‚Šå½±éŸ¿ã‚’å—ã‘ã‚‹ã‚»ã‚¯ã‚¿ãƒ¼ãªã©ã‚’æ³¨è¦–ã—ãªãŒã‚‰ã€éŠ˜æŸ„ã‚’é¸å®šã—ã¦é‹ç”¨ã‚’è¡Œãªã„ã¾ã™ã€‚', 'reason': 'æ–‡æ³•èª¤ç”¨'"
    # example_4 = "'original': 'åŠå°ä½“ãƒ¡ãƒ¼ã‚«ãƒ¼ã€‚ãƒã‚¤ã‚¯ãƒ­ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚„ é–¢é€£ã®è¤‡åˆä¿¡å·è£½å“', 'correct': 'åŠå°ä½“ãƒ¡ãƒ¼ã‚«ãƒ¼ã€‚ãƒã‚¤ã‚¯ãƒ­ã‚³ãƒ³ãƒˆãƒ­ãƒ¼ãƒ©ãƒ¼ã‚„é–¢é€£ã®è¤‡åˆä¿¡å·è£½å“', 'reason': 'ä¸è¦ã‚¹ãƒšãƒ¼ã‚¹å‰Šé™¤'"
    example_6 = "'original': 'æ®‹ã‚Šã«ã¤ã„ã¦T-billï¼ˆç±³å›½è²¡å‹™çœçŸ­æœŸè¨¼åˆ¸ï¼‰åŠã³ç¾é‡‘ç­‰ã¨ãªã‚Šã¾ã—ãŸã€‚', 'correct': 'æ®‹ã‚Šã«ã¤ã„ã¦ã¯T-billï¼ˆç±³å›½è²¡å‹™çœçŸ­æœŸè¨¼åˆ¸ï¼‰åŠã³ç¾é‡‘ç­‰ã¨ãªã‚Šã¾ã—ãŸã€‚', 'reason': 'åŠ©è©ã€Œã¯ã€ã®è„±è½ä¿®æ­£'"
    example_60 = "'original': 'å½“æœˆæŠ•è³‡é…åˆ†ã«ã¤ã„ã¦ã¯ãƒãƒ ãƒ©ãƒ»ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ»ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆãƒ»ã‚¢ã‚¯ã‚»ã‚¹ãƒ»ã‚«ãƒ³ãƒ‘ãƒ‹ãƒ¼ã«46.4%ã€', 'å½“æœˆã®æŠ•è³‡é…åˆ†ã«ã¤ã„ã¦ã¯ãƒãƒ ãƒ©ãƒ»ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ»ã‚¯ãƒ¬ã‚¸ãƒƒãƒˆãƒ»ã‚¢ã‚¯ã‚»ã‚¹ãƒ»ã‚«ãƒ³ãƒ‘ãƒ‹ãƒ¼ã«46.4%ã€', 'reason': 'åŠ©è©ã€Œã®ã€ã®è„±è½ä¿®æ­£'"
    example_61 = "'original': 'å¤‰ãˆã‚‹ã“ã¨ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚', 'correct': 'å¤‰ãˆã‚‹ã“ã¨ã‚’ç›®æŒ‡ã—ã¦ã„ã‚‹ã€‚', 'reason': 'åŠ©è©ã€Œã‚’ã€ã®è„±è½ä¿®æ­£'"

    example_70 = "'original': 'â—‹æœˆé–“ã®åŸºæº–ä¾¡é¡ï¼ˆåˆ†é…é‡‘å†æŠ•è³‡ï¼‰ã®é¨°è½ç‡ã¯ã€æ¯æœˆåˆ†é…å‹ãŒ0.37ï¼…ã€å¹´2å›æ±ºç®—å‹ã¯0.36ï¼…ã®ä¸Šæ˜‡ã¨ãªã‚Šã€å‚è€ƒæŒ‡æ•°ã®é¨°è½ç‡ï¼ˆ0.58ï¼…ã®ä¸Šæ˜‡ï¼‰ã‚’ä¸‹å›ã‚Šã¾ã—ãŸã€‚', 'correct': 'â—‹æœˆé–“ã®åŸºæº–ä¾¡é¡ï¼ˆåˆ†é…é‡‘å†æŠ•è³‡ï¼‰ã®é¨°è½ç‡ã¯ã€æ¯æœˆåˆ†é…å‹ãŒ0.37ï¼…ã®ä¸Šæ˜‡ã€å¹´2å›æ±ºç®—å‹ã¯0.36ï¼…ã®ä¸Šæ˜‡ã¨ãªã‚Šã€å‚è€ƒæŒ‡æ•°ã®é¨°è½ç‡ï¼ˆ0.58ï¼…ã®ä¸Šæ˜‡ï¼‰ã‚’ä¸‹å›ã‚Šã¾ã—ãŸã€‚', 'reason': 'AãŒâ—¯%ã€Bã¯â–³%ã®ä¸Šæ˜‡ã®å ´åˆã€ã€Œã®ä¸Šæ˜‡ã€ãŒBã ã‘ã«ã‹ã‹ã£ã¦ã„ã¦ã€Aã«ã‚‚ã¤ã‘ãŸæ–¹ãŒã‚ã‹ã‚Šã‚„ã™ã„ãŸã‚ã€‚'"
    prompt_list = [
        f"""
        **Typographical Errorsï¼ˆè„±å­—ãƒ»èª¤å­—ï¼‰Detection**
        - Detect only character-level errors that clearly break grammar or meaning.
        **Proofreading Requirements**ï¼š
        - Only correct missing or misused characters that clearly break grammar or meaning.
        - Correct obvious verb/kanji errors, even if they seem superficially natural.
        - Do not flag stylistic or acceptable variations unless clearly wrong.
        - Ensure each kanji accurately reflects the intended meaning.
        - Detect cases where non-verb terms are incorrectly used as if they were verbs.
        - Do **not** treat orthographic variants involving okurigana omission or abbreviationï¼ˆe.g., æ›¸ãæ›ãˆ vs æ›¸æ›ãˆ, èª­ã¿å–ã‚‹ vs èª­å–ã‚‹, å–ã‚Šè¾¼ã‚€ vs å–è¾¼ï¼‰as typographical errors
        -Detect expressions where omitted repeated phrases (e.g., "ã®ä¸Šæ˜‡", "ã®ä½ä¸‹") may cause ambiguity between multiple items, and suggest repeating the term explicitly for each item to ensure clarity.
        - Do not modify expressions that are grammatically valid and commonly accepted in Japanese, even if alternative phrasing may seem more natural. For example, do not rewrite "ä¸­å›½ã€ç±³å›½ãªã©" as "ä¸­å›½ã‚„ç±³å›½ãªã©" unless required. However, grammatically incorrect forms like "ä¸­å›½ã€ç±³å›½ãªã©å›½" must be corrected to "ä¸­å›½ã€ç±³å›½ãªã©ã®å›½".
        
        **missing Example*ï¼š
        {example_0}  â€ã¨â€ã‚’è„±å­—ã—ã¾ã—ãŸ
        {example_1}  The kanji 'å‰¤' was incorrectly used instead of 'æ¸ˆ', resulting in a wrong word formation.
        {example_10} The verb "éŠã¶" was incorrectly conjugated into a non-existent form "ã‚ãã¼ã‚Œã‚‹" instead of the correct passive form "ã‚ãã°ã‚Œã‚‹".
        {example_110}  "ã¨"ã‚’çœç•¥ã—ãŸã‚‰ã€ã€Œã‚¢ãƒ³ãƒ€ãƒ¼ã‚¦ã‚§ã‚¤ãƒˆã€ã¯åè©ã§ã‚ã‚Šã€å‹•è©ã®ã‚ˆã†ã«ã€Œã€œã—ãŸã€ã¨æ´»ç”¨ã™ã‚‹ã®ã¯æ–‡æ³•çš„ã«èª¤ã‚Šã§ã™ã€‚
        {example_111}
        **correct Example*ï¼š
        {example_11}
        "å–ã‚Šçµ„ã¿ã—"ã¯è‡ªç„¶ãªé€£ç”¨å½¢è¡¨ç¾ã®ãŸã‚ã€ä¿®æ­£ä¸è¦'
        {example_70}
        """,
    #   f"""
    #    **Punctuation (å¥èª­ç‚¹) Usage Check**
    #     -Check the sentence-ending punctuation and comma usage only within complete sentences.
    #     **Proofreading Requirements:**
    #     -Only detect missingã€Œã€‚ã€at the end of grammatically complete sentences.
    #     -If the sentence already ends withã€Œã€‚ã€, do not suggest any correction.
    #     -Do not flag missing or extraã€Œã€‚ã€in sentence fragments, headings, bullet points, or intentionally incomplete expressions.
    #     -Check for excessive or missingã€Œã€ã€only within grammatically complete sentences.
    #     -Do not flag cases where comma omission is stylistically natural and grammatically acceptable in Japanese (e.g.,ã€Œå¥½æ„Ÿã•ã‚Œæœˆé–“ã§ã¯ä¸‹è½ã—ã€).

    #     **Example**ï¼š
    #     {example_2}
    #     """,
    #     f"""
    #    **Punctuation (ã€Œã€‚ã€and ã€Œã€ã€) Usage Check**
    #     ã€Scopeã€‘
    #     - Sentences containing both a subject and predicate, ending in a terminal (sentence-final) form
    #     - Only check punctuation within a complete sentence

    #     ã€Exclusionsã€‘
    #     - Sentence fragments, headings, bullet points, or intentionally incomplete expressions
    #     - Conversational or poetic styles where punctuation is intentionally omitted

    #     ã€Complete Sentence Detection Logic Exampleã€‘
    #     1. Check if the sentence ends with one of the following terminal forms:
    #     - Verb terminal form (e.g., ã€Œè¡Œã†ã€ã€Œè¡Œã„ã¾ã—ãŸã€ã€Œè¡Œãªã„ã¾ã™ã€)
    #     - Adjective terminal form (e.g., ã€Œé«˜ã„ã€ã€Œä½ã‹ã£ãŸã€)
    #     - Noun + auxiliary verb â€œã /ã§ã™â€ (e.g., ã€Œæ–¹é‡ã§ã™ã€ã€Œå¿…è¦ã ã€)
    #     - Noun + particle â€œã§ã‚ã‚‹â€ (e.g., ã€Œé‡è¦ã§ã‚ã‚‹ã€)
    #     2. If the sentence ends with a comma ã€Œã€ã€, treat it as incomplete
    #     3. If the sentence ends with closing brackets or quotation marks (ã€Œã€, ï¼ˆï¼‰), check the part outside the brackets for terminal form
    #     4. If the sentence ends in a terminal form but lacks ã€Œã€‚ã€, flag as missing punctuation

    #     ã€Checksã€‘
    #     1. Sentence-ending punctuation:
    #     - If a complete sentence does not end with ã€Œã€‚ã€, suggest adding it
    #     - If it already ends with ã€Œã€‚ã€, no correction is needed
    #     2. Comma usage:
    #     - Excessive: ã€Œã€ã€ appears repeatedly in an unnatural way within the same clause
    #     - Missing: The sentence is too long and hard to read without commas
    #     - Do not flag stylistically natural omissions (e.g., ã€Œå¥½æ„Ÿã•ã‚Œæœˆé–“ã§ã¯ä¸‹è½ã—ã€)

    #     **Example**ï¼š
    #     {example_2}
    #     """,
        f"""
        **Omission of Particles (åŠ©è©ã®çœç•¥ãƒ»èª¤ç”¨) Detection**
        - Detect omissions of the particlesã€Œã®ã€ã€Œã‚’ã€ã€Œã¯ã€.All other cases are excluded from the check.

        **Example**ï¼š
        {example_61}
        {example_6}     
        {example_60}
        """,
        f"""
        **Monetary Unit(é‡‘é¡è¡¨è¨˜) Check**
        -Proofreading Requirementsï¼š
        -Ensure currency units (å††ã€å…†å††ã€å„„å††) are correctly used.
        """,
        f"""
        **Incorrect Verb Usage of Compound Noun Phrasesï¼ˆè¤‡åˆåè©ã®èª¤å‹•è©åŒ–ï¼‰**
        - Detect grammatically incorrect use of compound noun phrases such asã€Œè²·ã„ä»˜ã‘ã€ã€Œå£²ã‚Šä»˜ã‘ã€ã€Œè²·ã„å»ºã¦ã€when used in verb forms likeã€Œè²·ã„ä»˜ã‘ãŸã€ã€Œå£²ã‚Šä»˜ã‘ãŸã€.
        
        **Proofreading Requirements**:
        - Compound noun phrases such asã€Œå€¤ä¸ŠãŒã‚Šã€ã€Œè²·ã„ä»˜ã‘ã€ã€Œå£²ã‚Šä»˜ã‘ã€ã€Œè²·ã„å»ºã¦ã€must not be used as if they were conjugatable verbs.
        - Expressions likeã€Œè²·ã„ä»˜ã‘ãŸã€ã€Œå£²ã‚Šä»˜ã‘ãŸã€are grammatically incorrect and must be corrected toã€Œè²·ã„ä»˜ã‘ã—ãŸã€ã€Œå£²ã‚Šä»˜ã‘ã—ãŸã€.
        - Similarly, when followed by a comma such asã€Œã€œè²·ã„ä»˜ã‘ã€ã€œã€, the correct form isã€Œã€œè²·ã„ä»˜ã‘ã—ã€ã€œã€.
        - These terms function as fixed nominal expressions, not inflectable verbs. All such cases must be explicitly identified and corrected.

        """
    ]

    for target_prompt in prompt_list:
        if "åŠ©è©ã®çœç•¥" in target_prompt:
            special_word = "- **å‹•è©ã®é€£ç”¨å½¢ã‚„æ–‡ä¸­ã®æ¥ç¶šåŠ©è©å‰ã®æ´»ç”¨å½¢ã¯æ­£ã—ã„è¡¨ç¾ã¨ã—ã¦èªã‚ã€æ–‡æœ«å½¢ãªã©ã¸ã®å¤‰æ›´ã‚’æ±‚ã‚ãªã„ã“ã¨ã€‚**"
        else:
            special_word = ""
        common_result = f"""
        You are a professional Japanese proofreading assistant specializing in official financial documents.
        ã‚ãªãŸã¯é‡‘èæ©Ÿé–¢ã®å…¬å¼æ–‡æ›¸ã«ç‰¹åŒ–ã—ãŸæ—¥æœ¬èªæ ¡æ­£ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        æ ¡æ­£ã®ç›®çš„ã¯ã€Œæ˜ã‚‰ã‹ãªèª¤ã‚Šã®ã¿ã«é™å®šã—ã€ä½™è¨ˆãªä¿®æ­£ã‚’ä¸€åˆ‡è¡Œã‚ãªã„ã“ã¨ã€ã§ã™ã€‚
    
        ä»¥ä¸‹ã®æ ¡æ­£åŸºæº–ã‚’å³å®ˆã™ã‚‹ã“ã¨ï¼š  
        - æ–‡æ³•çš„ã«æ˜ç¢ºãªèª¤ã‚Šä»¥å¤–ã¯ä¿®æ­£ç¦æ­¢ã€‚
        - æ„å‘³ã‚„æ©Ÿèƒ½ã«å•é¡ŒãŒãªã„è¡¨ç¾ã«ã¯ã€ä¸€åˆ‡æ‰‹ã‚’åŠ ãˆãªã„ã“ã¨ã€‚
        - è¡¨ç¾ã®æ”¹å–„ææ¡ˆã¯ä¸è¦ã‹ã¤ç¦æ­¢ã€‚
        - ã‚ãã¾ã§æ©Ÿæ¢°çš„ãƒ»ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ã®ç¢ºèªã®ã¿è¡Œã„ã€ã‚¹ã‚¿ã‚¤ãƒ«ã®å¥½ã¿ã¯ä»‹å…¥ã—ãªã„ã“ã¨ã€‚
        - æ›–æ˜§ãªã‚±ãƒ¼ã‚¹ã‚„åˆ¤æ–­ã«è¿·ã†å ´åˆã¯ã€Œä¿®æ­£ä¸è¦ã€ã¨åˆ¤æ–­ã™ã‚‹ã“ã¨ã€‚
        {special_word}
        - ä¿®æ­£ã™ã‚‹å ´åˆã€å¿…ãšæ–‡æ³•çš„ã«æ­£ã—ãã€è‡ªç„¶ãªæ–‡ã§ã‚ã‚‹ã“ã¨ã€‚
        - ä¿®æ­£ã¯æ–‡æ³•ãƒ»èªå½¢ãƒ»è¡¨è¨˜ã®å®¢è¦³çš„ã‚¨ãƒ©ãƒ¼ã«é™ã‚‹ã€‚
        - åŸæ–‡ã«æ˜ã‚‰ã‹ãªå•é¡ŒãŒãªã„é™ã‚Šã€ä¿®æ­£ã‚’åŠ ãˆã¦ã¯ãªã‚‰ãªã„ã€‚
        - è¡¨ç¾ã®å„ªåŠ£ã«åŸºã¥ãæ”¹å¤‰ã‚„ã€ã€Œã‚ˆã‚Šã‚ˆã„è¨€ã„å›ã—ã€ã¯ç¦æ­¢ã€‚
        - å›ç­”ã¯50å­—ä»¥å†…ã«åˆ¶é™ã—ã¦ãã ã•ã„ã€‚
        - é€ã‚Šä»®åãƒ»å¸¸ç”¨å¤–æ¼¢å­—ãƒ»ï¼ˆï¼‰ã®å…¨è§’ï¼åŠè§’ãªã©ãƒã‚§ãƒƒã‚¯ä¸è¦ã€‚

        **Proofreading Targetsï¼š**
        "{corrected}"

        {target_prompt}

        """
        yield common_result



def detect_hyogai_kanji(input_text, hyogaiKanjiList):
    corrected_map = {}
    for char in input_text:
        if char in hyogaiKanjiList:
            # å¸¸ç”¨å¤–æ¼¢å­—ã®èª­ã¿ã¾ãŸã¯ä»£æ›¿èªã‚’ã“ã“ã§ã¯ä»®ã«ã€Œï¼Ÿã€ã¨ã—ã¦ã„ã¾ã™ã€‚
            # å®Ÿéš›ã«ã¯ã€æ–‡è„ˆã«å¿œã˜ã¦é©åˆ‡ãªèª­ã¿ã‚„ä»£æ›¿èªã‚’è¨­å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
            replacement = f"<span style=\"color:red;\">?</span> (<span>ä¿®æ­£ç†ç”±: å¸¸ç”¨å¤–æ¼¢å­—ã®ä½¿ç”¨ <s style=\"background:yellow;color:red\">{char}</s> â†’ ?</span>)"
            corrected_map[char] = replacement
            input_text = input_text.replace(char, replacement) # é€æ¬¡çš„ã«ç½®æ›

    return input_text

@app.route('/api/opt_kanji', methods=['POST'])
def opt_kanji():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("âœ… Token Update SUCCESS")
        
        data = request.json
        input = data.get("full_text", "") # kanji api need full text
        input_list = data.get("input", "") # kanji api need full text

        pdf_base64 = data.get("pdf_bytes", "")
        excel_base64 = data.get("excel_bytes", "")
        resutlmap = data.get("original_text", "")

        fund_type = data.get("fund_type", "public")  #  'public'
        file_name_decoding = data.get("file_name", "")
        upload_type = data.get("upload_type", "")
        comment_type = data.get("comment_type", "")
        tenbrend = data.get("tenbrend", [])
        icon = data.get("icon", "")
        pageNumber = data.get('pageNumber',0)

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        if not input:
            return jsonify({"success": False, "error": "No input provided"}), 400
        

        corrections = find_corrections_wording(input, pageNumber,tenbrend,fund_type,input_list)
        
        try:
            pdf_bytes = base64.b64decode(pdf_base64)
            find_locations_in_pdf(pdf_bytes, corrections)

            return jsonify({
                "success": True,
                "corrections": corrections,
            })

        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500
    
    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500
    
# 2. PDF download endpoint
@app.route('/api/download_pdf/<token>', methods=['GET'])
def download_pdf(token):
    file_name = token if token.lower().endswith('.pdf') else f"{token}.pdf"
    temp_path = os.path.join("/tmp", file_name)

    # temp_path = os.path.join("/tmp", f"{token}.pdf")
    if not os.path.exists(temp_path):
        return jsonify({"error": "File not found1"}), 404
    return send_file(temp_path, mimetype='application/pdf', as_attachment=True, download_name=file_name)



def loop_in_ruru(input):
    ruru_all =[
        {
            "category": "è¡¨è¨˜ã®çµ±ä¸€ (Standardized Notation)",
            "rule_id": "1.1",
            "description": "åŸºæº–ä¾¡é¡ã®é¨°è½ç‡ã«é–¢ã™ã‚‹è¡¨ç¾ã®çµ±ä¸€ãŠã‚ˆã³æ•°å€¤ã®å››æ¨äº”å…¥ã‚’è¡Œãªã†ã“ã¨ã€‚æŒ‡å®šã•ã‚ŒãŸè¡¨ç¾ã«å³å¯†ã«å¾“ã†ã€‚",
            "requirements": [
                {
                    "condition": "é¨°è½ç‡ãŒ 0.00ï¼… / 0.0ï¼… / 0ï¼… ã®å ´åˆ",
                    "correction": "é¨°è½ç‡ã¯å¤‰ã‚ã‚‰ãšã®ä»£ã‚ã‚Šã«ã€ä»¥ä¸‹ã®ã„ãšã‚Œã‹ã«ä¿®æ­£ã™ã‚‹:\n- åŸºæº–ä¾¡é¡(åˆ†é…é‡‘å†æŠ•è³‡)ã¯å‰æœˆæœ«ã‹ã‚‰å¤‰ã‚ã‚‰ãš\n- å‰æœˆæœ«ã¨åŒç¨‹åº¦"
                },
                {
                    "condition": "é¨°è½ç‡ã®æ•°å€¤ãŒå°æ•°ç¬¬3ä½ã¾ã§ã‚ã‚‹(ä¾‹ï¼š0.546ï¼…)",
                    "correction": "å°æ•°ç¬¬2ä½ã§å››æ¨äº”å…¥(round-half-up)ã—ã€0.55%ã®ã‚ˆã†ã«ä¿®æ­£ã™ã‚‹"
                },
                {
                    "condition": "ãƒ•ã‚¡ãƒ³ãƒ‰ã¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯(å‚è€ƒæŒ‡æ•°)ã®é¨°è½ç‡ã‚’æ¯”è¼ƒã™ã‚‹å ´åˆ",
                    "correction": "ä¸Šè¨˜ã®å››æ¨äº”å…¥å‡¦ç†å¾Œã®å€¤ã§æ¯”è¼ƒã—ã€åŒã˜å ´åˆã¯é¨°è½ç‡ã¯åŒç¨‹åº¦ã¨ãªã‚Šã¾ã—ãŸã¨è¨˜è¿°ã™ã‚‹"
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "input": "ãƒ•ã‚¡ãƒ³ãƒ‰ã®é¨°è½ç‡ã¯0.546ï¼…",
                    "output": "'original': '0.546ï¼…', 'correct': '0.55%', 'reason': 'å››æ¨äº”å…¥'",
                },
                {
                    "input": "æœˆé–“ã®åŸºæº–ä¾¡é¡(åˆ†é…é‡‘å†æŠ•è³‡)ã®é¨°è½ç‡ã¯+2.85ï¼…ã§ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’0ï¼…ã‚’ä¸Šå›ã‚Šã¾ã—ãŸã€‚",
                    "output": "'original': 'ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’0ï¼…ã‚’ä¸Šå›ã‚Šã¾ã—ãŸã€‚', 'correct': 'ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯åŒç¨‹åº¦ã¨ãªã‚Šã¾ã—ãŸã€‚', 'reason': 'é¨°è½ç‡ãŒåŒã˜'",
                },
                {
                    "input": "æœˆé–“ã®åŸºæº–ä¾¡é¡(åˆ†é…é‡‘å†æŠ•è³‡)ã®é¨°è½ç‡ã¯+2.85ï¼…ã§ã€ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’0.2ãƒã‚¤ãƒ³ãƒˆã‚’ä¸Šå›ã‚Šã¾ã—ãŸã€‚",
                    "output": "'original': 'ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚’0.2ãƒã‚¤ãƒ³ãƒˆã‚’ä¸Šå›ã‚Šã¾ã—ãŸã€‚', 'correct': 'ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¯åŒç¨‹åº¦ã¨ãªã‚Šã¾ã—ãŸã€‚', 'reason': 'é¨°è½ç‡ãŒåŒã˜'",
                },
                {
                    "input": "0.00ï¼…ã¨ãªã‚Šã¾ã—ãŸ",
                    "output": "'original': '0.00ï¼…ã¨ãªã‚Šã¾ã—ãŸ', 'correct': 'å‰æœˆæœ«ã‹ã‚‰å¤‰ã‚ã‚‰ãš', 'reason': 'è¡¨è¨˜ã®ä¿®æ­£'",
                }
            ]
        },
        {
        "category": "æ•°å€¤è¨˜å·ã®çµ±ä¸€(Numeric Sign Consistency)",
        "rule_id": "1.2",
        "description": "åç›Šç‡ãƒ»é¨°è½ç‡ãªã©ã«ãŠã„ã¦ã€æ­£ã®æ•°å€¤ã«ã¯æ˜ç¤ºçš„ã«ã€Œ+ã€ã‚’ä»˜ä¸ã—ã¦çµ±ä¸€æ€§ã‚’ä¿ã¤ã€‚æ—¢ã«ã€Œ+ã€ã€Œâˆ’ã€ãŒä»˜ã„ã¦ã„ã‚‹ã‚‚ã®ã‚„ã€æ¯”è¼ƒçš„è¡¨ç¾ã§å¢—æ¸›ãŒç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯å¤‰æ›´ã—ãªã„ã€‚",
        "requirements": [
            {
            "condition": "åç›Šç‡ã€é¨°è½ç‡ãªã©ã§ã€æ­£ã®æ•°å€¤ã«ç¬¦å·(+)ãŒä»˜ã„ã¦ã„ãªã„å ´åˆ",
            "correction": "ç¬¦å·(+)ã‚’ä»˜ä¸ã™ã‚‹ (ä¾‹ï¼š4.04ï¼… â†’ +4.04ï¼…)"
            },
            {
            "condition": "ã™ã§ã«ã€Œ+ã€ã‚„ã€Œâˆ’ã€ãŒä»˜ã„ã¦ã„ã‚‹æ•°å€¤",
            "correction": "å¤‰æ›´ã—ãªã„"
            },
            {
            "condition": "ã€ä¸‹å›ã£ãŸã€ã€ä¸Šå›ã£ãŸã€ã€æ¸›å°‘ã€ã€å¢—åŠ ã€ãªã©ã€æ–‡è„ˆã§å¢—æ¸›ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆ",
            "correction": "ç¬¦å·ã¯ä»˜ã‘ãªã„ï¼ˆæ–‡è„ˆã«ã‚ˆã‚Šæ–¹å‘ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹ãŸã‚ï¼‰"
            }
        ],
        "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'ã€Œï¼‹ã€ã€Œâˆ’ã€ã®æ˜ç¤ºçš„çµ±ä¸€'",
        "Examples": [
            {
            "input": "â—‹æœˆé–“ã®åŸºæº–ä¾¡é¡ã®é¨°è½ç‡ã¯4.04ï¼…",
            "output": "'original': '4.04ï¼…', 'correct': '+4.04ï¼…', 'reason': 'ã€Œï¼‹ã€ã€Œâˆ’ã€ã®æ˜ç¤ºçš„çµ±ä¸€'"
            },
            {
            "input": "ã‚¤ãƒ³ãƒ•ãƒ¬ç‡ã¯0.05ãƒã‚¤ãƒ³ãƒˆä¸‹å›ã£ã¦ã„ã‚‹",
            "output": "å¤‰æ›´ã—ãªã„"
            }
        ],
        "notes": "å¯¾è±¡æ•°å€¤ã¯ä¸€èˆ¬çš„ã«ï¼… or ãƒã‚¤ãƒ³ãƒˆ ãŒå¾Œã‚ã«ä»˜ãåç›Šã‚„æˆé•·å€¤ãªã©ã«é™å®šã€‚æ•´æ•°ãƒ»å°æ•°ã¨ã‚‚å¯¾è±¡(ä¾‹ï¼š5ï¼…ã€0.00ï¼…ã€1.234ãƒã‚¤ãƒ³ãƒˆãªã©)ã€‚ãŸã ã—ã€Œä¸‹å›ã£ã¦ã„ã‚‹ã€ã€Œä¸Šå›ã£ã¦ã„ã‚‹ã€ã€Œå¢—åŠ ã€ã€Œæ¸›å°‘ã€ãªã©æ–‡è„ˆçš„ã«æ–¹å‘ãŒæ˜ç¤ºã•ã‚Œã¦ã„ã‚‹å ´åˆã¯è¨˜å·ä¸è¦ã€‚æ–‡ç« å†…ã«è¤‡æ•°è©²å½“ãŒã‚ã‚‹å ´åˆã‚‚ã™ã¹ã¦å€‹åˆ¥ã«å¯¾å¿œã™ã‚‹ã€‚"
        },
        {
        "category": "è¡¨ç¾ãƒ«ãƒ¼ãƒ«ï¼šã€å¤§æ‰‹ã€ã®èªé †ã¨ä¼æ¥­åã®ä¸€èˆ¬åŒ–",
        "rule_id": "CorrectOoteOrder_And_GeneralizeCompanyNames",
        "description": "ã“ã®ãƒ«ãƒ¼ãƒ«ã¯ã€ã€å¤§æ‰‹ã€ã¨ã„ã†èªãŒæ–‡ä¸­ã«ä½¿ã‚ã‚Œã¦ã„ã‚‹å ´åˆã«ã®ã¿é©ç”¨ã•ã‚Œã¾ã™ã€‚ã€â—‹â—‹å¤§æ‰‹ã€ã®ã‚ˆã†ã«èªé †ãŒé€†è»¢ã—ã¦ã„ã‚‹å ´åˆã¯ã€å¤§æ‰‹â—‹â—‹ä¼æ¥­ã€ã«ä¿®æ­£ã—ã€ã‹ã¤ä¼æ¥­åãŒå«ã¾ã‚Œã‚‹å ´åˆã«ã¯æ¥­ç¨®ãƒ»åœ°åŸŸã«ä¸€èˆ¬åŒ–ã—ã¾ã™ã€‚ãŸã ã—ã€ã€å¤§æ‰‹ã€ã¨ã„ã†èªãŒå«ã¾ã‚Œãªã„å ´åˆã¯ã€ã“ã®ãƒ«ãƒ¼ãƒ«ã‚’é©ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚ä¼æ¥­åã‚’ä¸€å¾‹ã«å‰Šé™¤ãƒ»ä¸€èˆ¬åŒ–ã™ã‚‹ã“ã¨ã¯ç¦æ­¢ã—ã¾ã™ã€‚",
        "requirements": [
            {
                "condition": "ã€å¤§æ‰‹ã€ã¨ã„ã†èªãŒè¡¨ç¾å†…ã«å«ã¾ã‚Œã¦ãŠã‚Šã€ã‹ã¤ã€â—‹â—‹å¤§æ‰‹ã€ã®ã‚ˆã†ã«å¾Œç½®ã•ã‚Œã¦ã„ã‚‹å ´åˆã€èªé †ã‚’ã€å¤§æ‰‹â—‹â—‹ã€ã«ä¿®æ­£ã™ã‚‹ã€‚",
                "correction": "ä¾‹ï¼šã€ã‚²ãƒ¼ãƒ å¤§æ‰‹ä¼æ¥­ã€â‡’ã€å¤§æ‰‹ã‚²ãƒ¼ãƒ ä¼æ¥­ã€"
            },
            {
                "condition": "ã€å¤§æ‰‹ã€ãŒå«ã¾ã‚Œã¦ãŠã‚Šã€ã‹ã¤ç‰¹å®šä¼æ¥­åï¼ˆä¾‹ï¼šã‚¯ãƒ¬ãƒ‡ã‚£ãƒ»ã‚¹ã‚¤ã‚¹ãªã©ï¼‰ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆã€ä¼æ¥­åã‚’å‰Šé™¤ã—ã¦åœ°åŸŸã‚„æ¥­ç¨®ã«ä¸€èˆ¬åŒ–ã™ã‚‹ã€‚",
                "correction": "ä¾‹ï¼šã€ã‚¹ã‚¤ã‚¹é‡‘èå¤§æ‰‹ã‚¯ãƒ¬ãƒ‡ã‚£ãƒ»ã‚¹ã‚¤ã‚¹ã€â‡’ã€ã‚¹ã‚¤ã‚¹ã®å¤§æ‰‹é‡‘èã‚°ãƒ«ãƒ¼ãƒ—ã€"
            },
            {
                "condition": "ã€å¤§æ‰‹ã€ã¨ã„ã†èªãŒå«ã¾ã‚Œã¦ã„ãªã„å ´åˆã€ã“ã®ãƒ«ãƒ¼ãƒ«ã¯é©ç”¨ã—ãªã„ã€‚ä¼æ¥­åã®ã¿ãŒè¨˜è¼‰ã•ã‚Œã¦ã„ã‚‹å ´åˆï¼ˆä¾‹ï¼šã‚¢ãƒã‚¾ãƒ³ã€ä»»å¤©å ‚ãªã©ï¼‰ã¯åŸæ–‡ã®ã¾ã¾ã¨ã—ã€ä¸€èˆ¬åŒ–ãƒ»èªé †ä¿®æ­£ã¯è¡Œã‚ãªã„ã€‚",
                "correction": "ä¾‹ï¼šã€ä»»å¤©å ‚ã€â‡’ ä¿®æ­£ä¸è¦ï¼ˆå¤§æ‰‹ã¨ã„ã†èªãŒãªã„ãŸã‚ï¼‰"
            }
        ],
        "output_format": "'original': 'èª¤ã‚Šã®ã‚ã‚‹è¡¨ç¾', 'correct': 'ä¿®æ­£å¾Œã®è¡¨ç¾', 'reason': 'ä¿®æ­£ã®ç†ç”±'",
        "examples": [
            {
                "input": "é€šä¿¡å¤§æ‰‹ãŒæ–°ã‚µãƒ¼ãƒ“ã‚¹ã‚’ç™ºè¡¨ã—ã¾ã—ãŸã€‚",
                "output": {
                    "original": "é€šä¿¡å¤§æ‰‹",
                    "correct": "å¤§æ‰‹é€šä¿¡ä¼šç¤¾",
                    "reason": "ã€å¤§æ‰‹ã€ã¯æ¥­ç¨®ï¼ˆé€šä¿¡ï¼‰ã®ç›´å‰ã«ç½®ãå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚"
                }
            },
            {
                "input": "ã‚²ãƒ¼ãƒ å¤§æ‰‹ä¼æ¥­ã®æ ªä¾¡ãŒä¸Šæ˜‡ã—ãŸã€‚",
                "output": {
                    "original": "ã‚²ãƒ¼ãƒ å¤§æ‰‹ä¼æ¥­",
                    "correct": "å¤§æ‰‹ã‚²ãƒ¼ãƒ ä¼æ¥­",
                    "reason": "ã€å¤§æ‰‹ã€ã¯ã€ã‚²ãƒ¼ãƒ ã€ã®ç›´å‰ã«é…ç½®ã™ã‚‹ã®ãŒé©åˆ‡ã§ã™ã€‚"
                }
            },
            {
                "input": "ã‚¹ã‚¤ã‚¹é‡‘èå¤§æ‰‹ã‚¯ãƒ¬ãƒ‡ã‚£ãƒ»ã‚¹ã‚¤ã‚¹ã¯çµŒå–¶ç ´ç¶»ã—ãŸã€‚",
                "output": {
                    "original": "ã‚¹ã‚¤ã‚¹é‡‘èå¤§æ‰‹ã‚¯ãƒ¬ãƒ‡ã‚£ãƒ»ã‚¹ã‚¤ã‚¹",
                    "correct": "ã‚¹ã‚¤ã‚¹ã®å¤§æ‰‹é‡‘èã‚°ãƒ«ãƒ¼ãƒ—",
                    "reason": "å€‹åˆ¥ä¼æ¥­åã¯çœç•¥ã—ã€ã€å¤§æ‰‹ã€ã¯æ¥­ç¨®ã®ç›´å‰ã«ç½®ãã¾ã™ã€‚"
                }
            },
            {
                "input": "ä»»å¤©å ‚ã¯æ–°ä½œã‚²ãƒ¼ãƒ ã‚’ç™ºè¡¨ã—ãŸã€‚",
                "output": {
                    "original": "ä»»å¤©å ‚",
                    "correct": "ä»»å¤©å ‚",
                    "reason": "ã€å¤§æ‰‹ã€ã¨ã„ã†èªãŒå«ã¾ã‚Œã¦ã„ãªã„ãŸã‚ã€ä¿®æ­£ã®å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ã€‚"
                }
            }
        ]
    },
        {
            "category": "YieldMovementdescription",
            "rule_id": "1.3",
            "description": "When describing the movement of yields (åˆ©å›ã‚Š), ensure that the inverse relationship with prices is properly reflected.",
            "requirements": [
                {
                    "condition": "If yields rise, it implies that prices fall.",
                    "correction": ""
                },
                {
                    "condition": "If yields fall, it implies that prices rise.",
                    "correction": ""
                },
                {
                    "condition": "If this inverse relationship is not mentioned where necessary, highlight and prompt for correction.",
                    "correction": ""
                },
                {
                    "condition": "åˆ©å›ã‚Šã¯ã€Œä¸Šæ˜‡(ä¾¡æ ¼ã¯ä¸‹è½)ã€ã¾ãŸã¯ã€Œä½ä¸‹(ä¾¡æ ¼ã¯ä¸Šæ˜‡)ã€ã¨è¡¨è¨˜ã€‚",
                    "correction": ""
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "Input": "åˆ©å›ã‚Šã®ä½ä¸‹",
                    "Output": "'original': 'åˆ©å›ã‚Šã®ä½ä¸‹', 'correct': 'Corrected text', 'åˆ©å›ã‚Šã®ä½ä¸‹(ä¾¡æ ¼ã¯ä¸Šæ˜‡)','reason': 'åˆ©å›ã‚Šã¨ä¾¡æ ¼ã®é€†ç›¸é–¢é–¢ä¿‚ã‚’æ˜è¨˜ã™ã‚‹å¿…è¦ã‚ã‚Š'",
                },
                {
                    "Input": "æ—¥æœ¬10å¹´å›½å‚µåˆ©å›ã‚Šã¯ã€æœˆé–“ã§ä¸Šæ˜‡ã—ã¾ã—ãŸã€‚",
                    "Output": "'original': 'ä¸Šæ˜‡', 'correct': 'Corrected text', 'ä¸Šæ˜‡(ä¾¡æ ¼ã¯ä½ä¸‹)','reason': 'åˆ©å›ã‚Šã¨ä¾¡æ ¼ã®é€†ç›¸é–¢é–¢ä¿‚ã‚’æ˜è¨˜ã™ã‚‹å¿…è¦ã‚ã‚Š'",
                },
                {
                    "Input": "åˆ©å›ã‚Šã®ä¸Šæ˜‡",
                    "Output": "'original': 'åˆ©å›ã‚Šã®ä¸Šæ˜‡', 'correct': 'Corrected text', 'åˆ©å›ã‚Šã®ä¸Šæ˜‡(ä¾¡æ ¼ã¯ä½ä¸‹)','reason': 'åˆ©å›ã‚Šã¨ä¾¡æ ¼ã®é€†ç›¸é–¢é–¢ä¿‚ã‚’æ˜è¨˜ã™ã‚‹å¿…è¦ã‚ã‚Š'",
                },
                {
                    "Input": "æœˆé–“ã§ã¯å‚µåˆ¸åˆ©å›ã‚Šã¯ä¸Šæ˜‡ã—ã¾ã—ãŸã€‚",
                    "Output": "'original': 'åˆ©å›ã‚Šã¯ä¸Šæ˜‡', 'correct': 'Corrected text', 'åˆ©å›ã‚Šã¯ä¸Šæ˜‡(ä¾¡æ ¼ã¯ä½ä¸‹)','reason': 'åˆ©å›ã‚Šã¨ä¾¡æ ¼ã®é€†ç›¸é–¢é–¢ä¿‚ã‚’æ˜è¨˜ã™ã‚‹å¿…è¦ã‚ã‚Š'",
                },
                {
                    "Input": "æ—¥æœ¬10å¹´å›½å‚µåˆ©å›ã‚Šã¯ã€æœˆé–“ã§ä¸‹è½ã—ã¾ã—ãŸã€‚",
                    "Output": "'original': 'ä¸‹è½', 'correct': 'ä¸‹è½(ä¾¡æ ¼ã¯ä¸Šæ˜‡)', 'reason': 'åˆ©å›ã‚Šã¨ä¾¡æ ¼ã®é€†ç›¸é–¢é–¢ä¿‚ã‚’æ˜è¨˜ã™ã‚‹å¿…è¦ã‚ã‚Š'",
                }
            ]
        },
        {
            "category": "Correct Usage Of Teika And Geraku",
            "rule_id": "1.4",
            "description": "When describing changes in yields, prices, or interest rates, apply the following word choice rules strictly",
            "requirements": [
                {
                    "condition": "åˆ©å›ã‚Šã«ã¤ã„ã¦ã®æ•°å€¤å¤‰æ›ã®å ´åˆ",
                    "correction": "use ä½ä¸‹ for decline, not ä¸‹è½."
                },
                {
                    "condition": "ä¾¡æ ¼ã«ã¤ã„ã¦ã®æ•°å€¤å¤‰æ›ã®å ´åˆ",
                    "correction": "use ä¸‹è½ for decline, not ä½ä¸‹."
                },
                {
                    "condition": "é‡‘åˆ©ã«ã¤ã„ã¦ã®æ•°å€¤å¤‰æ›ã®å ´åˆ",
                    "correction": "use ä½ä¸‹ for decline, not ä¸‹è½."
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "Input": "ç±³å›½å‚µåˆ©å›ã‚ŠãŒä¸‹è½ã—ã¾ã—ãŸã€‚",
                    "Output": "'original': 'ç±³å›½å‚µåˆ©å›ã‚ŠãŒä¸‹è½ã—ã¾ã—ãŸã€‚', 'correct': 'ç±³å›½å‚µåˆ©å›ã‚ŠãŒä½ä¸‹ã—ã¾ã—ãŸã€‚', 'reason': 'åˆ©å›ã‚Šã«ã¯ã€Œä½ä¸‹ã€ã‚’ä½¿ç”¨'",
                },
                {
                    "Input": "ä¾¡é¡ãŒä½ä¸‹ã—ã¾ã—ãŸã€‚",
                    "Output": "'original': 'ä¾¡é¡ãŒä½ä¸‹ã—ã¾ã—ãŸã€‚', 'correct': 'ä¾¡é¡ãŒä¸‹è½ã—ã¾ã—ãŸ', 'reason': 'ä¾¡æ ¼ã«ã¯ã€Œä¸‹è½ã€ã‚’ä½¿ç”¨'",
                },
                {
                    "Input": "é‡‘åˆ©ãŒä¸‹è½ã—ã¾ã—ãŸã€‚",
                    "Output": "'original': 'é‡‘åˆ©ãŒä¸‹è½ã—ã¾ã—ãŸã€‚', 'correct': 'é‡‘åˆ©ãŒä½ä¸‹ã—ã¾ã—ãŸã€‚', 'reason': 'é‡‘åˆ©ã«ã¯ã€Œä½ä¸‹ã€ã‚’ä½¿ç”¨'",
                }
            ]
        },
        {
            "category": "ZeroPercentCompositionNotation",
            "rule_id": "1.8",
            "description": "When describing a composition ratio of 0%, use either ã€Œ0ï¼…ç¨‹åº¦ã€ or ã€Œã‚¼ãƒ­ï¼…ç¨‹åº¦ã€",
            "requirements": [
                {
                    "condition": "When describing a composition ratio of 0%",
                    "correction": "use either ã€Œ0ï¼…ç¨‹åº¦ã€ or ã€Œã‚¼ãƒ­ï¼…ç¨‹åº¦ã€ direct expressions like just \"0%\" without ã€Œç¨‹åº¦ã€ should be corrected."
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'æ§‹æˆæ¯”0ï¼…ã®è¡¨è¨˜çµ±ä¸€'",
            "Example": {
                "Input": "å½“ãƒ•ã‚¡ãƒ³ãƒ‰ã®æ§‹æˆæ¯”ã¯0ï¼…ã§ã™ã€‚",
                "Output": "'original': '0ï¼…', 'correct': '0ï¼…ç¨‹åº¦', 'reason': 'æ§‹æˆæ¯”0ï¼…ã®è¡¨è¨˜çµ±ä¸€'",
            }
        },
        {
            "category": "TerminologyConsistency_Calm",
            "rule_id": "2.0",
            "description": "If the usage does not match the context, correct it according to the appropriate meaning.ä¿®æ­£ç†ç”±: æ„å‘³ã®èª¤ç”¨",
            "requirements": [
                {
                    "condition": "Use ã€Œæ²ˆé™ã€ when referring to natural calming down over time.",
                    "correction": ""
                },
                {
                    "condition": "Use ã€Œé®é™ã€ when referring to intentional or artificial suppression (e.g., medical treatment, intervention).",
                    "correction": ""
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "Input": "å¸‚å ´ã¯å¾ã€…ã«é®é™ã—ã¦ã„ã£ãŸã€‚",
                    "Output": "'original': 'é®é™', 'correct': 'æ²ˆé™', 'reason': 'æ„å‘³ã®èª¤ç”¨'",
                },
                {
                    "Input": "åŒ»ç™‚ãƒãƒ¼ãƒ ã¯æ‚£è€…ã®æš´å‹•ã‚’æ²ˆé™ã•ã›ãŸã€‚",
                    "Output": "'original': 'æ²ˆé™', 'correct': 'é®é™', 'reason': 'æ„å‘³ã®èª¤ç”¨'",
                }
            ]
        },
        {
            "category": "Prohibited_or_Cautioned_Expressions_Rise_and_Decline_Factors",
            "rule_id": "2.8",
            "description": "When 'ä¸Šæ˜‡' or 'ä¸‹è½' appears, check the paragraph-level context for an explicit causal explanation. If no cause is provided, highlight the word and prompt the user. Do not modify the sentence itselfâ€”annotate only. If both rise and fall occurred in different courses, prioritize the one with the larger change. Describing both is also acceptable. (ä¸Šæ˜‡ãƒ»ä¸‹è½è¦å› ã®è¨˜è¼‰æ¼ã‚Œã«å¯¾ã™ã‚‹è­¦å‘Š)",
            "requirements": [
                {
                    "condition": "ä¸Šæ˜‡",
                    "correction": "ä¸Šæ˜‡ã®è¦å› (èƒŒæ™¯ã‚„ç†ç”±)ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚"
                },
                {
                    "condition": "ä¸‹è½",
                    "correction": "ä¸‹è½ã®è¦å› (èƒŒæ™¯ã‚„ç†ç”±)ã‚’æ˜è¨˜ã—ã¦ãã ã•ã„ã€‚"
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'ä¸Šæ˜‡ãƒ»ä¸‹è½è¦å› ã®è¨˜è¼‰æ¼ã‚Œã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æç¤º'",
            "Examples": [
                {
                    "Input": "æœ€è¿‘ã€ç±³å›½çµŒæ¸ˆã«ãŠã„ã¦è³ƒé‡‘ä¸Šæ˜‡ã¨ç‰©ä¾¡é«˜ãŒç¶šã„ã¦ãŠã‚Šã€æ™¯æ°—éç†±æ„ŸãŒæŒ‡æ‘˜ã•ã‚Œã¦ã„ã¾ã™ã€‚ç±³å›½ã®ã‚¤ãƒ³ãƒ•ãƒ¬æ‡¸å¿µã®é«˜ã¾ã‚Šã‚’èƒŒæ™¯ã«ã€æ ªå¼å¸‚å ´ã¯ä¸‹è½ã—ã¾ã—ãŸã€‚é‡‘èå½“å±€ã¯ä»Šå¾Œã‚‚åˆ©ä¸Šã’ã‚’ç¶šã‘ã‚‹è¦‹é€šã—ã§ã™ã€‚",
                    "Output": "(ã‚¨ãƒ©ãƒ¼ãªã—:å‰æ–‡ã«è¦å› è¨˜è¼‰ã‚ã‚Š)"
                }
            ]
        },
        {
            "category": "Replacement Rules for Verb-Type Expressions(å‹•è©ãƒ»æ´»ç”¨å½¢ã‚’å«ã‚€è¡¨ç¾ã®ç½®ãæ›ãˆ)",
            "rule_id": "2.9",
            "description":"Certain terms or expressions require more than simple string or regex-based replacement. These are called dynamically varying expressions, which include but are not limited to: Register-sensitive expressions (e.g., polite/humble language variations) Compound phrases or abbreviations that appear in flexible forms When the term to be replaced is a verb, the system must detect and process all conjugated or inflected forms. Do not use rigid pattern matching. Ensure grammatical accuracy after replacement. In general, all such replacements must be done in a context-sensitive manner, ensuring the result remains grammatically and semantically correct",
            "requirements": [
                {
                    "condition": "ï½ã«è³­ã‘ã‚‹ to ï½ã‚’äºˆæƒ³ã—ã¦ ,æ—¥æœ¬èªã®ä½¿ã„å‹å¤‰æ›ã‚’æ³¨æ„ã™ã¹ã",
                    "correction": "ï½ã‚’äºˆæƒ³ã—ã¦"
                },
                {
                    "condition": "ã€Œæ¨ªã°ã„ã€ã¨ã„ã†è¡¨ç¾ã¯ã€æœŸé–“ä¸­ã®ä¾¡æ ¼ãƒ»åˆ©å›ã‚Šç­‰ã®å€¤å‹•ããŒéå¸¸ã«å°ã•ã„å ´åˆã«é™å®šã—ã¦ä½¿ç”¨ã™ã‚‹ã“ã¨ã€‚ä¸€æ–¹ã§ã€æœŸé–“ä¸­ã«ä¸€å®šã®å¤‰å‹•ãŒã‚ã£ãŸã‚‚ã®ã®ã€æœ€çµ‚çš„ã«é–‹å§‹æ™‚ç‚¹ã¨åŒç¨‹åº¦ã®æ°´æº–ã«æˆ»ã£ãŸå ´åˆã«ã¯ã€ã€Œã»ã¼å¤‰ã‚ã‚‰ãšã€ã€ŒåŒç¨‹åº¦ã¨ãªã‚‹ã€ãªã©ã®è¡¨ç¾ã‚’ä½¿ç”¨ã™ã‚‹ã€‚èª¤ã£ã¦ã€Œæ¨ªã°ã„ã€ã¨è¨˜è¿°ã™ã‚‹ã¨ã€å€¤å‹•ããŒãªã‹ã£ãŸã‚ˆã†ãªèª¤èªã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ãŸã‚ã€äº‹å®Ÿã«åŸºã¥ã„ãŸæ­£ç¢ºãªè¡¨ç¾é¸æŠãŒæ±‚ã‚ã‚‰ã‚Œã‚‹ã€‚",
                    "correction": "ã»ã¼å¤‰ã‚ã‚‰ãš"
                }
            ],
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason text'",
            "Examples": [
                {
                    "Input": "ï½ã«è³­ã‘ã‚‹",
                    "Output": "'original': 'ï½ã«è³­ã‘ã‚‹', 'correct': 'ï½ã‚’äºˆæƒ³ã—ã¦', 'reason': 'Reason text'",
                },
                {
                    "Input": "å½“ä½œæˆæœŸã‚’é€šã—ã¦ã¿ã‚‹ã¨å‚µåˆ¸åˆ©å›ã‚Šã¯æ¨ªã°ã„ã§ã—ãŸã€‚",
                    "Output": "'original': 'æ¨ªã°ã„', 'correct': 'ã»ã¼å¤‰ã‚ã‚‰ãš', 'reason': 'æœŸé–“ä¸­ã«ä¸€å®šã®å¤‰å‹•å¹…ãŒç¢ºèªã•ã‚Œã¦ãŠã‚Šã€ã€Œæ¨ªã°ã„ã€ã¨ã„ã†è¡¨ç¾ã¯å®Ÿæ…‹ã¨åˆã‚ãªã„ãŸã‚ã€ã€Œã»ã¼å¤‰ã‚ã‚‰ãšã€ã¨ã™ã‚‹ã®ãŒé©åˆ‡ã€‚'",
                }
            ]
        },
        {
            "category": "è¡Œã£ã¦æ¥ã„ â‡’ ã€Œä¸Šæ˜‡(ä¸‹è½)ã—ãŸã®ã¡ä¸‹è½(ä¸Šæ˜‡)ã€ç­‰ã¸æ›¸ãæ›ãˆã‚‹",
            "rule_id": "3.0",
            "description": "The expression â€œè¡Œã£ã¦æ¥ã„â€ is informal and vague. It must not be used in formal financial documents or reports intended for external audiences.Replace it with a precise description of the price movement, such as: â€œä¸Šæ˜‡ã—ãŸã®ã¡ä¸‹è½ã—ãŸâ€ ä¸‹è½ã—ãŸã®ã¡ä¸Šæ˜‡ã—ãŸ Always use fact-based, objective wording that clearly describes the market movement.",
            "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'ã€Œè¡Œã£ã¦æ¥ã„ã€ã¯æ›–æ˜§ã‹ã¤å£èªçš„ãªè¡¨ç¾ã§ã‚ã‚Šã€æ­£å¼ãªé‡‘èæ–‡æ›¸ã§ã¯å…·ä½“çš„ãªå€¤å‹•ãã‚’æ˜è¨˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚'",
            "Examples": [
                {
                    "Input": "ç›¸å ´ã¯è¡Œã£ã¦æ¥ã„ã®å±•é–‹ã¨ãªã‚Šã¾ã—ãŸã€‚",
                    "Output": "'original': 'è¡Œã£ã¦æ¥ã„', 'correct': 'ä¸€æ™‚ä¸Šæ˜‡ã—ãŸã‚‚ã®ã®ã€ãã®å¾Œä¸‹è½ã—ã€å‰æ—¥ã¨åŒæ°´æº–ã§çµ‚äº†ã—ã¾ã—ãŸã€‚', 'reason': 'ã€Œè¡Œã£ã¦æ¥ã„ã€ã¯æ›–æ˜§ã‹ã¤å£èªçš„ãªè¡¨ç¾ã§ã‚ã‚Šã€æ­£å¼ãªé‡‘èæ–‡æ›¸ã§ã¯å…·ä½“çš„ãªå€¤å‹•ãã‚’æ˜è¨˜ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚'",
                }
            ]
        }
        # ,{
        #     "category": "ä¸»èªã®æ¬ è½ãƒã‚§ãƒƒã‚¯",
        #     "rule_id": "4.0",
        #     "description": "æ—¥æœ¬èªã®æ–‡ã§ã€èª°ãŒãƒ»ä½•ãŒã‚’ç¤ºã™ä¸»èªãŒçœç•¥ã•ã‚Œã‚‹ã¨æ–‡ãŒä¸è‡ªç„¶ã«ãªã‚Šã€èª­ã¿æ‰‹ã«èª¤è§£ã‚’ä¸ãˆã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚ç‰¹ã«é‡‘èæ–‡æ›¸ã‚„ãƒ¬ãƒãƒ¼ãƒˆã§ã¯ã€ä¸»èªã‚’æ˜ç¢ºã«ã™ã‚‹ã“ã¨ã§è²¬ä»»ä¸»ä½“ã‚„å‹•ä½œä¸»ä½“ãŒæ­£ç¢ºã«ä¼ã‚ã‚Šã¾ã™ã€‚ä¸»èªãŒæ¬ ã‘ã¦ã„ã‚‹å ´åˆã¯ã€æ–‡è„ˆã«åŸºã¥ãã€ŒåŒç¤¾ã€ã€Œå¸‚å ´ã€ã€Œæ±ºç®—ç™ºè¡¨ã€ãªã©ã‚’è£œã†å½¢ã§ä¿®æ­£ã—ã¦ãã ã•ã„ã€‚",
        #     "output_format": "'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'ä¸»èªãŒæ¬ è½ã—ã¦ãŠã‚Šã€èª°ãŒç¤ºå”†ã—ãŸã®ã‹ãŒä¸æ˜ç¢ºã§ä¸è‡ªç„¶ã§ã™ã€‚é‡‘èæ–‡æ›¸ã§ã¯å‹•ä½œã®ä¸»ä½“ã‚’æ˜ç¢ºã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚'",
        #     "Examples": [
        #         {
        #             "Input": "ã‚ªãƒ©ãƒ³ãƒ€æ ªã¯å¤§æ‰‹åŠå°ä½“è£½é€ è£…ç½®ãƒ¡ãƒ¼ã‚«ãƒ¼ã®2025å¹´4ï¼6æœˆæœŸå—æ³¨é¡ã¯å¸‚å ´äºˆæƒ³ã‚’ä¸Šå›ã£ãŸã‚‚ã®ã®ã€2026å¹´ã®æˆé•·ãŒæœŸå¾…ã§ããªã„ã¨ç¤ºå”†ã—ãŸã“ã¨ãªã©ã‚’èƒŒæ™¯ã«ä¸‹è½ã—ã¾ã—ãŸã€‚",
        #             "Output": "'original': '2026å¹´ã®æˆé•·ãŒæœŸå¾…ã§ããªã„ã¨ç¤ºå”†ã—ãŸ', 'correct': 'åŒç¤¾ãŒ2026å¹´ã®æˆé•·ãŒæœŸå¾…ã§ããªã„ã“ã¨ã‚’ç¤ºå”†ã—ãŸ', 'reason': 'ä¸»èªãŒæ¬ è½ã—ã¦ãŠã‚Šã€ã€Œèª°ãŒç¤ºå”†ã—ãŸã®ã‹ã€ãŒåˆ†ã‹ã‚‰ãªã„ãŸã‚ä¸è‡ªç„¶ã§ã™ã€‚ä¸»èªã‚’è£œã†ã“ã¨ã§æ–‡æ„ãŒæ˜ç¢ºã«ãªã‚Šã¾ã™ã€‚'"
        #         }
        #     ]
        # }
    ]

    for ruru_split in ruru_all:
        result = f"""
        You are a professional Japanese business document proofreader specialized in financial and public disclosure materials. 
        Your task is to carefully and strictly proofread the provided Japanese report based on the detailed rules specified below.

        The proofreading targets include:

        Important:
        
        Each section must be strictly followed without omission.
        You are prohibited from making subjective judgments or skipping steps, even if an error seems minor.
        Always prioritize rule adherence over general readability or aesthetic preference.
        Final Output Requirements:
        Use the specified correction format for each detected error.
        Preserve the original sentence structure and paragraph formatting unless explicitly instructed otherwise.
        If no corrections are needed for a section, explicitly state "No errors detected" (æ¤œå‡ºã•ã‚ŒãŸèª¤ã‚Šãªã—).
        Follow all instructions strictly and proceed only according to the rules provided.:
        
        Do not correct or modify kana orthography variations (e.g., ã€Œè¡Œãªã†ã€ vs ã€Œè¡Œã†ã€), unless explicitly instructed.
        Do not apply standardization unless listed in the rules.
        
        **Report Content to Proofread:**
        {input}

        **Proofreading Requirements:**
        {ruru_split}

        **Output Requirements:**
        1. **Return only structured correction results as a Python-style list of dictionaries:**
        - Format:
            'original': 'Incorrect text', 'correct': 'Corrected text', 'reason': 'Reason for correction'
        - Example:
            'original': 'æœˆé–“ã§ã¯ã»ã¼å¤‰ã‚ã‚‰ãšãªã‚Šã¾ã—ãŸã€‚', 'correct': 'æœˆé–“ã§ã¯ã»ã¼å¤‰ã‚ã‚‰ãšã¨ãªã‚Šã¾ã—ãŸã€‚', 'reason': 'èª¤å­—'
        
        2. **Each dictionary must include:**
            - 'original': the original incorrect text
            - 'correct': the corrected text
            - 'reason': a concise explanation for the correction
        3. **Do not include any explanation, HTML tags, or narrative. Only return the data in this dictionary format.**
        4. **Maintain the original document structure internally during processing, but the output should only contain corrections in the required format.**
    
        """
        yield result

@app.route('/api/opt_wording', methods=['POST'])
def opt_wording():
    try:
        token = token_cache.get_token()
        openai.api_key = token
        print("âœ… Token Update SUCCESS")
        
        data = request.json

        def convert_fullwidth_to_halfwidth(text):
            return text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
        
        # input = data.get("input", "")
        input = convert_fullwidth_to_halfwidth(data.get("input", ""))

        pdf_base64 = data.get("pdf_bytes", "")

        fund_type = data.get("fund_type", "public")  #  'public'
        file_name_decoding = data.get("file_name", "")
        icon = data.get("icon", "")
        comment_type = data.get("comment_type", "")
        upload_type = data.get("upload_type", "")
        pageNumber = data.get('pageNumber',0)
        
        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        if not input:
            return jsonify({"success": False, "error": "No input provided"}), 400

        prompt_result = loop_in_ruru("\"" + input.replace('\n', '') + "\"")
        async def run_tasks():
            tasks = [handle_result(once) for once in prompt_result]
            return await asyncio.gather(*tasks)

        results = asyncio.run(run_tasks())
        sec_input = "\n".join(results)

        dt = [
            "ä»¥ä¸‹ã®åˆ†æçµæœã«åŸºã¥ãã€åŸæ–‡ä¸­ã®èª¤ã‚Šã‚’æŠ½å‡ºã—ã¦ãã ã•ã„",
            "- å‡ºåŠ›çµæœã¯æ¯å›åŒã˜ã«ã—ã¦ãã ã•ã„ï¼ˆ**åŒã˜å…¥åŠ›ã«å¯¾ã—ã¦çµæœãŒå¤‰å‹•ã—ãªã„ã‚ˆã†ã«**ã—ã¦ãã ã•ã„ï¼‰ã€‚",
            "å‡ºåŠ›ã¯ä»¥ä¸‹ã®JSONå½¢å¼ã§ãŠé¡˜ã„ã—ã¾ã™:",
            "- [{'original': '[åŸæ–‡ä¸­ã®èª¤ã£ã¦ã„ã‚‹éƒ¨åˆ†:]', 'correct': '[èª¤ã‚Šéƒ¨åˆ†ã‚’æ­£ã—ã„éƒ¨åˆ†ã®ãƒ†ã‚­ã‚¹ãƒˆã«ä¿®æ­£:]', 'reason': '[ç†ç”±:]'}]",
            "- åˆ†æçµæœãŒæ­£ã—ã„å ´åˆã¯ã€ç©ºã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¾ã™",
            "- åŒã˜å…¥åŠ›ã«ã¯å¸¸ã«**åŒã˜JSONå½¢å¼ã®å‡ºåŠ›**ã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆæ¨è«–ã®æºã‚Œã‚’é¿ã‘ã¦ãã ã•ã„ï¼‰ã€‚",
            f"åŸæ–‡:'{input}'\nåˆ†æçµæœ:'{sec_input}'"
        ]
        sec_prompt = "\n".join(dt)

        _content = opt_common(input,sec_prompt,pdf_base64,pageNumber,False,False,False,False,False)
        
        return _content
    
        # loop = asyncio.new_event_loop()
        # asyncio.set_event_loop(loop)
        # _content = loop.run_until_complete(opt_common_wording(file_name,fund_type,input,prompt_result,excel_base64,pdf_base64,resutlmap,upload_type,comment_type,icon,pageNumber))
        
        # return _content

    except Exception as e:
        return jsonify({"success": False, "error": str(e)}), 500

# 820 ,pre-process
def get_words(converted_data, fund_type):
    filter_words = {
        " ã€Œæ—¥æœ¬ãƒ—ãƒ­": True,
        " æ—¥æœ¬å–å¼•æ‰€": True,
        " 15æ—¥ã«æ±": True,
        " æ—¥æœ¬çµŒæ¸ˆã¯": True,
        "  (    ": True,
        " )    ": True,
        " åœ°æ–¹ä¸»è¦éƒ½": True,
        " )    ": True,
        " ç›¸å¯¾çš„ã«åˆ©": True,
        " ãƒãƒ¼ãƒˆãƒ•ã‚©": True,
        " ç±³å›½ã¨ä¸­å›½": True,
        " ç±³å›½ã®å‚µåˆ¸": True,
        " ãƒ»ãƒ»ãƒ» æ™¯": True,
        " ãƒ»ãƒ»ãƒ» ãƒ‰": True,
        " ãƒ»ãƒ»ãƒ» æ—¥": True,
        " ãƒ»ãƒ»ãƒ» F": True,
        " ãƒ»ãƒ»ãƒ» é•·": True,
        " ãƒ»ãƒ»ãƒ» ã‚¬": True,
        " ç‚ºæ›¿ ãƒ»ãƒ»": True,
        " ï¼œæœˆé–“ã®åŸº": True,
        " é•·ã‚ï¼ˆåœ°åŸŸ": True,
        " ç¾åœ¨ )": True,
        " 2025å¹´": True,
        " æ„›ç§°ï¼š3çœŒ": True,
        " å½“ãƒ•ã‚¡ãƒ³ãƒ‰": True,
        " 2024å¹´": True,
        " æ–°ãŸãªãƒ‡ã‚¸": True,
        " ç›´è¿‘ã§ã¯ã€": True,
        " 2025å¹´": True,
        "  (2025": True,
        "ç¾åœ¨ï¼‰": True,
        " ( ": True,
        "ç¾åœ¨ ": True,
        "ç¾åœ¨)": True,
        " ç¾åœ¨ï¼‰": True,
        "ï¼‘": True,
        "ï¼¦ï¼µï¼®ï¼¤ï¼³": True,
        "ï¼®ï¼¥ï¼¸ï¼´": True,
        "ï¼ˆé©æ ¼æ©Ÿé–¢": True,
        ")\n": True,
        "ã‚¯ã‚¹ãƒ•ã‚¡ãƒ³ãƒ‰\nãƒ•ã‚¡ãƒ³ãƒ‰ã¯ã€å€¤": True,
        "#VALUE!\né‡æ‘": True,
        "ï¼‰ã®ã§ã€åŸºæº–ä¾¡é¡ã¯å¤‰å‹•ã—ã¾ã™ã€‚": True,
        "ï¼ˆUSDï¼‰": True,
    }
    result_data = []
    for data in converted_data:
        afterChange = data["comment"].split("â†’")[-1].strip()
        beforeChange = data["original_text"].strip()
        if data["reason_type"] not in ["å¸¸ç”¨å¤–æ¼¢å­—ã®ä½¿ç”¨", "æ–°è¦éŠ˜æŸ„", "ä¸è‡ªç„¶ãªç©ºç™½", "åŒä¸€è¡¨ç¾", "ç•°å¸¸ãªè‰²"]:
            if afterChange == beforeChange:
                continue
        if "æ—¥ä»˜è¡¨è¨˜ã¨ã—ã¦ä¸è‡ªç„¶ãªãŸã‚" in data["reason_type"]:
            continue
        #---821,fix the error disable

        if "æ­£ã—ã„è¦³ç‚¹" in data["reason_type"]:
            continue
        if "ä¿®æ­£ä¸è¦" in data["reason_type"]:
            continue
        #---821,-----------------
        if beforeChange in ["å…ˆæœˆã®æŠ•è³‡ç’°å¢ƒ", "10", "å…ˆæœˆã®é‹ç”¨çµŒé", "ä»Šå¾Œã®é‹ç”¨æ–¹é‡", "å¿…ãš", "éŠ˜æŸ„\nç´”è³‡ç”£æ¯”", "ä¼šç¤¾ï¼ˆä»¥ä¸‹ã€Œï¼ªï¼°ï¼¸ã€ã¨ã„ã†ã€‚", "ï¼ˆUSDï¼‰"]:
            continue
        if re.search(r"^\d+/\d", beforeChange):
            continue
        if fund_type == "public" and filter_words.get(beforeChange):
            continue
        if re.search(r"ç¾åœ¨|è©³ã—ãã¯ã€|ï¼ˆé‹ç”¨å®Ÿç¸¾ã€åˆ†é…é‡‘ã¯ã€|4æœˆã®J-|ã‚ã‚Šã¾ã™ï¼‰ã€‚|å½“ãƒ•ã‚¡ãƒ³ãƒ‰|ã“ã®å ±å‘Šæ›¸ã¯ã€ãƒ•ã‚¡ãƒ³ãƒ‰ã®é‹ç”¨çŠ¶|ï¼‰ã€‚|ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‹•å‘ã¯|å½“æœˆã®æŠ•è³‡é…åˆ†|è²·ã„å»ºã¦ã—|è²·ã„ä»˜ã‘ã—ãªã©ã‚’ã—ãŸ|è´…æ²¢å“æ ªã®è²·|ãªã©ã®", afterChange):
            continue
        # 827 fix
        if afterChange == "ã€‚" and beforeChange == "":
            continue
        # 903 fix
        if afterChange == "æ±äº¬ã‚¨ãƒ¬ã‚¯ãƒˆãƒ­ãƒ³ã¯ç¤¾ä¼šåŠ¹ç‡åŒ–ã€":
            continue
        ignore_list = [
        "â—‹ã€‚",
        "ã€‡ã€‚",
        "3ã€‚",
        "éŠ˜æŸ„ã€‚",
        "\nâ—†è¨­å®šãƒ»é‹ç”¨ã¯\nè¿½åŠ å‹æŠ•ä¿¡ï¼å†…å¤–ï¼æ ªå¼\n6/10\n1\n2ã€‚",
        "å“¡\nâ—†è¨­å®šãƒ»é‹ç”¨ã¯\nè¿½åŠ å‹æŠ•ä¿¡ï¼å†…å¤–ï¼æ ªå¼\n6/10\n1\n2ã€‚",
        "â—‹\nãƒãƒ³ã‚¹ãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆã€‚",
        "â—‹\n9 ARGENX SE-ADR\nã‚¢ãƒ«ã‚¸ã‚§ãƒ³Xã€‚",
        "10 STRYKER CORPORATION\nã‚¹ãƒˆãƒ©ã‚¤ã‚«ãƒ¼ã€‚",
        "â—‹\n1 ELI LILLY & CO.\nã‚¤ãƒ¼ãƒ©ã‚¤ãƒªãƒªãƒ¼ã€‚",
        "â—‹\n4 DANAHER CORPORATION\nãƒ€ãƒŠãƒãƒ¼ã€‚",
        "TICALS INC\nã‚¢ãƒ«ãƒŠã‚¤ãƒ©ãƒ ãƒ»ãƒ•ã‚¡ãƒ¼ãƒã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ã‚«ãƒ«ã‚ºã€‚",
        "BBOTT LABORATORIES\nã‚¢ãƒœãƒƒãƒˆãƒ©ãƒœãƒ©ãƒˆãƒªãƒ¼ã‚ºã€‚",
        "EALTH GROUP INC\nãƒ¦ãƒŠã‚¤ãƒ†ãƒƒãƒ‰ãƒ˜ãƒ«ã‚¹ãƒ»ã‚°ãƒ«ãƒ¼ãƒ—ã€‚",
        "NSON & JOHNSON\nã‚¸ãƒ§ãƒ³ã‚½ãƒ³ãƒ»ã‚¨ãƒ³ãƒ‰ãƒ»ã‚¸ãƒ§ãƒ³ã‚½ãƒ³ã€‚",
        "CIENTIFIC CORP\nãƒœã‚¹ãƒˆãƒ³ãƒ»ã‚µã‚¤ã‚¨ãƒ³ãƒ†ã‚£ãƒ•ã‚£ãƒƒã‚¯ã€‚"
        ]

        if afterChange in ignore_list:
            continue
    
        #---0901,fix the error disable
        if "ä¸è‡ªç„¶ãªç©ºç™½" in data["reason_type"] and fund_type == "public":
            continue
        
        result_data.append(data)
    return result_data

# ruru_update_save_corrections
@app.route('/api/save_corrections', methods=['POST'])
def save_corrections():
    try:
        data = request.get_json()
        corrections = data.get('corrections','')
        fund_type = data.get("fund_type",'')
        pdf_base64 = data.get("pdf_base64",'')
        file_name_decoding = data.get('file_name','')
        icon = data.get('icon','')

        # URL Decoding
        file_name = urllib.parse.unquote(file_name_decoding)

        # match = re.search(r'(\d{0,}(?:-\d+)?_M\d{4})', file_name)
        # if match:
        #     file_id = match.group(1)
        # else:
        #     file_id = file_name

        if not file_name or not isinstance(corrections, list):
            return jsonify({"success": False, "error": "file_name å’Œ corrections(list)."}), 400
        
        # container name Setting
        container_name = f"{fund_type}_Fund"
        # 2. Cosmos DB è¿æ¥
        container = get_db_connection(container_name)

        existing_item = list(container.query_items(
            query="SELECT * FROM c WHERE c.id = @id",
            parameters=[{"name": "@id", "value": file_name}],
            enable_cross_partition_query=True
        ))

        # corrections
        existing_corrections = []
        if existing_item:
            result = existing_item[0].get("result", {})
            existing_corrections = result.get("corrections", [])

        corrections = get_words(corrections, fund_type)
        final_corrections  = existing_corrections + corrections

        item = {
            'id': file_name,
            'fileName': file_name,
            'icon': icon,
            "result": {
                "corrections": final_corrections
            },
            'updateTime': datetime.utcnow().isoformat(),
        }
        

        if not existing_item:
            container.create_item(body=item)
            logging.info(f"âœ… Cosmos DB Update Success: {file_name}")
        else:
            existing_item[0].update(item)
            container.upsert_item(existing_item[0])

            logging.info(f"ğŸ”„ Cosmos DB update success: {file_name}")

        if not pdf_base64:
            return jsonify({"success": True, "message": "Data Update Success"}), 200
        
        try:
            pdf_bytes = base64.b64decode(pdf_base64)

            # Save temporarily (in memory or disk), generate a token or filename
            updated_pdf = add_comments_to_pdf(pdf_bytes, corrections)
           
            # store in blob then save in DB
            # temp_file = file_name+'_CHECKED.pdf'

            # rename file_name add suffix _checked
            root, ext = os.path.splitext(file_name)
            if ext.lower() == ".pdf":
                file_name = root + "_checked" + ext
            # ---------PDF -----------
            if pdf_base64:
                try:
                    pdf_bytes = base64.b64decode(pdf_base64)

                    response_data = {
                        "corrections": []
                    }

                    # Blob Upload
                    link_url = upload_checked_pdf_to_azure_storage(pdf_bytes, file_name, fund_type)
                    if not link_url:
                        return jsonify({"success": False, "error": "Blob upload failed"}), 500

                    # Cosmos DB Save
                    save_checked_pdf_cosmos(file_name, response_data, link_url, fund_type, upload_type, comment_type,icon)

                except ValueError as e:
                    return jsonify({"success": False, "error": str(e)}), 400
                except Exception as e:
                    return jsonify({"success": False, "error": str(e)}), 500

            

            # temp_path = os.path.join("/tmp", temp_filename)

            # with open(temp_path, "wb") as f:
            #     f.write(updated_pdf.read())
            #     updated_pdf.seek(0)


            return jsonify({
                "success": True,
                "corrections": corrections,
                "pdf_download_token": temp_filename
            })

        except ValueError as e:
            return jsonify({"success": False, "error": str(e)}), 400
        except Exception as e:
            return jsonify({"success": False, "error": str(e)}), 500

        
    
    except CosmosHttpResponseError as e:
        logging.error(f"Cosmos DB Error: {str(e)}")
        return jsonify({"success": False, "error": "DB Error"}), 500
    except Exception as e:
        logging.error(f"Server error: {str(e)}")
        return jsonify({"success": False, "error": "Server error"}), 500
    
#th for test api

@app.route("/api/integrated_test", methods=["POST"])
async def integrated_test():
    data = request.json
    prompt = data.get("prompt", "")
    input_data = data.get("input_data", "")
    flag_type = data.get("flag_type", "")
    base64_img = data.get("base64_img", "")
    token = token_cache.get_token()
    openai.api_key = token

    if prompt and input_data:
        question = [
            {"role": "system", "content": "ã‚ãªãŸã¯æ—¥æœ¬èªæ–‡æ›¸ã®æ ¡æ­£ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™"},
            {"role": "user", "content": input_data}
        ]
        if flag_type == "picture":
            question.append({"role": "user", "content": [{"type": "text", "text": input_data},
                            {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_img}"}}]})
        else:
            question.append({"role": "user", "content": input_data})

        response = await openai.ChatCompletion.acreate(
            deployment_id=deployment_id,  # Deploy Name
            messages=question,
            max_tokens=MAX_TOKENS,
            temperature=TEMPERATURE,
            seed=SEED  # seed
        )
        answer = response['choices'][0]['message']['content'].strip()
        return jsonify({"response_ai": answer})

@app.after_request
def after_request(response):
    try:
        data = request.json
        error_text = ""
        file_name = data.get("file_name", "")
        log_controller = get_db_connection(LOG_RECORD_CONTAINER_NAME)
        response_json = json.loads(response.get_data(as_text=True))
        if response.status_code > 200:
            response_json = json.loads(response.get_data(as_text=True))
            error_text = response_json.get("error", "")
        if file_name:
            if re.search(r"save_corrections|write_upload_save", request.path) or (
                    "check_file" in request.path and response_json.get("success", False)):
                log_info = {
                    "id": str(uuid.uuid4()),
                    "fileName": file_name,
                    "path": request.path,
                    "ip_address": request.remote_addr,
                    "result": "NG" if response.status_code > 200 else "OK",
                    "error_text": error_text,
                    "created_at": datetime.now(UTC).isoformat(),
                }
                log_controller.upsert_item(log_info)

    finally:
        return response


#10é“­æŸ„æ–°è¿½åŠ 

# PDF å®¹å™¨è·¯å¾„

def copy_row_style(ws, source_row_idx, target_row_idx):
    """
    å°† source_row_idx çš„æ ·å¼å¤åˆ¶åˆ° target_row_idx è¡Œï¼ˆåŒ…æ‹¬å­—ä½“ã€è¾¹æ¡†ã€å¡«å……ã€å¯¹é½æ–¹å¼ã€æ•°å­—æ ¼å¼ç­‰ï¼‰
    """
    for col_idx in range(1, ws.max_column + 1):
        source_cell = ws.cell(row=source_row_idx, column=col_idx)
        target_cell = ws.cell(row=target_row_idx, column=col_idx)

        if source_cell.has_style:
            target_cell.font = copy(source_cell.font)
            target_cell.border = copy(source_cell.border)
            target_cell.fill = copy(source_cell.fill)
            target_cell.number_format = copy(source_cell.number_format)
            target_cell.protection = copy(source_cell.protection)
            target_cell.alignment = copy(source_cell.alignment)


def write_wrapped_stock_cell(ws, row, col, stock_value):
    """
    å†™å…¥ stock åˆ° Excel å•å…ƒæ ¼ï¼Œè‡ªåŠ¨åœ¨è‹±æ—¥åˆ†ç•Œå¤„æ¢è¡Œå¹¶è®¾ç½® wrap_textã€‚
    """
    if not stock_value:
        return

    # âœ… åœ¨è‹±æ–‡(ASCII)å’Œæ—¥æ–‡ä¹‹é—´æ’å…¥æ¢è¡Œ
    stock_value = re.sub(r'([a-zA-Z0-9]+)([^\x00-\x7F])', r'\1\n\2', stock_value)

    cell = ws.cell(row=row, column=col, value=stock_value)
    cell.alignment = Alignment(wrap_text=True)


def extract_pdf_table(pdf_path):
    tables = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            if "çµ„å…¥ä¸Šä½10éŠ˜æŸ„ã®è§£èª¬" in text or "çµ„å…¥ä¸Šä½éŠ˜æŸ„ã®è§£èª¬" in text:
                tables += page.extract_tables()
    return tables


def extract_pdf_table_special(pdf_path):
    tables = []
    with pdfplumber.open(pdf_path) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            if "çµ„å…¥éŠ˜æŸ„è§£èª¬" in text:
                tables += page.extract_tables()
    return tables


# æ¸…é™¤æ ·å¼
# def clean_text(text):
#     if not text:
#         return ""
#     return re.sub(r'\s+', '', text.replace('\n', '').strip())
def clean_text(text):
    if pd.isna(text):   # Excel ç©ºå•å…ƒæ ¼æˆ– NaN æƒ…å†µ
        return ""
    
    text = str(text)    # æ— æ¡ä»¶è½¬å­—ç¬¦ä¸²ï¼Œé˜²æ­¢ float æŠ¥é”™

    # å…¨è§’è½¬åŠè§’ï¼Œå¹¶å»æ‰æ¢è¡Œã€ç©ºç™½ç¬¦ï¼ˆå«å…¨è§’ç©ºæ ¼ï¼‰
    text = jaconv.z2h(text, kana=False, digit=True, ascii=True)
    return re.sub(r'[\s\u3000]+', '', text.strip())


# è·å–å†³ç®—æœˆ
def get_prev_month_str():
    today = datetime.today()
    prev_month_date = (today.replace(day=1) - timedelta(days=1))
    return prev_month_date.strftime("%Y%m")


# å¾€10é“­æŸ„çš„å±¥å†è¡¨é‡Œå†™
def insert_tenbrend_history(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "filename": record["filename"],
            "id": str(uuid.uuid4()),
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": record["å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": record["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "åˆ†é¡": record["åˆ†é¡"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # âœ… å½“å‰æ—¶é—´
        }
        container.create_item(body=history_item)


def insert_tenbrend_history42(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "å…ƒ1": record["å…ƒ1"],
            "æ–°1": record["æ–°1"],
            "å…ƒ2": record["å…ƒ2"],
            "æ–°2": record["æ–°2"],
            "å…ƒ3": record["å…ƒ3"],
            "æ–°3": record["æ–°3"],
            "å…ƒ4": record["å…ƒ4"],
            "æ–°4": record["æ–°4"],
            "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": record["å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": record["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "å…ƒESGç†ç”±": record["å…ƒESGç†ç”±"],
            "æ–°ESGç†ç”±": record["æ–°ESGç†ç”±"],
            "åˆ†é¡": record["åˆ†é¡"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # âœ… å½“å‰æ—¶é—´
        }
        container.create_item(body=history_item)


def insert_tenbrend_history41(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "fcode": record["fcode"],
            "filename": record["filename"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": record["å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": record["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "å…ƒESGç†ç”±": record["å…ƒESGç†ç”±"],
            "æ–°ESGç†ç”±": record["æ–°ESGç†ç”±"],
            "åˆ†é¡": record["åˆ†é¡"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # âœ… å½“å‰æ—¶é—´
        }
        container.create_item(body=history_item)


def insert_tenbrend_history5(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "å…ƒè§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ": record["å…ƒè§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ"],
            "æ–°è§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ": record["æ–°è§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ"],
            "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": record["å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": record["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "å…ƒESGç†ç”±": record["å…ƒESGç†ç”±"],
            "æ–°ESGç†ç”±": record["æ–°ESGç†ç”±"],
            "åˆ†é¡": record["åˆ†é¡"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # âœ… å½“å‰æ—¶é—´
        }
        container.create_item(body=history_item)


# ğŸš©è¯»å–æºæ–‡ä»¶å¹¶æ›´æ–° diff_rowsï¼Œå¾€10é“­æŸ„çš„excelä¸­å†™å…¥
def update_excel_with_diff_rows(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_å…¬å‹Ÿ.xlsx"
    else:
        # å¦‚éœ€ç§å‹Ÿé€»è¾‘å¯æ‰©å±•
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_ç§å‹Ÿ.xlsx"

    # ä¸‹è½½åŸå§‹ Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        new_desc = row["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"]
        classify = row["åˆ†é¡"]
        # no = row["no"]
        try:
            no = int(row["no"])
        except (KeyError, TypeError, ValueError):
            no = 0
        months = row["months"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # è·å–è¡¨å¤´ä½ç½®
        headers = {cell.value: idx for idx, cell in enumerate(ws[3])}
        stock_col = headers.get("çµ„å…¥éŠ˜æŸ„") or headers.get("éŠ˜æŸ„")
        desc_col = headers.get("çµ„å…¥éŠ˜æŸ„è§£èª¬") or headers.get("éŠ˜æŸ„è§£èª¬")
        no_col = headers.get("No.")
        months_col = headers.get("æ±ºç®—æœˆ")

        if stock_col is None or desc_col is None:
            continue

        # æŸ¥æ‰¾ fcode æ‰€å±å—çš„èŒƒå›´
        fcode_col = headers.get("Fã‚³ãƒ¼ãƒ‰")
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "æ–°è¦éŠ˜æŸ„" and fcode_block_end:
            # æ’å…¥æ–°è§„åˆ° fcode ç»„æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œ
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)

        elif classify == "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š" and target_row_idx:
            # ç›´æ¥æ›´æ–°åŸå€¼
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)

    # ä¸Šä¼ åˆ°åŸå§‹ Blob è·¯å¾„ï¼ˆè¦†ç›–ï¼‰
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def find_column_by_keyword(header_row, keywords):
    """
    åœ¨ header_row ä¸­æŸ¥æ‰¾åŒ…å«å…³é”®å­—çš„åˆ—ç´¢å¼•
    """
    for idx, cell in enumerate(header_row):
        title = str(cell.value).strip() if cell.value else ""
        for key in keywords:
            if key in title:
                return idx
    return None


def update_excel_with_diff_rows4(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_å…¬å‹Ÿ.xlsx"
    else:
        # å¦‚éœ€ç§å‹Ÿé€»è¾‘å¯æ‰©å±•
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_ç§å‹Ÿ.xlsx"

    # ä¸‹è½½åŸå§‹ Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        new_desc = row["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"]
        classify = row["åˆ†é¡"]
        no = row["no"]
        months = row["months"]
        new_esg = row["æ–°æœ€é«˜ç›Šæ›´æ–°å›æ•°"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # è·å–è¡¨å¤´ä½ç½®
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„", "éŠ˜æŸ„"])
        desc_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„è§£èª¬", "éŠ˜æŸ„è§£èª¬"])
        esg_col = find_column_by_keyword(header_row, ["æœ€é«˜ç›Šæ›´æ–°å›æ•°"])  # ä»…å½“ä½ å¤„ç†ESGè¡¨æ—¶éœ€è¦
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["æ±ºç®—æœˆ"])
        fcode_col = find_column_by_keyword(header_row, ["Fã‚³ãƒ¼ãƒ‰"])

        if stock_col is None or desc_col is None:
            continue

        # æŸ¥æ‰¾ fcode æ‰€å±å—çš„èŒƒå›´
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "æ–°è¦éŠ˜æŸ„" and fcode_block_end:
            # æ’å…¥æ–°è§„åˆ° fcode ç»„æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œ
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            # ws.cell(row=insert_idx, column=stock_col + 1, value=stock)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š" and target_row_idx:
            # ç›´æ¥æ›´æ–°åŸå€¼
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ä¸Šä¼ åˆ°åŸå§‹ Blob è·¯å¾„ï¼ˆè¦†ç›–ï¼‰
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def update_excel_with_diff_rows42(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_å…¬å‹Ÿ.xlsx"
    else:
        # å¦‚éœ€ç§å‹Ÿé€»è¾‘å¯æ‰©å±•
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_ç§å‹Ÿ.xlsx"

    # ä¸‹è½½åŸå§‹ Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        new_1 = row["æ–°1"]
        new_2 = row["æ–°2"]
        new_3 = row["æ–°3"]
        new_4 = row["æ–°4"]
        classify = row["åˆ†é¡"]
        no = row["no"]
        months = row["months"]
        new_desc = row["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"]
        new_esg = row["æ–°ESGç†ç”±"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # è·å–è¡¨å¤´ä½ç½®
        header_row = ws[2]

        stock_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„", "éŠ˜æŸ„"])
        desc_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„è§£èª¬", "éŠ˜æŸ„è§£èª¬"])
        esg_col = find_column_by_keyword(header_row, ["ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±"])  # ä»…å½“ä½ å¤„ç†ESGè¡¨æ—¶éœ€è¦
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["æ±ºç®—æœˆ"])
        fcode_col = find_column_by_keyword(header_row, ["Fã‚³ãƒ¼ãƒ‰"])

        if stock_col is None or desc_col is None:
            continue

        # æŸ¥æ‰¾ fcode æ‰€å±å—çš„èŒƒå›´
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "æ–°è¦éŠ˜æŸ„" and fcode_block_end:
            # æ’å…¥æ–°è§„åˆ° fcode ç»„æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œ
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            # ws.cell(row=insert_idx, column=stock_col + 1, value=stock)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)
            ws.cell(row=insert_idx, column=6, value=new_1)
            ws.cell(row=insert_idx, column=7, value=new_2)
            ws.cell(row=insert_idx, column=8, value=new_3)
            ws.cell(row=insert_idx, column=9, value=new_4)

        elif classify == "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š" and target_row_idx:
            # ç›´æ¥æ›´æ–°åŸå€¼
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)
            ws.cell(row=target_row_idx, column=6, value=new_1)
            ws.cell(row=target_row_idx, column=7, value=new_2)
            ws.cell(row=target_row_idx, column=8, value=new_3)
            ws.cell(row=target_row_idx, column=9, value=new_4)

    # ä¸Šä¼ åˆ°åŸå§‹ Blob è·¯å¾„ï¼ˆè¦†ç›–ï¼‰
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def update_excel_with_diff_rows41(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_å…¬å‹Ÿ.xlsx"
    else:
        # å¦‚éœ€ç§å‹Ÿé€»è¾‘å¯æ‰©å±•
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_ç§å‹Ÿ.xlsx"

    # ä¸‹è½½åŸå§‹ Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        classify = row["åˆ†é¡"]
        no = row["no"]
        months = row["months"]
        new_desc = row["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"]
        new_esg = row["æ–°ESGç†ç”±"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # è·å–è¡¨å¤´ä½ç½®
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„", "éŠ˜æŸ„"])
        desc_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„è§£èª¬", "éŠ˜æŸ„è§£èª¬", "çµ„å…¥ç™ºè¡Œä½“è§£èª¬"])
        esg_col = find_column_by_keyword(header_row, ["ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±",
                                                      "è„±ç‚­ç´ ç¤¾ä¼šã®å®Ÿç¾ã¸ã®è²¢çŒ®ã¨ä¼æ¥­è©•ä¾¡ã®ãƒã‚¤ãƒ³ãƒˆ"])  # ä»…å½“ä½ å¤„ç†ESGè¡¨æ—¶éœ€è¦
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["æ±ºç®—æœˆ"])
        fcode_col = find_column_by_keyword(header_row, ["Fã‚³ãƒ¼ãƒ‰"])

        if stock_col is None or desc_col is None:
            continue

        # æŸ¥æ‰¾ fcode æ‰€å±å—çš„èŒƒå›´
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "æ–°è¦éŠ˜æŸ„" and fcode_block_end:
            # æ’å…¥æ–°è§„åˆ° fcode ç»„æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œ
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            # ws.cell(row=insert_idx, column=stock_col + 1, value=stock)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š" and target_row_idx:
            # ç›´æ¥æ›´æ–°åŸå€¼
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ä¸Šä¼ åˆ°åŸå§‹ Blob è·¯å¾„ï¼ˆè¦†ç›–ï¼‰
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def update_excel_with_diff_rows_shang(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_å…¬å‹Ÿ.xlsx"
    else:
        # å¦‚éœ€ç§å‹Ÿé€»è¾‘å¯æ‰©å±•
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_ç§å‹Ÿ.xlsx"

    # ä¸‹è½½åŸå§‹ Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        new_desc = row["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"]
        classify = row["åˆ†é¡"]
        no = row["no"]
        months = row["months"]
        new_esg = row["æ–°ä¸Šå ´å¹´æœˆ"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # è·å–è¡¨å¤´ä½ç½®
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„", "éŠ˜æŸ„"])
        desc_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„è§£èª¬", "éŠ˜æŸ„è§£èª¬"])
        esg_col = find_column_by_keyword(header_row, ["ä¸Šå ´å¹´æœˆ"])  # ä»…å½“ä½ å¤„ç†ESGè¡¨æ—¶éœ€è¦
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["æ±ºç®—æœˆ"])
        fcode_col = find_column_by_keyword(header_row, ["Fã‚³ãƒ¼ãƒ‰"])

        if stock_col is None or desc_col is None:
            continue

        # æŸ¥æ‰¾ fcode æ‰€å±å—çš„èŒƒå›´
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "æ–°è¦éŠ˜æŸ„" and fcode_block_end:
            # æ’å…¥æ–°è§„åˆ° fcode ç»„æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œ
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            # ws.cell(row=insert_idx, column=stock_col + 1, value=stock)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š" and target_row_idx:
            # ç›´æ¥æ›´æ–°åŸå€¼
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ä¸Šä¼ åˆ°åŸå§‹ Blob è·¯å¾„ï¼ˆè¦†ç›–ï¼‰
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def update_excel_with_diff_rows5(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_å…¬å‹Ÿ.xlsx"
    else:
        # å¦‚éœ€ç§å‹Ÿé€»è¾‘å¯æ‰©å±•
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_ç§å‹Ÿ.xlsx"

    # ä¸‹è½½åŸå§‹ Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:

        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        classify = row["åˆ†é¡"]
        no = row["no"]
        months = row["months"]
        new_keti = row["æ–°è§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ"]
        new_desc = row["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"]
        new_esg = row["æ–°ESGç†ç”±"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # è·å–è¡¨å¤´ä½ç½®
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„", "éŠ˜æŸ„"])
        keti_col = find_column_by_keyword(header_row,
                                          ["è§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ", "æ¥­ç¨®", "æŠ•è³‡åˆ†é‡", "åˆ†é‡", "ç›®æŒ‡ã™ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ"])
        desc_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„è§£èª¬", "éŠ˜æŸ„è§£èª¬"])
        esg_col = find_column_by_keyword(header_row, ["ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±",
                                                      "ç¤¾ä¼šçš„èª²é¡Œã®è§£æ±ºã¨åˆ©ç›Šæˆé•·ã‚’ä¸¡ç«‹ã•ã›ã‚‹ãƒã‚¤ãƒ³ãƒˆ"])
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["æ±ºç®—æœˆ"])
        fcode_col = find_column_by_keyword(header_row, ["Fã‚³ãƒ¼ãƒ‰"])

        if stock_col is None or desc_col is None:
            continue

        # æŸ¥æ‰¾ fcode æ‰€å±å—çš„èŒƒå›´
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx

                    break

        if classify == "æ–°è¦éŠ˜æŸ„" and fcode_block_end:
            # æ’å…¥æ–°è§„åˆ° fcode ç»„æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œ
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=keti_col + 1, value=new_keti)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š" and target_row_idx:
            # ç›´æ¥æ›´æ–°åŸå€¼
            ws.cell(row=target_row_idx, column=keti_col + 1, value=new_keti)
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ä¸Šä¼ åˆ°åŸå§‹ Blob è·¯å¾„ï¼ˆè¦†ç›–ï¼‰
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


# 10é“­æŸ„çš„check
def check_tenbrend(filename, fund_type):
    try:
        fcode = os.path.basename(filename).split("_")[0]

        if fund_type == 'private':
            TENBREND_CONTAINER_NAME = 'tenbrend_private'
        else:
            TENBREND_CONTAINER_NAME = 'tenbrend'

        container = get_db_connection(TENBREND_CONTAINER_NAME)

        query = "SELECT c.sheetname FROM c WHERE CONTAINS(c.fcode, @fcode)"
        parameters = [{"name": "@fcode", "value": fcode}]
        result = list(container.query_items(query=query, parameters=parameters, enable_cross_partition_query=True))

        if not result:
            return "æ•°æ®åº“æ²¡æœ‰æŸ¥åˆ°æ•°æ®,æ²¡æœ‰è¿™ä¸ªfcodeçš„æ•°æ®"

        sheetname = result[0]["sheetname"]
        pdf_url = f"{PDF_DIR}/{filename}"

        if sheetname == "éå»åˆ†æ•´ç†3åˆ—":
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDFä¸‹è½½å¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°pdf"
            if fcode in ['140193', '140386','140565-6','180295-8',"180291-2"]:
                # å°† .pdf æ›¿æ¢ä¸º .xlsx ä½œä¸º Excel æ–‡ä»¶è·¯å¾„
                excel_url = pdf_url.replace(".pdf", ".xlsx")

                # ä¸‹è½½ Excel æ–‡ä»¶å†…å®¹
                response = requests.get(excel_url)
                if response.status_code != 200:
                    return "Excelä¸‹è½½å¤±è´¥"

                # è½¬ä¸º BytesIO å¯¹è±¡ä¼ ç»™ extract_excel_table3
                excel_file = io.BytesIO(response.content)
                tables = extract_excel_table3(excel_file,fcode)
            elif fcode in ["140675", "140655-6", "140695-6"]:
                tables = extract_pdf_table_special(io.BytesIO(pdf_response.content))
            else:

                tables = extract_pdf_table(io.BytesIO(pdf_response.content))
            if not tables:
                return "PDFä¸­æœªæå–åˆ°è¡¨æ ¼"

            excel_url = f"{PDF_DIR}/10mingbing.xlsx"
            excel_response = requests.get(excel_url)
            if excel_response.status_code != 200:
                return "Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œä¸èƒ½æ‰“å¼€excel"

            wb = load_workbook(filename=io.BytesIO(excel_response.content))
            ws = wb.active

            seen_stocks = set()
            unique_rows = []
            if fcode in ['140193', '140386','140565-6','180295-8']:
                for row in tables:
                    stock = clean_text(row[0])
                    desc = clean_text(row[1])
                    seen_stocks.add(stock)
                    unique_rows.append([stock, desc])

                    if len(unique_rows) >= 10:
                        break                
            else:
                for table in tables:
                    for row in table:
                        if len(row) < 3:
                            continue
                        if (row[1] and ('çµ„å…¥éŠ˜æŸ„' in row[1] or 'éŠ˜æŸ„' in row[1])) and \
                                ((row[2] and 'éŠ˜æŸ„è§£èª¬' in row[2]) or (len(row) > 3 and row[3] and 'éŠ˜æŸ„è§£èª¬' in row[3])):
                            continue
                        if not row or str(row[0]).strip() not in [str(i) for i in range(1, 11)]:
                            continue
                        if not row[1]:
                            pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[2]), flags=re.IGNORECASE)
                        else:

                            pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[1]), flags=re.IGNORECASE)
                        if not row[2]:
                            pdf_desc = clean_text(row[3])
                        else:
                            pdf_desc = clean_text(row[2])

                        if pdf_stock and not pdf_desc:
                            alt_desc = clean_text(row[3]) if len(row) > 3 else ""
                            if alt_desc:
                                pdf_desc = alt_desc
                            else:
                                continue

                        if not pdf_stock or pdf_stock in seen_stocks:
                            continue

                        seen_stocks.add(pdf_stock)
                        unique_rows.append([pdf_stock, pdf_desc])
                        if len(unique_rows) >= 10:
                            break
                    if len(unique_rows) >= 10:
                        break

            # âœ… ä¸ Cosmos DB æ¯”å¯¹å¹¶æ’å…¥å¿…è¦è®°å½•
            diff_rows = []
            for stock, desc in unique_rows:
                # æŸ¥è¯¢å½“å‰é¡¹æ˜¯å¦å­˜åœ¨
                query = """
                    SELECT * FROM c
                    WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
                """
                params = [
                    {"name": "@sheetname", "value": sheetname},
                    {"name": "@fcode", "value": fcode},
                    {"name": "@stock", "value": stock}
                ]
                matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

                # return matched[0]["çµ„å…¥éŠ˜æŸ„è§£èª¬"]

                if matched:
                    old_desc = clean_text(matched[0]["çµ„å…¥éŠ˜æŸ„è§£èª¬"])

                    if old_desc != desc:
                        # âœ… å·®å¼‚æ›´æ–°
                        matched_item = matched[0]
                        matched_item["çµ„å…¥éŠ˜æŸ„è§£èª¬"] = desc
                        container.replace_item(item=matched_item["id"], body=matched_item)

                        diff_rows.append({
                            "filename": filename,
                            "fcode": fcode,
                            "sheetname": sheetname,
                            "no": 0,
                            "months": "",
                            "stocks": stock,
                            "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": old_desc,
                            "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                            "åˆ†é¡": "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š"
                        })
                else:
                    # âœ… æ–°è¦æ’å…¥
                    query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                    max_no = list(container.query_items(
                        query=query_max,
                        parameters=[{"name": "@fcode", "value": fcode}],
                        enable_cross_partition_query=True
                    ))[0] or 0

                    new_item = {
                        "id": str(uuid.uuid4()),
                        "filename": filename,
                        "fcode": fcode,
                        "months": get_prev_month_str(),  # å‡1æœˆ
                        "no": max_no + 1,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "ã‚³ãƒ¡ãƒ³ãƒˆ": "",
                        "åˆ†é¡": "æ–°è¦éŠ˜æŸ„"
                    }
                    container.create_item(body=new_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": "",
                        "no": max_no + 1,
                        "months": get_prev_month_str(),
                        "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "åˆ†é¡": "æ–°è¦éŠ˜æŸ„"
                    })
            insert_tenbrend_history(diff_rows)
            # update_excel_with_diff_rows(diff_rows, fund_type)

            return diff_rows or "å…¨éƒ¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°"

        elif sheetname == "éå»åˆ†æ•´ç†4åˆ—ESGä¸€ç·’":
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDFä¸‹è½½å¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°pdf"

            tables = extract_pdf_table(io.BytesIO(pdf_response.content))
            if not tables:
                return "PDFä¸­æœªæå–åˆ°è¡¨æ ¼"

            excel_url = f"{PDF_DIR}/10mingbing.xlsx"
            excel_response = requests.get(excel_url)
            if excel_response.status_code != 200:
                return "Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œä¸èƒ½æ‰“å¼€excel"

            wb = load_workbook(filename=io.BytesIO(excel_response.content))
            ws = wb.active

            seen_stocks = set()
            unique_rows = []

            for table in tables:
                header_found = False
                for row in table:
                    if len(row) < 4:
                        continue

                    if (row[1] == "çµ„å…¥éŠ˜æŸ„" and
                            "æœ€é«˜ç›Šæ›´æ–°å›æ•°" in row[2] and
                            "çµ„å…¥éŠ˜æŸ„è§£èª¬" in row[3]):
                        header_found = True
                        continue
                    if not header_found:
                        continue
                    if not row or str(row[0]).strip() not in [str(i) for i in range(1, 11)]:
                        continue
                    pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[1]), flags=re.IGNORECASE)
                    pdf_esg = clean_text(row[2])
                    pdf_desc = clean_text(row[3])

                    if not pdf_stock or pdf_stock in seen_stocks:
                        continue

                    seen_stocks.add(pdf_stock)
                    unique_rows.append([pdf_stock, pdf_desc, pdf_esg])
                    if len(unique_rows) >= 10:
                        break
                if len(unique_rows) >= 10:
                    break

            # âœ… Excel æœ€åä¸€è¡Œå†™å…¥ï¼ˆè°ƒè¯•ç”¨ï¼‰
            for row in unique_rows:
                ws.append(row)

            output_stream = io.BytesIO()
            wb.save(output_stream)
            output_stream.seek(0)
            container_client = get_storage_container()
            blob_client = container_client.get_blob_client("10mingbing.xlsx")
            blob_client.upload_blob(output_stream, overwrite=True)

            # âœ… æ¯”å¯¹é€»è¾‘
            diff_rows = []
            for stock, desc, esg in unique_rows:
                query = """
                    SELECT * FROM c
                    WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
                """
                params = [
                    {"name": "@sheetname", "value": sheetname},
                    {"name": "@fcode", "value": fcode},
                    {"name": "@stock", "value": stock}
                ]
                matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

                if matched:
                    old_desc = clean_text(matched[0].get("çµ„å…¥éŠ˜æŸ„è§£èª¬", ""))
                    old_esg = clean_text(matched[0].get("æœ€é«˜ç›Šæ›´æ–°å›æ•°", ""))

                    classify = None
                    if old_desc != desc or old_esg != esg:
                        classify = "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š"

                    if classify:
                        matched_item = matched[0]
                        matched_item["çµ„å…¥éŠ˜æŸ„è§£èª¬"] = desc
                        matched_item["æœ€é«˜ç›Šæ›´æ–°å›æ•°"] = esg
                        container.replace_item(item=matched_item["id"], body=matched_item)

                        diff_rows.append({
                            "filename": filename,
                            "fcode": fcode,
                            "sheetname": sheetname,
                            "stocks": stock,
                            "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": old_desc,
                            "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                            "å…ƒæœ€é«˜ç›Šæ›´æ–°å›æ•°": old_esg,
                            "æ–°æœ€é«˜ç›Šæ›´æ–°å›æ•°": esg,
                            "åˆ†é¡": classify,
                            "no": matched_item.get("no", 0),
                            "months": matched_item.get("months", ""),
                            "åˆ†é¡": "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š"
                        })

                else:
                    query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                    max_no = list(container.query_items(
                        query=query_max,
                        parameters=[{"name": "@fcode", "value": fcode}],
                        enable_cross_partition_query=True
                    ))[0] or 0

                    new_item = {
                        "id": str(uuid.uuid4()),
                        "filename": filename,
                        "fcode": fcode,
                        "months": get_prev_month_str(),
                        "no": max_no + 1,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "æœ€é«˜ç›Šæ›´æ–°å›æ•°": esg,
                        "ã‚³ãƒ¡ãƒ³ãƒˆ": ""
                    }
                    container.create_item(body=new_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": "",
                        "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "æœ€é«˜ç›Šæ›´æ–°å›æ•°": "",
                        "æ–°æœ€é«˜ç›Šæ›´æ–°å›æ•°": esg,
                        "åˆ†é¡": "æ–°è¦éŠ˜æŸ„",
                        "no": max_no + 1,
                        "months": get_prev_month_str()
                    })

            insert_tenbrend_history(diff_rows)
            # update_excel_with_diff_rows4(diff_rows, fund_type)

            return diff_rows or "å…¨éƒ¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°"

        elif sheetname == "éå»åˆ†æ•´ç†4åˆ—+4åˆ—ã€‡äºŒè¡Œ":
            return handle_sheet_plus42(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname == "éå»åˆ†æ•´ç†4åˆ—ï¼†ï¼ˆ4+1ï¼‰äºŒè¡Œ":
            return handle_sheet_plus41(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname == "éå»åˆ†æ•´ç†4åˆ—ä¸Šå ´å¹´æœˆ":
            return handle_sheet_plus4(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname == "éå»åˆ†æ•´ç†5åˆ—ï¼†ï¼ˆ5+1ï¼‰":
            return handle_sheet_plus5(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname in ["300355", "300469", "300481"]:
            return handle_sheet_plus_si4(pdf_url, fcode, sheetname, fund_type, container, filename)
        elif sheetname in ["300449", "300462", "300387"]:
            return handle_sheet_plus_si5(pdf_url, fcode, sheetname, fund_type, container, filename)

        else:
            return "æ‰¾ä¸åˆ°è¿™ä¸ªsheeté¡µ"

    except Exception as e:
        return f"âŒ check_tenbrend error: {str(e)}"


def handle_sheet_plus42(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        pdf_response = requests.get(pdf_url)
        if pdf_response.status_code != 200:
            return "PDFä¸‹è½½å¤±è´¥"

        tables = extract_pdf_table(io.BytesIO(pdf_response.content))
        if not tables:
            return "PDFä¸­æœªæå–åˆ°è¡¨æ ¼"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥"

        wb = load_workbook(filename=io.BytesIO(excel_response.content))
        ws = wb.active

        seen_stocks = set()
        unique_rows = []

        i = 0

        # åˆå¹¶æ‰€æœ‰è¡¨æ ¼è¡Œä¸ºä¸€ä¸ªå¤§åˆ—è¡¨
        all_rows = [row for table in tables for row in table]

        while i < len(all_rows) - 1:
            row1 = all_rows[i]
            row2 = all_rows[i + 1]

            if len(row1) < 2 or str(row1[0]).strip() not in [str(n) for n in range(1, 11)]:
                i += 1
                continue

            stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row1[1]), flags=re.IGNORECASE)
            if not stock or stock in seen_stocks:
                i += 1  # â— è¿™é‡Œæ˜¯è·³1è¡Œè€Œä¸æ˜¯2è¡Œ
                continue

            v1 = clean_text(row1[3])
            v2 = clean_text(row1[4])
            v3 = clean_text(row1[5])
            v4 = clean_text(row1[6])
            desc = clean_text(row1[7]) if len(row1) > 7 else ""
            esg = clean_text(row2[7]) if len(row2) > 7 else ""

            seen_stocks.add(stock)
            unique_rows.append([stock, v1, v2, v3, v4, desc, esg])
            i += 2  # âœ… åªæœ‰è¿½åŠ æˆåŠŸæ‰è·³è¿‡2è¡Œ

            if len(unique_rows) >= 10:
                break

        diff_rows = []
        for row in unique_rows:
            stock, v1, v2, v3, v4, desc, esg = row
            query = """
                SELECT * FROM c
                WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
            """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_1 = clean_text(matched[0].get("1", ""))
                old_2 = clean_text(matched[0].get("2", ""))
                old_3 = clean_text(matched[0].get("3", ""))
                old_4 = clean_text(matched[0].get("4", ""))
                old_desc = clean_text(matched[0].get("çµ„å…¥éŠ˜æŸ„è§£èª¬", ""))
                old_esg = clean_text(matched[0].get("ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±", ""))
                if old_desc != desc or old_esg != esg or old_1 != v1 or old_2 != v2 or old_3 != v3 or old_4 != v4:
                    matched_item = matched[0]
                    matched_item.update({
                        "1": v1,
                        "2": v2,
                        "3": v3,
                        "4": v4,
                        "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±": esg
                    })
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "æ–°1": v1,
                        "å…ƒ1": old_1,
                        "æ–°2": v2,
                        "å…ƒ2": old_2,
                        "æ–°3": v3,
                        "å…ƒ3": old_3,
                        "æ–°4": v4,
                        "å…ƒ4": old_4,
                        "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": old_desc,
                        "æ–°ESGç†ç”±": esg,
                        "å…ƒESGç†ç”±": old_esg,
                        "åˆ†é¡": "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š",
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", "")
                    })
            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "1": v1,
                    "2": v2,
                    "3": v3,
                    "4": v4,
                    "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±": esg,
                    "no": max_no + 1,
                    "months": get_prev_month_str(),
                    "ã‚³ãƒ¡ãƒ³ãƒˆ": "",
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "æ–°1": v1,
                    "å…ƒ1": "",
                    "æ–°2": v2,
                    "å…ƒ2": "",
                    "æ–°3": v3,
                    "å…ƒ3": "",
                    "æ–°4": v4,
                    "å…ƒ4": "",
                    "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": "",
                    "æ–°ESGç†ç”±": esg,
                    "å…ƒESGç†ç”±": "",
                    "åˆ†é¡": "æ–°è¦éŠ˜æŸ„",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history42(diff_rows)
        # update_excel_with_diff_rows42(diff_rows, fund_type)

        return diff_rows or "å…¨éƒ¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°"

    except Exception as e:
        return f"âŒ handle_sheet_plus42 error: {str(e)}"


def extract_excel_table(file_like,fcode):
    try:
        # æ”¯æŒä¼ å…¥ BytesIO æˆ–æœ¬åœ°è·¯å¾„
        if fcode == "180371-2":
            sheet_name = "PIC_24_S"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        elif fcode == "140764-5":
            sheet_name = "éŠ˜æŸ„ç´¹ä»‹"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        elif fcode == "140793-6":
            sheet_name = "çµ„å…¥éŠ˜æŸ„(å‚µåˆ¸ãƒ»1)"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        else:
            sheet_name = "éŠ˜æŸ„è§£èª¬"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
    except Exception as e:
        print(f"âŒ Excel è¯»å–å¤±è´¥: {e}")
        return []

    df = df.reset_index(drop=True)
    results = []

    for i in range(len(df) - 1):
        index_val = str(df.iloc[i, 0]).strip()

        if index_val in [str(n) for n in range(1, 11)]:  # åªå¤„ç†1~10
            stock = clean_text(df.iloc[i, 1])
            desc = clean_text(df.iloc[i, 2])
            esg = clean_text(df.iloc[i + 1, 2])
            if stock:
                results.append([stock, desc, esg])
                
    if fcode == "140793-6":
        sheet_name = "çµ„å…¥éŠ˜æŸ„(å‚µåˆ¸ãƒ»2)"
        df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        df = df.reset_index(drop=True)
        for i in range(len(df) - 1):
            index_val = str(df.iloc[i, 0]).strip()

            if index_val in [str(n) for n in range(1, 11)]:  # åªå¤„ç†1~10
                stock = clean_text(df.iloc[i, 1])
                desc = clean_text(df.iloc[i, 2])
                esg = clean_text(df.iloc[i + 1, 2])
                if stock:
                    results.append([stock, desc, esg])
    return results

def extract_excel_table3(file_like,fcode):
    try:
        # æ”¯æŒä¼ å…¥ BytesIO æˆ–æœ¬åœ°è·¯å¾„
        if fcode == "140193":
            sheet_name = "140193"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:E", dtype=str)
        elif fcode == "140386":
            sheet_name = "140386 (3)"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:E", dtype=str)
        elif fcode == "140565-6":
            sheet_name = "éŠ˜æŸ„è§£èª¬å…¥åŠ›ï½¼ï½°ï¾„"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        elif fcode == "180291-2":
            sheet_name = "ä¸Šä½10éŠ˜æŸ„ã‚³ãƒ¡ãƒ³ãƒˆ"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
        else:
            sheet_name = "éŠ˜æŸ„è§£èª¬"
            df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:E", dtype=str)
    except Exception as e:
        print(f"âŒ Excel è¯»å–å¤±è´¥: {e}")
        return []

    df = df.reset_index(drop=True)
    results = []

    for i in range(len(df) - 1):
        index_val = str(df.iloc[i, 0]).strip()

        if index_val in [str(n) for n in range(1, 11)]:  # åªå¤„ç†1~10
            stock = clean_text(df.iloc[i, 1])
            if fcode == "140193":
                desc = clean_text(df.iloc[i, 4])
            elif fcode == "140565-6":
                desc = clean_text(df.iloc[i, 3])
            else:
                desc = clean_text(df.iloc[i, 2])
            if stock:
                results.append([stock, desc])

    return results

def handle_sheet_plus41(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        if fcode in ['140752', '140302-3','180371-2','140389-90','140764-5','140793-6']:
            # å°† .pdf æ›¿æ¢ä¸º .xlsx ä½œä¸º Excel æ–‡ä»¶è·¯å¾„
            excel_url = pdf_url.replace(".pdf", ".xlsx")

            # ä¸‹è½½ Excel æ–‡ä»¶å†…å®¹
            response = requests.get(excel_url)
            if response.status_code != 200:
                return "Excelä¸‹è½½å¤±è´¥"

            # è½¬ä¸º BytesIO å¯¹è±¡ä¼ ç»™ extract_excel_table
            excel_file = io.BytesIO(response.content)
            tables = extract_excel_table(excel_file,fcode)
        else:
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDFä¸‹è½½å¤±è´¥"

            tables = extract_structured_tables(io.BytesIO(pdf_response.content))

        if not tables:
            return "PDFä¸­æœªæå–åˆ°è¡¨æ ¼"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥"

        wb = load_workbook(filename=io.BytesIO(excel_response.content))
        ws = wb.active

        seen_stocks = set()
        unique_rows = []

        i = 0

        # åˆå¹¶æ‰€æœ‰è¡¨æ ¼è¡Œä¸ºä¸€ä¸ªå¤§åˆ—è¡¨
        all_rows = tables

        if fcode in ['140752', '140302-3','180371-2','140389-90','140764-5','140793-6']:
            for row in all_rows:
                stock = clean_text(row[0])
                desc = clean_text(row[1])
                esg = clean_text(row[2])
                seen_stocks.add(stock)
                unique_rows.append([stock, desc, esg])

                if len(unique_rows) >= 10:
                    break
        else:

            while i < len(all_rows) - 1:
                row1 = all_rows[i]
                row2 = all_rows[i + 1]

                if len(row1) < 2 or str(row1[0]).strip() not in [str(n) for n in range(1, 11)]:
                    i += 1
                    continue

                stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row1[1]), flags=re.IGNORECASE)
                
                if not stock or stock in seen_stocks:
                    i += 1  # â— è¿™é‡Œæ˜¯è·³1è¡Œè€Œä¸æ˜¯2è¡Œ
                    continue
                if fcode in ["140793-6", "140764-5"]:
                    desc = clean_text(row1[3]) if len(row1) > 2 else ""
                    esg = clean_text(row2[3]) if len(row2) > 2 else ""
                else:

                    desc = clean_text(row1[2]) if len(row1) > 2 else ""
                    esg = clean_text(row2[2]) if len(row2) > 2 else ""

                seen_stocks.add(stock)
                unique_rows.append([stock, desc, esg])
                i += 2  # âœ… åªæœ‰è¿½åŠ æˆåŠŸæ‰è·³è¿‡2è¡Œ

                if len(unique_rows) >= 10:
                    break

        diff_rows = []
        for row in unique_rows:
            stock, desc, esg = row
            query = """
                SELECT * FROM c
                WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
            """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_desc = clean_text(matched[0].get("çµ„å…¥éŠ˜æŸ„è§£èª¬", ""))
                old_esg = clean_text(matched[0].get("ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±", ""))
                if old_desc != desc or old_esg != esg:
                    matched_item = matched[0]
                    matched_item.update({
                        "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±": esg
                    })
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": old_desc,
                        "æ–°ESGç†ç”±": esg,
                        "å…ƒESGç†ç”±": old_esg,
                        "åˆ†é¡": "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š",
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", "")
                    })
            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±": esg,
                    "no": max_no + 1,
                    "months": get_prev_month_str(),
                    "ã‚³ãƒ¡ãƒ³ãƒˆ": "",
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": "",
                    "æ–°ESGç†ç”±": esg,
                    "å…ƒESGç†ç”±": "",
                    "åˆ†é¡": "æ–°è¦éŠ˜æŸ„",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history41(diff_rows)
        # update_excel_with_diff_rows41(diff_rows, fund_type)

        return diff_rows or "å…¨éƒ¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°"

    except Exception as e:
        return f"âŒ handle_sheet_plus41 error: {str(e)}"


# å¾€10é“­æŸ„çš„å±¥å†è¡¨é‡Œå†™
def insert_tenbrend_history4(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": record["å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": record["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "å…ƒä¸Šå ´å¹´æœˆ": record["å…ƒä¸Šå ´å¹´æœˆ"],
            "æ–°ä¸Šå ´å¹´æœˆ": record["æ–°ä¸Šå ´å¹´æœˆ"],
            "åˆ†é¡": record["åˆ†é¡"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # âœ… å½“å‰æ—¶é—´
        }
        container.create_item(body=history_item)
    
def format_date(value):
    try:
        if pd.isna(value):
            return ""

        # å°è¯•å°†çº¯æ•°å­—å­—ç¬¦ä¸²å½“ä½œæ•°å­—å¤„ç†
        try:
            value_numeric = float(value)
            is_numeric = True
        except (ValueError, TypeError):
            is_numeric = False

        if is_numeric:
            base = datetime(1899, 12, 30)  # Excelåºåˆ—å·èµ·ç‚¹
            real_date = base + timedelta(days=value_numeric)
            return f"{real_date.year}å¹´{real_date.month}æœˆ"

        # datetime æˆ– pd.Timestamp ç±»å‹
        elif isinstance(value, (datetime, pd.Timestamp)):
            return f"{value.year}å¹´{value.month}æœˆ"

        # å…¶ä½™å­—ç¬¦ä¸²
        else:
            parsed = pd.to_datetime(str(value), errors='coerce')
            if pd.isna(parsed):
                return str(value)
            return f"{parsed.year}å¹´{parsed.month}æœˆ"

    except Exception:
        return str(value)


def extract_excel_table4(file_like):
    try:
        # æ”¯æŒä¼ å…¥ BytesIO æˆ–æœ¬åœ°è·¯å¾„
        sheet_name = "çµ„å…¥éŠ˜æŸ„"
        df = pd.read_excel(file_like, sheet_name=sheet_name, header=1, usecols="A:D", dtype=str)
    except Exception as e:
        print(f"âŒ Excel è¯»å–å¤±è´¥: {e}")
        return []

    df = df.reset_index(drop=True)
    results = []

    for i in range(len(df)):
        index_val = str(df.iloc[i, 0]).strip()
        if index_val in [str(n) for n in range(1, 11)]:
            stock = clean_text(df.iloc[i, 1])
            desc = clean_text(df.iloc[i, 2])
            date_val = df.iloc[i, 3]
            date_str = format_date(date_val)
            if stock:
                results.append([stock, desc, date_str])

    return results


def handle_sheet_plus4(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        if fcode in ['140749']:
            # å°† .pdf æ›¿æ¢ä¸º .xlsx ä½œä¸º Excel æ–‡ä»¶è·¯å¾„
            excel_url = pdf_url.replace(".pdf", ".xlsx")

            # ä¸‹è½½ Excel æ–‡ä»¶å†…å®¹
            response = requests.get(excel_url)
            if response.status_code != 200:
                return "Excelä¸‹è½½å¤±è´¥"

            # è½¬ä¸º BytesIO å¯¹è±¡ä¼ ç»™ extract_excel_table4
            excel_file = io.BytesIO(response.content)
            tables = extract_excel_table4(excel_file)
            
        else:
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDFä¸‹è½½å¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°pdf"

            tables = extract_pdf_table(io.BytesIO(pdf_response.content))
            if not tables:
                return "PDFä¸­æœªæå–åˆ°è¡¨æ ¼"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œä¸èƒ½æ‰“å¼€excel"

        wb = load_workbook(filename=io.BytesIO(excel_response.content))
        ws = wb.active

        seen_stocks = set()
        unique_rows = []
        if fcode in ['140749']:
            for row in tables:

                pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[0]), flags=re.IGNORECASE)

                pdf_desc = clean_text(row[1])
                pdf_esg = re.sub(r"(\d{4})å¹´(\d{1,2})æœˆ", lambda m: f"{m.group(1)}/{int(m.group(2))}/1", clean_text(row[2]))

                seen_stocks.add(pdf_stock)
                unique_rows.append([pdf_stock, pdf_desc, pdf_esg])
                if len(unique_rows) >= 10:
                    break
        else:

            for table in tables:
                header_found = False
                for row in table:
                    if len(row) < 4:
                        continue

                    if not row or str(row[0]).strip() not in [str(i) for i in range(1, 11)]:
                        continue
                    pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[1]), flags=re.IGNORECASE)
                    pdf_desc = clean_text(row[2])
                    pdf_esg = re.sub(r"(\d{4})å¹´(\d{1,2})æœˆ", lambda m: f"{m.group(1)}/{int(m.group(2))}/1",
                                     clean_text(row[3]))

                    if not pdf_stock or pdf_stock in seen_stocks:
                        continue

                    seen_stocks.add(pdf_stock)
                    unique_rows.append([pdf_stock, pdf_desc, pdf_esg])
                    if len(unique_rows) >= 10:
                        break
                if len(unique_rows) >= 10:
                    break

        # âœ… Excel æœ€åä¸€è¡Œå†™å…¥ï¼ˆè°ƒè¯•ç”¨ï¼‰
        for row in unique_rows:
            ws.append(row)

        output_stream = io.BytesIO()
        wb.save(output_stream)
        output_stream.seek(0)
        container_client = get_storage_container()
        blob_client = container_client.get_blob_client("10mingbing.xlsx")
        blob_client.upload_blob(output_stream, overwrite=True)

        # âœ… æ¯”å¯¹é€»è¾‘
        diff_rows = []
        for stock, desc, esg in unique_rows:
            query = """
                    SELECT * FROM c
                    WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
                """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_desc = clean_text(matched[0].get("çµ„å…¥éŠ˜æŸ„è§£èª¬", ""))
                old_esg = clean_text(matched[0].get("ä¸Šå ´å¹´æœˆ", ""))

                classify = None
                if old_desc != desc or old_esg != esg:
                    classify = "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š"

                if classify:
                    matched_item = matched[0]
                    matched_item["çµ„å…¥éŠ˜æŸ„è§£èª¬"] = desc
                    matched_item["ä¸Šå ´å¹´æœˆ"] = esg
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": old_desc,
                        "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "å…ƒä¸Šå ´å¹´æœˆ": old_esg,
                        "æ–°ä¸Šå ´å¹´æœˆ": esg,
                        "åˆ†é¡": classify,
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", ""),
                        "åˆ†é¡": "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š"
                    })

            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "months": get_prev_month_str(),
                    "no": max_no + 1,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "ä¸Šå ´å¹´æœˆ": esg,
                    "ã‚³ãƒ¡ãƒ³ãƒˆ": ""
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": "",
                    "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "å…ƒä¸Šå ´å¹´æœˆ": "",
                    "æ–°ä¸Šå ´å¹´æœˆ": esg,
                    "åˆ†é¡": "æ–°è¦éŠ˜æŸ„",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history4(diff_rows)
        # update_excel_with_diff_rows_shang(diff_rows, fund_type)

        return diff_rows or "å…¨éƒ¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°"

    except Exception as e:
        return f"âŒ handle_sheet_4plus41 error: {str(e)}"


def extract_excel_table5(excel_file,fcode):
    try:
        # æ”¯æŒä¼ å…¥ BytesIO æˆ–æœ¬åœ°è·¯å¾„
        if fcode == "140312-3":
            sheet_name = "PIC_24_S"
            df = pd.read_excel(excel_file, sheet_name=sheet_name, header=1, usecols="A:G", dtype=str)
        # æ”¯æŒä¼ å…¥ BytesIO æˆ–æœ¬åœ°è·¯å¾„
        else:
            sheet_name = "éŠ˜æŸ„è§£èª¬"
            df = pd.read_excel(excel_file, sheet_name=sheet_name, header=1, usecols="A:G", dtype=str)

    except Exception as e:
        print(f"âŒ Excel è¯»å–å¤±è´¥: {e}")
        return []

    df = df.reset_index(drop=True)
    results = []

    for i in range(len(df) - 1):
        no_val = str(df.iloc[i, 0]).strip()
        if not no_val.isdigit():
            continue  # åªå¤„ç†æ•°å­—ç¼–å·è¡Œ

        stock = clean_text(df.iloc[i, 1])  # éŠ˜æŸ„å
        category = clean_text(df.iloc[i, 2])  # åˆ†é‡
        tmp_cat = clean_text(df.iloc[i, 4])
        if fcode == "140312-3":
            desc = clean_text(df.iloc[i, 3])  # çµ„å…¥éŠ˜æŸ„è§£èª¬ï¼ˆGåˆ—ï¼Œç¬¬1è¡Œï¼‰
            esg = clean_text(df.iloc[i + 1, 3])  # ESGç†ç”±ï¼ˆGåˆ—ï¼Œç¬¬2è¡Œï¼‰
        elif category == tmp_cat or tmp_cat == '':
            desc = clean_text(df.iloc[i, 6])  # çµ„å…¥éŠ˜æŸ„è§£èª¬ï¼ˆGåˆ—ï¼Œç¬¬1è¡Œï¼‰
            esg = clean_text(df.iloc[i + 1, 6])  # ESGç†ç”±ï¼ˆGåˆ—ï¼Œç¬¬2è¡Œï¼‰
        else:
            desc = clean_text(df.iloc[i, 4])  # çµ„å…¥éŠ˜æŸ„è§£èª¬ï¼ˆGåˆ—ï¼Œç¬¬1è¡Œï¼‰
            esg = clean_text(df.iloc[i + 1, 4])  # ESGç†ç”±ï¼ˆGåˆ—ï¼Œç¬¬2è¡Œï¼‰

        if stock:
            results.append([stock, category, desc, esg])

    return results


def handle_sheet_plus5(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        if fcode in ['140787', '180342-3','140312-3']:
            # å°† .pdf æ›¿æ¢ä¸º .xlsx ä½œä¸º Excel æ–‡ä»¶è·¯å¾„
            excel_url = pdf_url.replace(".pdf", ".xlsx")

            # ä¸‹è½½ Excel æ–‡ä»¶å†…å®¹
            response = requests.get(excel_url)
            if response.status_code != 200:
                return "Excelä¸‹è½½å¤±è´¥"

            # è½¬ä¸º BytesIO å¯¹è±¡ä¼ ç»™ extract_excel_table5
            excel_file = io.BytesIO(response.content)
            tables = extract_excel_table5(excel_file,fcode)
        else:
            pdf_response = requests.get(pdf_url)
            if pdf_response.status_code != 200:
                return "PDFä¸‹è½½å¤±è´¥"

            tables = extract_structured_tables(io.BytesIO(pdf_response.content))
        if not tables:
            return "PDFä¸­æœªæå–åˆ°è¡¨æ ¼"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥"

        seen_stocks = set()
        unique_rows = []

        i = 0

        # åˆå¹¶æ‰€æœ‰è¡¨æ ¼è¡Œä¸ºä¸€ä¸ªå¤§åˆ—è¡¨
        all_rows = tables

        if fcode in ['140787', '180342-3']:
            for row in all_rows:

                stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[0]), flags=re.IGNORECASE)
                keti = clean_text(row[1])
                desc = clean_text(row[2])
                esg = clean_text(row[3])

                seen_stocks.add(stock)
                unique_rows.append([stock, keti, desc, esg])
                if len(unique_rows) >= 10:
                    break
        else:

            while i < len(all_rows) - 1:
                row1 = all_rows[i]
                row2 = all_rows[i + 1]

                if len(row1) < 2 or str(row1[0]).strip() not in [str(n) for n in range(1, 11)]:
                    i += 1
                    continue

                stock = clean_text(row1[1])
                stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row1[1]), flags=re.IGNORECASE)
                if not stock or stock in seen_stocks:
                    i += 1  # â— è¿™é‡Œæ˜¯è·³1è¡Œè€Œä¸æ˜¯2è¡Œ
                    continue
                keti = clean_text(row1[2]) if len(row1) > 3 else ""
                if fcode in ["140793-6", "140406-7", "180340-1"]:
                    desc = clean_text(row1[3]) if len(row1) > 3 else ""
                    esg = clean_text(row2[3]) if len(row2) > 3 else ""
                else:
                    desc = clean_text(row1[4]) if len(row1) > 3 else ""
                    esg = clean_text(row2[4]) if len(row2) > 3 else ""

                seen_stocks.add(stock)
                unique_rows.append([stock, keti, desc, esg])
                i += 2  # âœ… åªæœ‰è¿½åŠ æˆåŠŸæ‰è·³è¿‡2è¡Œ

                if len(unique_rows) >= 10:
                    break

        diff_rows = []
        for row in unique_rows:
            stock, keti, desc, esg = row
            query = """
                SELECT * FROM c
                WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
            """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_keti = clean_text(matched[0].get("è§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ", ""))
                old_desc = clean_text(matched[0].get("çµ„å…¥éŠ˜æŸ„è§£èª¬", ""))
                old_esg = clean_text(matched[0].get("ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±", ""))
                if old_desc != desc or old_esg != esg:
                    matched_item = matched[0]
                    matched_item.update({
                        "è§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ": keti,
                        "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±": esg
                    })
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "æ–°è§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ": keti,
                        "å…ƒè§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ": old_keti,
                        "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": old_desc,
                        "æ–°ESGç†ç”±": esg,
                        "å…ƒESGç†ç”±": old_esg,
                        "åˆ†é¡": "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š",
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", "")
                    })
            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "è§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ": keti,
                    "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "ESGã¸ã®å–ã‚Šçµ„ã¿ãŒä¼æ¥­ä¾¡å€¤å‘ä¸Šã«è³‡ã™ã‚‹ç†ç”±": esg,
                    "no": max_no + 1,
                    "months": get_prev_month_str(),
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "æ–°è§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ": keti,
                    "å…ƒè§£æ±ºã™ã¹ãç¤¾ä¼šçš„èª²é¡Œ": "",
                    "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": "",
                    "æ–°ESGç†ç”±": esg,
                    "å…ƒESGç†ç”±": "",
                    "åˆ†é¡": "æ–°è¦éŠ˜æŸ„",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history5(diff_rows)
        # update_excel_with_diff_rows5(diff_rows, fund_type)

        return diff_rows or "å…¨éƒ¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°"

    except Exception as e:
        return f"âŒ handle_sheet_plus5 error: {str(e)}"


# ç§å‹Ÿç›¸å…³çš„å¤„ç†
def insert_tenbrend_history_si4(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": record["å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": record["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"],
            "åˆ†é¡": record["åˆ†é¡"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # âœ… å½“å‰æ—¶é—´
        }
        container.create_item(body=history_item)


def update_excel_with_diff_si4(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_å…¬å‹Ÿ.xlsx"
    else:
        # å¦‚éœ€ç§å‹Ÿé€»è¾‘å¯æ‰©å±•
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_ç§å‹Ÿ.xlsx"

    # ä¸‹è½½åŸå§‹ Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        classify = row["åˆ†é¡"]
        no = row["no"]
        months = row["months"]
        new_desc = row["æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # è·å–è¡¨å¤´ä½ç½®
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["éŠ˜æŸ„"])
        desc_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„è§£èª¬", "éŠ˜æŸ„è§£èª¬"])
        no_col = find_column_by_keyword(header_row, ["No", "NO"])
        months_col = find_column_by_keyword(header_row, ["æ±ºç®—æœˆ"])
        fcode_col = find_column_by_keyword(header_row, ["Fã‚³ãƒ¼ãƒ‰"])

        if stock_col is None or desc_col is None:
            continue

        # æŸ¥æ‰¾ fcode æ‰€å±å—çš„èŒƒå›´
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "æ–°è¦éŠ˜æŸ„" and fcode_block_end:
            # æ’å…¥æ–°è§„åˆ° fcode ç»„æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œ
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)

        elif classify == "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š" and target_row_idx:
            # ç›´æ¥æ›´æ–°åŸå€¼
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)

    # ä¸Šä¼ åˆ°åŸå§‹ Blob è·¯å¾„ï¼ˆè¦†ç›–ï¼‰
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def handle_sheet_plus_si4(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        pdf_response = requests.get(pdf_url)
        if pdf_response.status_code != 200:
            return "PDFä¸‹è½½å¤±è´¥ï¼Œæ²¡æœ‰æ‰¾åˆ°pdf"

        tables = extract_pdf_table(io.BytesIO(pdf_response.content))
        if not tables:
            return "PDFä¸­æœªæå–åˆ°è¡¨æ ¼"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥ï¼Œä¸èƒ½æ‰“å¼€excel"

        seen_stocks = set()
        unique_rows = []

        for table in tables:
            for row in table:
                if len(row) < 3:
                    continue

                if not row or str(row[0]).strip() not in [str(i) for i in range(1, 11)]:
                    continue
                
                pdf_stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row[1]), flags=re.IGNORECASE)
                if not row[2]:
                    pdf_desc = clean_text(row[3]) if len(row) > 3 else ""
                else:
                    pdf_desc = clean_text(row[2])

                if not pdf_stock or pdf_stock in seen_stocks:
                    continue

                seen_stocks.add(pdf_stock)
                unique_rows.append([pdf_stock, pdf_desc])
                if len(unique_rows) >= 10:
                    break
            if len(unique_rows) >= 10:
                break

        # âœ… æ¯”å¯¹é€»è¾‘
        diff_rows = []
        for stock, desc in unique_rows:
            query = """
                    SELECT * FROM c
                    WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
                """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_desc = clean_text(matched[0].get("çµ„å…¥éŠ˜æŸ„è§£èª¬", ""))

                classify = None
                if old_desc != desc:
                    classify = "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š"

                if classify:
                    matched_item = matched[0]
                    matched_item["çµ„å…¥éŠ˜æŸ„è§£èª¬"] = desc
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": old_desc,
                        "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                        "åˆ†é¡": classify,
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", ""),
                        "åˆ†é¡": "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š"
                    })

            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "months": get_prev_month_str(),
                    "no": max_no + 1,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "ã‚³ãƒ¡ãƒ³ãƒˆ": ""
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "å…ƒçµ„å…¥éŠ˜æŸ„è§£èª¬": "",
                    "æ–°çµ„å…¥éŠ˜æŸ„è§£èª¬": desc,
                    "åˆ†é¡": "æ–°è¦éŠ˜æŸ„",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history_si4(diff_rows)
        # update_excel_with_diff_si4(diff_rows, fund_type)

        return diff_rows or "å…¨éƒ¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°"

    except Exception as e:
        return f"âŒ handle_sheet_4plus41 error: {str(e)}"


def insert_tenbrend_history_si5(diff_rows):
    container = get_db_connection(TENBREND_CONTAINER_NAME)

    for record in diff_rows:
        history_item = {
            "id": str(uuid.uuid4()),
            "filename": record["filename"],
            "fcode": record["fcode"],
            "sheetname": record["sheetname"],
            "stocks": record["stocks"],
            "æ–°ç¤¾ä¼šçš„èª²é¡Œ": record["æ–°ç¤¾ä¼šçš„èª²é¡Œ"],
            "å…ƒç¤¾ä¼šçš„èª²é¡Œ": record["å…ƒç¤¾ä¼šçš„èª²é¡Œ"],
            "æ–°ã‚³ãƒ¡ãƒ³ãƒˆ": record["æ–°ã‚³ãƒ¡ãƒ³ãƒˆ"],
            "å…ƒã‚³ãƒ¡ãƒ³ãƒˆ": record["å…ƒã‚³ãƒ¡ãƒ³ãƒˆ"],
            "æ–°ESGã‚³ãƒ¡ãƒ³ãƒˆ": record["æ–°ESGã‚³ãƒ¡ãƒ³ãƒˆ"],
            "å…ƒESGã‚³ãƒ¡ãƒ³ãƒˆ": record["å…ƒESGã‚³ãƒ¡ãƒ³ãƒˆ"],
            "åˆ†é¡": record["åˆ†é¡"],
            "no": record["no"],
            "created_at": datetime.now(UTC).isoformat()  # âœ… å½“å‰æ—¶é—´
        }
        container.create_item(body=history_item)


def update_excel_with_diff_si5(diff_rows, fund_type):
    if not diff_rows:
        return

    if fund_type == "public":
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_å…¬å‹Ÿ.xlsx"
    else:
        # å¦‚éœ€ç§å‹Ÿé€»è¾‘å¯æ‰©å±•
        target_excel_name = "10éŠ˜æŸ„ãƒã‚¹ã‚¿ç®¡ç†_ç§å‹Ÿ.xlsx"

    # ä¸‹è½½åŸå§‹ Excel
    excel_url = f"{PDF_DIR}/{target_excel_name}"
    excel_response = requests.get(excel_url)
    if excel_response.status_code != 200:
        raise Exception("Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥")

    wb = load_workbook(filename=io.BytesIO(excel_response.content))

    for row in diff_rows:
        sheetname = row["sheetname"]
        fcode = row["fcode"]
        stock = row["stocks"]
        classify = row["åˆ†é¡"]
        no = row["no"]
        months = row["months"]
        new_keti = row["æ–°ç¤¾ä¼šçš„èª²é¡Œ"]
        new_desc = row["æ–°ã‚³ãƒ¡ãƒ³ãƒˆ"]
        new_esg = row["æ–°ESGã‚³ãƒ¡ãƒ³ãƒˆ"]

        if sheetname not in wb.sheetnames:
            continue
        ws = wb[sheetname]

        # è·å–è¡¨å¤´ä½ç½®
        header_row = ws[3]

        stock_col = find_column_by_keyword(header_row, ["çµ„å…¥éŠ˜æŸ„", "éŠ˜æŸ„", "éŠ˜æŸ„å"])
        keti_col = find_column_by_keyword(header_row, ["ç¤¾ä¼šçš„èª²é¡Œ", "ç›®æŒ‡ã™ã‚¤ãƒ³ãƒ‘ã‚¯ãƒˆ"])
        desc_col = find_column_by_keyword(header_row, ["ã‚³ãƒ¡ãƒ³ãƒˆ"])
        esg_col = find_column_by_keyword(header_row, ["ESGã‚³ãƒ¡ãƒ³ãƒˆ"])  # ä»…å½“ä½ å¤„ç†ESGè¡¨æ—¶éœ€è¦
        no_col = find_column_by_keyword(header_row, ["No"])
        months_col = find_column_by_keyword(header_row, ["æ±ºç®—æœˆ"])
        fcode_col = find_column_by_keyword(header_row, ["Fã‚³ãƒ¼ãƒ‰"])

        if stock_col is None or desc_col is None:
            continue

        # æŸ¥æ‰¾ fcode æ‰€å±å—çš„èŒƒå›´
        if fcode_col is None:
            continue

        last_row = ws.max_row
        target_row_idx = None
        fcode_block_end = None

        for row_idx in range(2, last_row + 1):
            if str(ws.cell(row=row_idx, column=fcode_col + 1).value).strip() == fcode:
                if fcode_block_end is None or row_idx > fcode_block_end:
                    fcode_block_end = row_idx
                # æŸ¥æ‰¾æ˜¯å¦å·²å­˜åœ¨è¯¥ stock
                current_stock = str(ws.cell(row=row_idx, column=stock_col + 1).value).strip()
                current_stock = clean_text(current_stock)
                if current_stock == stock:
                    target_row_idx = row_idx
                    break

        if classify == "æ–°è¦éŠ˜æŸ„" and fcode_block_end:
            # æ’å…¥æ–°è§„åˆ° fcode ç»„æœ€åä¸€è¡Œçš„ä¸‹ä¸€è¡Œ
            insert_idx = fcode_block_end + 1
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.insert_rows(insert_idx)
            copy_row_style(ws, insert_idx - 1, insert_idx)
            ws.cell(row=insert_idx, column=fcode_col + 1, value=fcode)
            write_wrapped_stock_cell(ws, insert_idx, stock_col + 1, stock)
            ws.cell(row=insert_idx, column=keti_col + 1, value=new_keti)
            ws.cell(row=insert_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=insert_idx, column=no_col + 1, value=no)
            ws.cell(row=insert_idx, column=months_col + 1, value=months)
            ws.cell(row=insert_idx, column=esg_col + 1, value=new_esg)

        elif classify == "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š" and target_row_idx:
            # ç›´æ¥æ›´æ–°åŸå€¼
            ws.cell(row=target_row_idx, column=keti_col + 1, value=new_keti)
            ws.cell(row=target_row_idx, column=desc_col + 1, value=new_desc)
            ws.cell(row=target_row_idx, column=esg_col + 1, value=new_esg)

    # ä¸Šä¼ åˆ°åŸå§‹ Blob è·¯å¾„ï¼ˆè¦†ç›–ï¼‰
    output_stream = io.BytesIO()
    wb.save(output_stream)
    output_stream.seek(0)
    container_client = get_storage_container()
    blob_client = container_client.get_blob_client(target_excel_name)
    blob_client.upload_blob(output_stream, overwrite=True)


def clean_text_si(text):
    return text.replace("\n", "").replace(" ", " ").strip()


def split_by_numbered_blocks(text_block):
    parts = re.split(r'\n?(?=\s*([1-9]|10)[^\d])', text_block)
    combined_parts = []
    i = 1
    while i < len(parts):
        num = parts[i].strip()
        content = parts[i + 1].strip() if i + 1 < len(parts) else ""
        combined_parts.append(f"{num} {content}")
        i += 2

    results = []
    for part in combined_parts:
        match = re.match(r"^([1-9]|10)[\s ]*([^\s\d]{2,30})[\s ]*([\s\S]+)", part)
        if match:
            no = match.group(1)
            company = clean_text_si(match.group(2))
            description = clean_text_si(match.group(3))
            # å¦‚æœä¸‰é¡¹éƒ½ä¸ä¸ºç©ºï¼Œå†åŠ å…¥
            if no and company and description:
                results.append([no, company, description])
    return results


def extract_structured_tables(pdf_input):
    all_rows = []
    with pdfplumber.open(pdf_input) as pdf:
        for page in pdf.pages:
            text = page.extract_text() or ""
            if "çµ„å…¥ä¸Šä½10éŠ˜æŸ„ã®è§£èª¬" not in text:
                continue

            tables = page.extract_tables()
            if not tables:
                continue

            for table in tables:
                if len(table) == 1 and len(table[0]) == 1:
                    # è¯´æ˜æ˜¯åˆå¹¶æ–‡æœ¬å—ï¼ˆå•å…ƒæ ¼ä¸­æ˜¯æ®µè½æ–‡å­—ï¼‰
                    text_block = table[0][0]
                    rows = split_by_numbered_blocks(text_block)
                    all_rows.extend(rows)
                else:
                    # æ™®é€šè¡¨æ ¼ç»“æ„
                    for row in table:
                        cleaned_row = [clean_text_si(cell) if cell else "" for cell in row]
                        # âœ… è¿‡æ»¤æ‰å­—æ®µæ•°é‡å°‘äº3çš„è¡Œ
                        if len(cleaned_row) >= 3:
                            all_rows.append(cleaned_row)
    return all_rows


def handle_sheet_plus_si5(pdf_url, fcode, sheetname, fund_type, container, filename):
    try:
        pdf_response = requests.get(pdf_url)
        if pdf_response.status_code != 200:
            return "PDFä¸‹è½½å¤±è´¥"

        tables = extract_structured_tables(io.BytesIO(pdf_response.content))
        if not tables:
            return "PDFä¸­æœªæå–åˆ°è¡¨æ ¼"

        excel_url = f"{PDF_DIR}/10mingbing.xlsx"
        excel_response = requests.get(excel_url)
        if excel_response.status_code != 200:
            return "Excelæ–‡ä»¶ä¸‹è½½å¤±è´¥"

        seen_stocks = set()
        unique_rows = []

        i = 0

        # åˆå¹¶æ‰€æœ‰è¡¨æ ¼è¡Œä¸ºä¸€ä¸ªå¤§åˆ—è¡¨
        all_rows = tables

        while i < len(all_rows) - 1:
            row1 = all_rows[i]
            row2 = all_rows[i + 1]

            if len(row1) < 2 or str(row1[0]).strip() not in [str(n) for n in range(1, 11)]:
                i += 1
                continue

            stock = re.sub(r'^(NEW\s*|new\s*)|(\s*NEW|\s*new)$', '', clean_text(row1[1]), flags=re.IGNORECASE)
            if not stock or stock in seen_stocks:
                i += 1  # â— è¿™é‡Œæ˜¯è·³1è¡Œè€Œä¸æ˜¯2è¡Œ
                continue
            keti = clean_text(row1[2]) if len(row1) > 3 else ""
            desc = clean_text(row1[3]) if len(row1) > 3 else ""
            esg = clean_text(row2[3]) if len(row2) > 3 else ""

            seen_stocks.add(stock)
            unique_rows.append([stock, keti, desc, esg])
            i += 2  # âœ… åªæœ‰è¿½åŠ æˆåŠŸæ‰è·³è¿‡2è¡Œ

            if len(unique_rows) >= 10:
                break

        diff_rows = []
        for row in unique_rows:
            stock, keti, desc, esg = row
            query = """
                SELECT * FROM c
                WHERE c.sheetname = @sheetname AND c.fcode = @fcode AND c.stocks = @stock
            """
            params = [
                {"name": "@sheetname", "value": sheetname},
                {"name": "@fcode", "value": fcode},
                {"name": "@stock", "value": stock}
            ]
            matched = list(container.query_items(query=query, parameters=params, enable_cross_partition_query=True))

            if matched:
                old_keti = clean_text(matched[0].get("ç¤¾ä¼šçš„èª²é¡Œ", ""))
                old_desc = clean_text(matched[0].get("ã‚³ãƒ¡ãƒ³ãƒˆ", ""))
                old_esg = clean_text(matched[0].get("ESGã‚³ãƒ¡ãƒ³ãƒˆ", ""))
                if old_desc != desc or old_esg != esg:
                    matched_item = matched[0]
                    matched_item.update({
                        "ç¤¾ä¼šçš„èª²é¡Œ": keti,
                        "ã‚³ãƒ¡ãƒ³ãƒˆ": desc,
                        "ESGã‚³ãƒ¡ãƒ³ãƒˆ": esg
                    })
                    container.replace_item(item=matched_item["id"], body=matched_item)

                    diff_rows.append({
                        "filename": filename,
                        "fcode": fcode,
                        "sheetname": sheetname,
                        "stocks": stock,
                        "æ–°ç¤¾ä¼šçš„èª²é¡Œ": keti,
                        "å…ƒç¤¾ä¼šçš„èª²é¡Œ": old_keti,
                        "æ–°ã‚³ãƒ¡ãƒ³ãƒˆ": desc,
                        "å…ƒã‚³ãƒ¡ãƒ³ãƒˆ": old_desc,
                        "æ–°ESGã‚³ãƒ¡ãƒ³ãƒˆ": esg,
                        "å…ƒESGã‚³ãƒ¡ãƒ³ãƒˆ": old_esg,
                        "åˆ†é¡": "éŠ˜æŸ„è§£èª¬æ›´æ–°ã‚ã‚Š",
                        "no": matched_item.get("no", 0),
                        "months": matched_item.get("months", "")
                    })
            else:
                query_max = "SELECT VALUE MAX(c.no) FROM c WHERE c.fcode = @fcode"
                max_no = list(container.query_items(
                    query=query_max,
                    parameters=[{"name": "@fcode", "value": fcode}],
                    enable_cross_partition_query=True
                ))[0] or 0

                new_item = {
                    "id": str(uuid.uuid4()),
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "ç¤¾ä¼šçš„èª²é¡Œ": keti,
                    "ã‚³ãƒ¡ãƒ³ãƒˆ": desc,
                    "ESGã‚³ãƒ¡ãƒ³ãƒˆ": esg,
                    "no": max_no + 1,
                    "months": get_prev_month_str(),
                }
                container.create_item(body=new_item)

                diff_rows.append({
                    "filename": filename,
                    "fcode": fcode,
                    "sheetname": sheetname,
                    "stocks": stock,
                    "æ–°ç¤¾ä¼šçš„èª²é¡Œ": keti,
                    "å…ƒç¤¾ä¼šçš„èª²é¡Œ": "",
                    "æ–°ã‚³ãƒ¡ãƒ³ãƒˆ": desc,
                    "å…ƒã‚³ãƒ¡ãƒ³ãƒˆ": "",
                    "æ–°ESGã‚³ãƒ¡ãƒ³ãƒˆ": esg,
                    "å…ƒESGã‚³ãƒ¡ãƒ³ãƒˆ": "",
                    "åˆ†é¡": "æ–°è¦éŠ˜æŸ„",
                    "no": max_no + 1,
                    "months": get_prev_month_str()
                })

        insert_tenbrend_history_si5(diff_rows)
        # update_excel_with_diff_si5(diff_rows, fund_type)

        return diff_rows or "å…¨éƒ¨ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°"

    except Exception as e:
        return f"âŒ handle_sheet_plussi5 error: {str(e)}"




app = WsgiToAsgi(app)

if __name__ == '__main__':
    app.run(host='0.0.0.0', debug=True) # å¯ç”¨HTTPS, ssl_context='adhoc'
